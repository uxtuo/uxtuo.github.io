<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[module]]></title>
    <url>%2F2018%2F03%2F14%2F%E6%A0%BC%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[简介 Oracle Database，又名Oracle RDBMS，或简称Oracle。是甲骨文公司的一款关系数据库管理系统。它是在数据库领域一直处于领先地位的产品。可以说Oracle数据库系统是目前世界上流行的关系数据库管理系统，系统可移植性好、使用方便、功能强，适用于各类大、中、小、微机环境。它是一种高效率、可靠性好的 适应高吞吐量的数据库解决方案。 软件 下载地址 Google云计算三大论文英文版 点击下载 Google-File-System中文版 点击下载 Google-MapReduce中文版 点击下载 Google-Bigtable中文版 点击下载 一级目录 二级目录 三级目录 四级目录 软件 下载地址 Google云计算三大论文英文版 点击下载 我是黑体字 我是微软雅黑 我是华文彩云 color=#0099ff size=72face=”黑体” color=#00ffff color=gray What is Hexo?]]></content>
      <categories>
        <category>Database</category>
      </categories>
      <tags>
        <tag>Oracle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Book Summary系列写作计划]]></title>
    <url>%2F2018%2F03%2F01%2FBook-Summary%E5%86%99%E4%BD%9C%E8%AE%A1%E5%88%92%2F</url>
    <content type="text"><![CDATA[简介 “Book Summary”-书的概述，预期是写成一个系列，主要内容有研究生课程学习的课本，对我的思想有重大启发的纪录片和文章书籍，包括不限于历史，宇宙，文化，才艺等等，工作中自学或用到的技术书籍，当前买了很多实体书，oracle/mysql/python/SQL/Spark/网络，还有”多看”上的电子书，没有人督促来读，也没有指标，进度很慢，准备在通读之后把书的内容整理出来，一是记录，二是分享。 规划 信息社会，知识已经不像以前那么宝贵，茫茫多的免费资料，各个机构分享出来的技术资料俯拾皆是，学习能力反而成了个人提升的瓶颈，就比如欧洲黑暗中世纪和我国的封建时代，皇权和神权不约而同奉行愚民政策，封锁知识流动，将社会极大部分人的知识层次和眼光局限在较低层次对社会和权力的稳定有奇效，然而近代科技爆炸，知识和信息在全社会无障碍流通，假设有一个人有无上限的学习能力和精力，那么就跟电影《永无止境》里一样，很容易就能突破一切局限，乐器，搏击，商业，政治，社交，任何方面都能极快地获得超凡成就。然而很遗憾，人力有穷，现实中的我们困在百十来斤的肉体中，困了要睡，伤了会痛，饿了要吃，高兴会笑伤心会哭，肉体的吃喝拉撒和情绪的喜怒哀乐这种本能的存在是生活多姿多彩的因素，是人类欲望的根源，是不可缺少的部分。 但对个人的自我学习来说，极为不利，当前我的解决方案是持中庸之道，乐而不淫，哀而不伤，形成稳定的人生观和世界观，冷静的处理生活，工作和生活节奏，同时&quot;stay hungry,stay curious&quot;,保持旺盛的好奇心和学习的心态，做一个终身学习者。结果上来说，能让人生轨迹的弧线往上翘的几率大那么一丢丢，而且过程中的压力，动力，成就，痛苦，不都是让人生更充实的资粮么，学习的过程本就是一种享受啊！ 原书作者在写书的时候付出了大量的时间和脑力劳动，出版商负责印刷，排版，出版发行，最后根据销量收入来调整书的发行数量和再版次数，付出不菲，然而大部分市面上的技术书籍价格跟价值比起来简直就是白菜价，如果销量还是一片惨淡，那可能负反馈太强烈以至于作者觉得写了书没人看，出版社赚不到钱不发行，结果就是好书越来越少，所以我建议手头不是仓促到下一顿饭不知道在哪的朋友尽量买正版书支持一下，让作者有动力写更多更优质的内容出来，这样就能学到更多干货吖\^o^/，当然也可以自己去网上找资源，pdf和免费资源多得是，只要认真看从中学到东西了，作者应该也会高兴地，不用有看盗版的心理负担，没关系的。 笔者主要阅读途径有三：实体书，”多看阅读”，bilibili纪录片。比如让我兴起写这个读书笔记系列念头的《Spark最佳实践》就是在“多看阅读”上趁着折扣30块钱买的，实体书阅读体验好但会稍贵，大概40-50，京东淘宝亚马逊等平台都有售，而一些python，oracle，爬虫等实体书大部分是在亚马逊买的，bilibili的纪录片资源很多，各个方面层次都有，比如清华的大数据课程，小象集团的spark课程，宇宙如何运行等等琳琅满目，免费无广告摆放整齐，力荐！ 总之，有条件就支持下作者，免费看了书专门去买书支持一把作者这种事我也干过不少，量力而行吧诸君^(*￣(oo)￣)^ 愿前进的途中，光照在你我的身上，早日实现生命的大和谐~~]]></content>
      <categories>
        <category>写作计划</category>
      </categories>
      <tags>
        <tag>Book Summary</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark最佳实践]]></title>
    <url>%2F2018%2F02%2F28%2FBook-Spark%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5%2F</url>
    <content type="text"><![CDATA[简介 本书是Spark的实战指南，全书共8章，前四章介绍Spark部署，工作机制和内核，后四章通过实战项目介绍Spark SQL，Spark Streaming，Spark GraphX和Spark MLlib功能模块。 通过阅读，读者可以接收的信息和获取的能力包括: Spark单机及集群部署，一键部署大规模集群的脚本； 对Spark运行过程，数据结构，内核函数进行深入解析； hdfs解决了数据的分布式存储，Spark作为优秀的计算框架解决了数据处理的效率问题，那么把数据屯起来处理，想要干啥？这就是本书第三个重要的部分，对大量数据进行查询支持的Spark SQL、应对网站实时统计，数据流处理的Spark Streaming、在社交网络关系挖掘中举足轻重的图处理 Spark GraphX、在当前最火的机器学习上提供了Spark MLlib模块，实现了分类聚类回归等多种算法，这些可以处理实际问题的模块是Spark 业内热度飙升的重要原因。 作者是陈欢和林世飞，在腾讯的社交和效果广告部负责大数据处理和分析的相关工作，掌握着海量的服务器资源，利用成规模的Spark集群来捣腾海量数据。 成书时间2016年2月，技术迭代很快又很慢，随着市场发展新的功能需求不断产生，技术的深度和使用技巧快速演进，慢则体现在技术的稳定性，五年前十年前的应用和概念多得是，很多十多年前的技术书籍现在拿来看依然唇齿留香，经典绝伦。Spark作为已经相当成熟的大数据领域工业应用级框架，以后肯定会不断发展，但根基整个推翻是不太可能了，那意味着整个行业所有跑在Spark上面的应用全部废掉，本书作为资深人士刚写出来结合行业内当前发展的基础性技术书籍，预期3-5年不会过时，作为入门书籍那是相当靠谱。 前言 互联网快速发展，最近几年全球数据量以每年50%的速度递增，大数据在事实上改变着我们的思维和生产方式。2015年9月国务院印发《促进大数据发展行动纲要》，大数据与各行各业的结合是大势所趋，大数据相关的企业创业和从业者的需求眼看着就是一波大发展。 03年开始谷歌陆续发表三大论文，在Apache软件基金会和雅虎等互联网公司的支持下，实现了大量了开源服务框架，包括hadoop，hive等重量级产品，hadoop拉开了IT行业使用大数据技术的序幕，后续发展如火如荼，数据量持续增长技术不断演化，10年美国加州伯克利分校陆续提出了多份RDD（Resilient Distributed Dataset，弹性分布式数据集）相关论文，随之推出了开源的Spark框架，对比传统的Hadoop，拥有深厚学术背景的Spark将以往的MapReduce，流式计算，机器学习等全部统合，提供了一站解决方案，让数据挖掘和机器学习的门槛大大降低，加速了大数据技术在各个行业的普及。 大数据热门之后出现了非常多的大数据技术，有Hadoop、Hive、HBase、Flume、Kafka、Lucene、Spark、Storm等，还有很多NoSQL技术可以归入大数据，比如MongoDB、CouchDB、Cassandra等。有人列举过大数据技术族谱，其中至少上百种技术。面对如此多的技术，我们往往会产生疑惑：到底该如何选择？这里有一个建议：在充分理解自身需求的基础上，挑选最合适的。 第一章 Spark和大数据 概述 数据只是工具，最终还是要用于创造价值，大数据只是一种新的实践。 介绍大数据的发展状况，以及Spark的起源、特点、优势、未来 大数据是什么 大数据大势所趋 “大数据”(big data)这个词最早出现在UNIX用户协会的会议上，来自SGI公司的科学家在其文章”大数据与下一代基础架构”中用它描述数据的快速增长，具有”4V”，大量（volume），多样（variety），快速（velocity），价值（value）的特征，从数据量上来看，通常认为当一个计算单元容纳不下要处理的数据，那就是大数据的场景。 电脑发明之前做人口统计，全国耕地面积普查，气象预报等；使用普通pc，服务器的时候碰到了互联网级别数据统计，如搜索引擎需要把网上的多数网页抓取来建立索引，这样我们搜索的时候才能毫秒级返回结果。假设每个网页平均大小20KB，大概有价值的中文网页有200亿，大概总共有400TB大小，那么一台计算机以30MB~40MB的速度从硬盘读写，大概需要4个月，还不包括做复杂的分析工作，这时候就需要计算机集群和大数据技术来存储处理。 大数据的两大问题：存储和计算。 X表示一台服务器是否正常工作，X=0表示正常工作，X=1表示不正常工作。服务器一个月内发生故障的概率是ε，P(X= 0)= 1 -ε，那么1000台电脑都正常工作的概率是(1-ε)^1000。假设故障率ε=0.001，所有1000台服务器1个月内正常工作的概率只有0.37，不到一半，所以随着数据的增长，机器会不断增加，数据存储和运行过程中都会产生宕机丢数据网络波动等意外状况。hadoop的hdfs从设计上解决了数据的分布式存储，集群的扩展性和容错性都非常完善。 计算框架包括很多优秀的平台，如hadoop的mapreduce做批处理任务，storm做流式处理，hive进行分布式的SQL查询，还有很多如mahout，mpi等，这些可以分别满足不同场景的优秀框架解决了海量数据的处理问题，使得从中进行数据挖掘，实现机器学习成为可能。 大数据发展的技术要素都已经存在，2015年9月5日国务院印发了《促进大数据发展行动纲要》，纲要提出要建设公共数据资源开放的统一开放平台，“2020年底前，逐步实现信用、交通、医疗、卫生、就业、社保、地理、文化、教育、科技、资源、农业、环境、安监、金融、质量、统计、气象、海洋、企业登记监管等民生保障服务相关领域的政府数据集向社会开放”，以及“到2020年，培育10家国际领先的大数据核心龙头企业，500家大数据应用、服务和产品制造企业”，大数据的浪潮已经不可阻挡！ 这里引出一个概念，”云计算”，云计算是一种按使用量付费的模式，这种模式提供可用的、便捷的、按需的网络访问， 进入可配置的计算资源共享池（资源包括网络，服务器，存储，应用软件，服务）,有不同层级的服务模式，IaaS(Infrastructure as Service)：基础设施即服务，PaaS(Platform as Service)：平台即服务，SaaS(Software as Service)：软件即服务。其中IaaS支持按天付费，而且可以动态按需扩容。在云计算服务商的帮助下，如今一个小创业公司都能够快速开发和部署大数据应用。云计算服务商的国外领导者是AmazonAWS，国内主要是阿里云、腾讯云和UCloud。 大数据的重要解决方案hadoop Hadoop开天辟地 谷歌的三大论文引发了人们在大数据领域的大量研究，直接导致了Hadoop的出现——MapReduce范式的开源实现。如今Hadoop，包括MapReduce与分布式文件系统（HDFS），已经成为数据处理的事实标准。大量的工业界应用，例如腾讯、百度、阿里巴巴、华为、迪斯尼、沃尔玛、AT&amp;T都已经有自己的Hadoop集群。 如图，Hadoop框架的核心是HDFS和MapReduce，其中HDFS是分布式文件系统，MapReduce是分布式数据处理模型和执行环境。 各个组件的功能互相配合使得hadoop能够完成很多场景的大数据任务，本文主角是Spark，这里不再赘述，有兴趣了解的可以移步。 但MapReduce不是唯一的大数据分析范式，有一些场景是不适合使用MapReduce的，比如处理网状的数据结构时，这要求能够处理顶点和边的增加和减少操作，并在所有节点进行运算，比如进行机器学习算法需要高速多次迭代的时候因为硬盘读写瓶颈会显得效率稍低，比如不擅长实时计算，需要使用storm，s4，akka等系统。 为什么要用Spark Spark应运而生 Spark替代hadoop不太准确，如上面生态系统图，确切来说spark替代的是MapReduce，凭借自身的内存计算，RDD数据结构在批处理方面比MapReduce表现得更加出色，同时Spark本身拥有Spark SQL，streaming，MLlib，GraphX这些模块，使得诸多功能用Spark一站搞定，不用像之前那样根据需要分别使用MapReduce，hive，storm等，这让Spark在使用门槛，维护成本和性能表现上具有明显优势。 机器学习算法通常需要对同一个数据集合进行多次迭代计算，而MapReduce中每次迭代都会涉及HDFS的读写，以及缺乏一个常驻的MapReduce作业，因此每次迭代需要初始化新的MapReduce任务，效率不高，基于MR的HIVE，PIG等技术也有类似问题。 Spark作为一个研究项目，诞生于加州大学伯克利分校AMP实验室（Algorithms, Machines, and People Lab），为了迭代算法和交互式查询两种典型的场景，Matei Zaharia和合作伙伴开发了Spark系统的最初版本。2009年Spark论文发布，Spark项目正式诞生，在某些任务表现上，Spark相对于Hadoop MapReduce有10～20倍的性能提升。2010年3月Spark开源，且在开源社区下发展迅速。2014年5月，Spark 1.0正式发布，如今已经是Apache基金会的顶级项目了。 如下图在官方博客的排序大赛可以有很清晰的对比，可以看看英文，还顺便创造了个世界纪录2333。最新的结果可以戳这里。 Spark一直寻求保持Spark引擎小而紧凑。Spark 0.3版本只有3900行代码，其中1300行为Scala解释器，600行为示例代码，300行为测试代码。即使在今天，Spark核心代码也只有约50 000行，因此更容易为许多开发人员所理解和供我们改变和提高。 Spark未来发展 Spark在过去的5年里发展迅速，社区活跃程度一点儿不亚于Hadoop社区。 Spark峰会是社区分享Spark应用的重要交流会，在Spark Summit 2015上，来自Databricks、UC Berkeley AMPLab、百度、阿里巴巴、雅虎、英特尔、亚马逊、Red Hat、微软等的数十个机构共分享了近100个精彩的报告。 目前Databricks、英特尔、雅虎、加州大学伯克利分校是Spark主要的贡献者。 已经大规模应用在生产环境，2015年Spark最大的集群来自腾讯，一共有8000个节点，单个Job最大分别是阿里巴巴和Databricks的1 PB，国内的BAT都在用Spark，可以从这里查看完整的用户列表，Spark作为一种优秀的大数据处理技术已经得到广泛认可，在如此多的应用场景中承担着任务，逐渐会积累庞大的用户经验和文档。 道家看事物分为”道，法，术，器”四个层次，拿到大数据领域来看，大概可以这么理解，Spark是一种工具-器，掌握用Spark去解决实际问题的方法-术，能够根据场景和需求熟练选择合适的技术，不滞于某种技术，这就是掌握了大数据领域解决问题的基本规则-法，而道，可能就是那些在十几年前就已经从计算机技术和社会发展的层次看到了数据膨胀和物理发展的冲突，那个时候就开始提出理论试图拿出解决方案的人，是那些将理论转为现实，设计出分布式存储和各种算法的人，是站在大数据发展的最前端，了解技术走向并引领潮流的人所处的层次吧，难望项背，对先行者和开拓者唯有敬意！ 第二章 Spark基础 君子藏器于身，待时而动 本章介绍Spark部署和编程。作者首先带领读者在本地单机下体验Spark的基本操作，然后部署一个包括ZooKeeper、Hadoop、Spark的可实际应用的高可用集群。并且，这一章还介绍了作者开发的一个自动化部署工具，最后介绍了Spark编程的基础知识，以及如何打包提交至集群上运行。 Spark集群部署 Spark真实的部署环境绝大部分都在Linux下，运行依赖JDK，大致思路就是在linux系统里配置JDK，将Spark的预编译包解压到指定路径即可使用。 笔者之前已经琢磨过Spark的安装部署，在本博客里另一篇博文Spark实践有详细介绍，所有搭建步骤都经过两遍以上实际操作，验证无误。本地已经有了现成的环境，如下。 这些基本操作没有什么太多的变化，建议按照笔者博文的步骤实际操作，亲手搭建环境后再深入学习原理，手头有环境操作原理并行学习效率会高很多。 本章里精华部分是作者对生产环境各种集群的配合部署和自动化脚本的详细介绍，看起来复杂，各种几千台服务器的初始化，自动化集群部署，实际上在自己实践并理解了整个流程之后会发现不是很难，下面介绍这部分内容，我猜想，作者写作的时候预定阅读群体是对Spark没有认识和实践经验的，所以过程会比较详细，而笔者是对Spark有了了解并且通读全书之后第二遍阅读并整理思路，所以假定本博文的阅读对象已经按照前文提到的博客内容有了实践经验，在此基础上不会那么详细（但保证不会出错！）。 集群总览 整个集群除了核心的Spark之外，还有多个子集群。为了实现集群的高可用性，引入zookeeper集群进行主备切换。Spark没有存储能力，引入HDFS。引入YARN让集群具备非常好的扩展性。 首先是zookeeper集群，集群机器数量为2n+1台，可以抵御n台机器宕机的风险，如果n取1，总共三台，可以抵御1台机器宕机的风险，所有节点都是相同角色，但运行中会自动选择一个作为leader，zookeeper集群担负终极仲裁者的角色，spark集群的master节点通过于zookeeper集群协商完成主备切换，hdfs文件系统的namenode及yarn中的ResourceManager也是通过ZooKeeper集群来协助完成主备切换。 其次是Spark集群，该集群有两类节点，master和slave。 master负责集群的资源管理，只要有一台正常工作即可，一般至少两台，一台主节点，当主节点异常下线时，其他master节点通过zookeeper仲裁竞选成为主节点 slave节点用于执行计算任务，机器数量可以是一台以上，几十几百或上千台。 hadoop集群从2.x开始把存储和计算分离开，分成hdfs和yarn两个子集群。 hdfs是一个分布式文件系统，为集群提供大文件的存储能力，namenode管理所有的文件信息，一主一备，通过zookeeper实现容灾切换，QJM节点数量为2n+1，一般三台即可，负责记录namenode的流水日志，确保数据不丢失，datanode担负集群的存储负载，用于数据存储。 yarn为MapReduce计算提供调度服务，也可为符合yarn编程接口的集群提供调度服务，比如Spark，该集群有两类节点，ResourceManager和NodeManager，RM一主多备，NM通常与datanode部署在一起，实现最高效的数据访问。 集群的设计理念就是利用普通服务器协同工作来实现大数据的计算，因此集群可以部署在任何普通服务器个人计算机甚至个人计算机的多个虚拟机上，但集群各个节点的工作性质不同，对硬件资源的消耗也不同，合理的搭配能更充分地发挥潜力。 “移动计算而不要移动数据”，计算离数据约近移动数据的开销越小，大部分场景移动数据的开销都会大于计算的开销，所以计算节点和存储节点一般会选择混合部署。 节点部署方面建议Spark slave，HDFS datanode，YARN NodeManager三类节点在一起，这些节点在集群中数量最为庞大，最消耗资源，CPU，硬盘，内存资源都可以倾斜。 其他节点的主要任务是管理，除了namenode对内存要求稍高，其他对硬件消耗都不大。可以考虑混合部署，比如笔者用了五台虚拟机，master，namenode，ResourceManager在一台服务器，其他四台服务器作为spark的slave，hdfs的datanode，yarn的nodemanager。机器资源充足的话也可以单独部署，毕竟这些管理节点的数目都不多，将存储，资源管理，计算的主节点按单节点容灾设计，分开部署消耗最多也不过12台服务器，当然会格外麻烦。建议选择稳定性较高的配置，比如双电源，raid，冗余。 具体到硬件配置作者结合Spark官方和实际项目经验，有如下建议 存储 : Spark Slave节点尽可能靠近存储系统（比如HDFS和HBase），如果不能部署在相同节点上，那么请尽可能让它们在物理位置上更近一些以减少网络IO的开销，比如尽可能在相同机架上、同一个路由器下、相同机房里。而且每个节点建议使用4块或更多独立挂载的硬盘，并且不要配置RAID，这样可以提升Spark读写本地缓存数据时的效率。 内存 : Spark节点可以适应几百吉字节大小的内存。更多的内存可以缓存更多的数据，在一些迭代计算场景下性能提升效果非常明显。另外，建议分配最多节点内存总量的75%给Spark使用，其他的留给操作系统。 网络 : 一些需要进行shuffle的byKey类操作，比如group-ByKey、reduceByKey、join，它们的瓶颈一般都在网络上，所以建议使用10 Gbit/s以上网络。在Spark程序的Web界面上，可以看到shuffle对于网络使用的状况。 CPU : Spark程序可以适应几十个内核的CPU，而且其计算性能与CPU核数量基本成正比。一般建议8~16个核，可以结合实际负载与成本综合考虑。一般情况下，如果数据已经在内存中了，CPU和网络IO会成为瓶颈。 单机硬件配置会严重影响集群性能，实际生产环境需要根据负载在成本和性能之间取得平衡。 部署思路 假设，有几千台服务器，刚刚装好系统配好ip，需要搭建一个Spark环境。 首先，需要初始化环境，包括创建账号，安装JDK，设置时间同步； 然后，依次部署zookeeper，hadoop，spark集群并启动服务。 上述操作在机器少的时候可以手动完成，但是需要在所有机器上操作，既要改配置文件，传软件包，还要执行一些命令，必须使用远程操作工具，笔者之前用的是rsync，还有ansible等等，工具只是实现目标的手段，殊途同归，高效解决问题就行，作者在这里用的是python的fabric。 创建账号 给集群所有机器创建账号 安装JDK 分发jdk软件包到指定路径并设置环境变量 设置时间同步 配置ntp服务器后设置集群所有机器时间同步 部署zookeeper集群 笔者部署自己集群的时候没有用到zookeeper，zookeeper主要是对master，namenode，ResourceManager节点进行仲裁，主备切换，属于容灾措施，生产环境必须要有，没有也不影响正常运行。笔者自己搭着玩所以当时没弄，有时间可以玩一玩(^_^) 部署hadoop集群 hadoop的部署没什么说的，解压安装包到指定路径，修改配置文件并同步，fabric，ansible，rsync都能搞定 部署spark集群Spark集群的部署跟hadoop大同小异，解压，改配置文件，起服务，多了一个配置ssh免密码登陆，用ssh-keygen和ssh-copy命令就可以轻松搞定，在所有机器上执行这些命令可以使用fabric这些远程执行工具 一键部署高可用hadoop+Spark集群 相信认真看了的朋友发现了，这个一键部署就是将所有的软件分发，批量修改配置文件，顺序执行各种命令都写在了install.sh这个脚本里面，用到的材料就是准备做集群的那些服务器，具体的内容分解开来就是上述所有步骤，只是用脚本实现批量执行，大大提高效率并且减少人工失误，完美，这就是成熟的运维人员该干的事，然而很遗憾，作者给的链接失效了，笔者没有拿到这个脚本┑(￣Д ￣)┍ 编程指南 集群工作方式 Spark用scala语言开发，scala语言的函数式编程特性让代码的可读性非常高，做spark开发和看spark源码都需要懂scala，Spark也支持Python，java和R，本节用scala语言为示例讲解运行在spark上的程序的编写。 交互式编程，以wordcount举例，执行spark-shell这个命令进入spark命令行，在屏幕滚动日志中可以看到Spark context available as sc，解释器已经初始化了一个名为sc的Spark context对象，可以直接使用。 spark-shell只适合调试，正式环境下Spark程序在调试好之后，需要编译，链接，打成jar包，最后提交给集群运行 编程核心思想 心之所向，一往无前 Spark工作的目的是调用经过清洗转换存储好的数据，用各种方法进行处理，实现查询，数据挖掘，流处理，机器学习等项目需求。 Spark怎么做这件事的？ 第一步，将数据拿进来生成RDD。Spark可以从很多数据源拿到数据，最终都是生成RDD，只是类型不同； 第二步，Spark调用各种函数语法对RDD进行操作计算，transcation定义RDD上的操作，action触发操作，最终结果也是一个RDD，可以将结果按需要保存到指定位置或者进行下一次迭代处理。 RDD是Spark最核心的东西，hadoop的hdfs是为生成RDD的数据提供分布式存储，没有hdfs还能从其他渠道得到，hdfs是一种比较好的解决方案而已，yarn是资源调度，起到Spark触发计算的时候各个节点分配调用计算资源的作用，zookeeper是可以说是冗余容错机制，为了保证spark集群/hdfs存储/计算调度主节点的高可用，他们都是为”Spark生成RDD并调用函数进行处理最终得到预期结果”这一目标而服务的。 那么问题来了，RDD是什么，都可以对RDD进行什么操作？理解了这个问题，Spark可以说已经初窥门径，RDD的本质在后面Spark内核章节进行深入解析，循序渐进，这里先教大家怎么用，怎么创建RDD，怎么对它进行操作。 RDD创建RDD生成方式总结起来两大类：由Driver程序的数据集生成，由外部数据集生成，如共享文件系统，HDFS文件和HBase等 1.从Driver程序的数据集生成 从Driver数据集生成RDD的直接方法是使用SparkContext对象的parallelize方法，其参数是一个Seq对象，或者其他可以被自动转换成Seq的对象也可以作为参数，比如Array，List，scala支持方便的隐式转换。 SparkContext对象还提供了range方法，通过指定长整数的开始值，结束值及步长，生成一定范围的长整型，最终调用parallelize生成RDD 这种方式的数据源收到Driver所在节点的资源限制，不适合处理特别大的数据，多适用于交互式环境下的程序调试，小数据量的数据集测试。 2.从外部数据集生成RDD Spark可以从任何Hadoop支持的存储类型的数据源生成RDD，包括本地文件系统，HDFS，HBase，Amazom S3等，还支持文本文件，Sequence文件，以及其他任何Hadoop InputFormat格式的数据。这使得RDD从生成到计算全过程都是分布式的，不会形成资源瓶颈。 文本文件可以使用SparkContext对象的textFile方法生成，其参数是文件的地址，可以是本地文件系统的完整路径，也可以是以hdfs://、s3n://等开头的URL。 除了最基本的文本文件外，Spark还支持其他类型的文件。 •wholeTextFiles可以一次读取一个目录下的许多小文件，并返回&lt;文件名、内容&gt;二元组，而不是像textFile那样每条记录是一行文本，不保留文件信息。 •对于SequenceFile格式的文件（Hadoop针对大数据计算使用的一种文件格式），可以使用SparkContext对象的se-quenceFile[K, V]方法，其中K和V分别代表文件中key和value的类型，这些类型必须继承自Hadoop的Writable接口，比如IntWritable和Text。不过Spark提供了便利，可以直接使用Scala基本类型，比如Int和String，它们已经实现了Writable接口，所以我们可以直接这样使用：se-quenceFile[Int, String]。 对于HBase文件，因为HBase使用HDFS作为存储，也是一种Hadoop InputFormat，所以也是用hadoopRDD或newAPIHadoopRDD来读取数据的 此外，Spark还提供了RDD对象的序列化和反序列化功能，通过RDD.savaAsObjectFile保存的RDD序列化对象，可以使用SparkContext.objectFile重新加载进来，非常简单实用。 SparkContext对象还提供了binaryFiles方法，以二进制的形式直接读取Hadoop Mapreduce计算的结果文件。 所有RDD创建方法，都在SparkContext对象中提供，完整的信息可以参考官方API文档，值得注意的一点，从外部创建RDD并不会马上读进内存进行计算，只是保存读取它们的信息，计算时通过DAG就近分配计算资源，避免移动数据，基本不用担心单点资源瓶颈。 以上就是RDD创建的几种方法，这里只是让读者有个概念，知道有这些个方法即可，真正用到的时候根据实际场景采用合适的方法，命令什么的都是细节。 RDD操作生成了RDD之后对其进行调用各种操作经过计算后得到预期的结果。这些操作分为两类: Transformation(转换) Action(动作) Spark下所有的Transformation都是一种lazy模式，计算不会马上进行，而是先记录下计算方式，Action被触发时才会启动真正的计算向Driver程序返回结果。这种模式的好处是高效，不需要将每次Transformation的非常大的结果返回Driver，只需要记录下要对RDD进行的操作，当Action触发后将计算完成的最终小很多的结果集返回给Driver，避免了大量的网络IO，数据流动，反观mapreduce，每次迭代都需要将hdfs里的数据取出来进行计算，然后写回去，下次迭代再取，再存，即使现在hadoop也支持memory-cache，尽量避免数据频繁读取，加快计算速度，但是本质上的区别使spark和mapreduce还是有着很明显的效率差异。 默认情况下除了最终结果集之外的RDD都是临时的，被Transformation或Action使用过后即丢弃，可以对RDD进行持久化或cache操作，这也会触发Transformation进行真正的计算，而且Spark会将RDD的结果保持在集群的内存或磁盘中，甚至复制多份，这样再次访问时不需要重复计算，就可以获取更快的响应速度。 举个栗子┑(￣Д ￣)┍ 上传测试文件到hdfs 第一行从hdfs目录下的文件生成一个名为ff的RDD 第二行生成一个对ff执行flatMap操作后名为words的RDD，flatMap这里的作用是将文档里所有的单词用空格分隔，生成一个连续的字符串。**没有执行实际文件操作** 第三行生成一个对words执行map操作后名为wordPairs的RDD，map在这里的作用是遍历文档里所有单词，如从 &quot;a&quot; 变成 &quot;a,1&quot;。**没有执行实际文件操作** 第四行生成一个对wordPairs执行reduceByKey操作后名为wordCounts的RDD，reduceByKey在这里的作用是将文档里 &quot;key,value&quot;相同&quot;key&quot;后面的value都累加起来，这里就已经实现了对hdfs中README.md文档进行词频统计的目标，该RDD就是期待最后返回的结果RDD，但是这里**仍然没有执行实际文件操作** 前面都是transcation操作，涵盖读取数据源生成RDD，中间调用各种函数对RDD进行转换，每次转换都生成一个新的RDD，最终用collect触发Action操作，这时候才会触发实际的文件操作，读数据到内存进行计算，最后返回一个结果集。 上面只是为了方便理解，实际没人那么写，一条命令直接搞定，完成词频统计就是这么简单干脆。 笔者同时还在学习一部小象学院的Spark视频教程，里面有详细讲解scala，具体形式是在spark环境中用scala做各种操作来演示scala的各种功能，因为Spark就是用scala开发的，编写程序也是用scala，那个scala的实际操作差不多就是spark日常操作的教程，到时候整理出来会在这里添加链接，敬请期待，不过那就是另一篇博客了O(∩_∩)O 2018/3/13 19:23:07 第三章 Spark工作机制 有生不生，有化不化。不生者能生生，不化者能化化。 本章主要内容是Spark底层的工作机制，包括调度管理、内存管理、容错机制、监控管理以及Spark程序配置管理，这对理解Spark程序的运行非常有帮助。 调度管理 普通程序运行在普通电脑上，比如QQ跑在笔记本上，由win10操作系统来进行统一调度分配可以使用的CPU，内存，硬盘资源，Spark集群运行在服务器的linux系统上，这时的资源主要是指CPU core数量和物理内存，Spark支持自身资源调度，master节点自身可以进行资源调度，同时支持与其他调度管理系统一起工作，yarn和mesos，这俩资源调度有俩好处，一是外部调度系统更强大，支持更灵活的调度策略，二是当Spark集群和其他分布式系统一起部署时，可以被统一统筹，避免Spark集群整体对资源的吞噬。 主要分为两类调度场景：主要是Spark程序之间的调度，另一个时Spark程序内部的调度。 集群名词解释 1.Driver程序 集群模式下，用户编写的Spark程序称为Driver程序，每个Driver程序包含一个代表集群环境的SparkContext对象并与之连接，程序的执行从Driver程序开始，中间过程会调用RDD操作（transcation和action），这些操作通过集群资源管理器来调度执行，一般在worker节点执行，所有操作执行结束后返回到Driver程序中，在Driver程序中结束。 2.SparkContext对象 每个驱动程序中都有一个SparkContext对象，担负着与集群沟通的职责。 sc对象联系集群管理器，分配CPU，内存等资源； 集群管理器在工作节点(worker node)上启动一个执行器； 程序代码被分发到相应的工作节点； sc分发任务(task)至各执行器执行。 3.集群管理器 集群管理器负责集群的资源调度，Spark支持三种资源管理器 standalone模式，资源管理器是master节点。这是最简单的一种集群模式，不依赖于其他系统，调度策略相对单一，只支持先进先出模式(FIFO，First-In-First-Out)。 spark部署在hadoop上，资源管理器是yarn集群。yarn支持动态资源管理，更适合多用户场景下的集群管理，而且yarn可以同时调度Spark计算和MR计算，还可以调度其他实现了yarn调度接口的集群计算，是目前最主流的一种资源管理系统 Apache Mesos，资源管理器是Mesos，是一个专门用于分布式系统资源管理的开源系统，与yarn类似，用c++开发，可以对集群中的资源作弹性管理。 还有一些需要了解的名词 Allocation。即“分配”。 •App、Application、Spark程序。泛指用户编写的运行在Spark上的程序，不仅是Scala语言编写的，其他支持的语言编写的也是。 •节点、Worker节点。集群上的计算节点，一般对应一台物理机器，仅在测试时会出现一台物理机器上启动多个节点。 •Worker。每个节点上会启动一个进程，名为Worker，负责管理本节点，运行jps命令可以看到Worker进程在运行 •core。Spark标识CPU资源的方式，对应一个或多个物理CPU核心，每个Task运行时至少需要一个core。 •执行器。每个Spark程序在每个节点上启动的一个进程，专属于一个Spark程序，与Spark程序有相同的生命周期，负责Spark在节点上启动的Task，管理内存和磁盘。如果一个节点上有多个Spark程序在运行，那么相应地就会启动多个执行器。 •Job。一次RDD Action对应一次Job，会提交至资源管理器调度执行。 •Stage。Job在执行过程中被分为多个阶段。介于Job与Task之间，是按Shuffle分隔的Task集合。 •Task。在执行器上执行的最小单元。比如RDD Transfor-mation操作时对RDD内每个分区的计算都会对应一个Task。 Spark程序之间的调度 两种分配策略，静态分配和动态分配。 静态分配指Spark程序启动时一次性分配所有资源，运行过程中固定不变直至推出。 所有集群管理器都支持静态资源分配，每个Spark程序都分配一个最大可用的资源数量，在程序运行的整个过程都持有它。这种策略简单可靠，强烈建议，除非非常确定这种分配方式无法满足需求。 动态分配指运行过程中不断调整分配的资源，可以按需增加或减少，比静态分配复杂很多，需要在实践中不断调试才能达到最优。 Spark 1.2引入了基于负载情况的集群动态资源分配，分配给Spark程序的资源可以增加或减少。这意味着我们的程序可能会在不使用资源的时候将资源还给集群，需要的时候再从集群申请。这个特性在多个程序共享集群资源的时候特别有用。如果分配给Spark程序的资源中有一部分是空闲的，它就可以返还给集群放入资源池，这样可以被其他程序请求使用。在Spark中，动态资源分配的粒度是执行器，即增加或减少执行器。前面已经介绍过，Spark程序在机器的每个节点上只有一个执行器，所以增加或减少执行器意味着为Spark程序服务的节点数据量的增加或减少。通常，每台机器启动一个节点进程，但也不排除启动多个的情况发生。你可以通过设置spark.dynamicAllocation.en-abled来启动这个功能。目前只有在yarn模式下可以使用，未来的版本也支持standalone和mesos模式。 Spark程序内部的调度 Spark程序内部，不同线程提交的Job可以并行执行，调度器是线程安全的，因此可以支持这种需要同时处理多个请求的服务型应用。 默认情况下，调度器以FIFO的方式运行Job，每个Job分成多个stage(如map和reduce阶段)，如果最前面Job的Stage有task要运行，优先获取所有资源，然后才是第二个Job，依此类推。如果队列中第一个Job不需要太多资源，那么第二个Job可以马上运行，但如果第一个Job特别大，则后面的Job会明显延迟。 从Spark0.8开始，可以配置Job间公平共享资源，在公平共享方式下，采用”循环”(round robin)方式为不同Job之间的task分配资源，这样所有的Job可以获得差不多相同的资源，意味着有长时间的Job运行的情况下，短的Job也可以在提交后马上运行，不用等待长Job结束，特别适合多用户场景。 开启程序内公平调度，只需要在sc中设置spark.scheduler.mode值为FAIR。 val conf = new SparkConf().setMaster(...).setAppName(...) conf.set(&quot;spark.scheduler.mode&quot;, &quot;FAIR&quot;) val sc = new SparkContext(conf) 公平调度还支持对多个Job进行分组，这个分组称为调度池，每个调度池可以设置不同的调度选项，比如想要为一些更重要的Job设置更高的优先级，比如为不同的用户设置不同的资源池，让各个资源池平等共享资源，而不是按Job来共享资源，不做设置的话，新提供的Job会自动进入默认调度池。我们可以指定让Job进入哪个调度池，具体方法是提交任务的线程在SparkContext中设置spark.scheduler.pool，这样该线程提交的所有Job都会使用这个调度池。设置按照线程来进行调度，可以很方便地让一个线程地所有Job在一个用户下。 sc.setLocalProperty(&quot;spark.scheduler.pool&quot;, &quot;pool1&quot;) sc.setLocalProperty(&quot;spark.scheduler.pool&quot;, null) 调度池的配置可以放在配置文件conf/fairscheduler.xml.template中，在sc中指定配置文件，每个调度池有如下三个属性。 schedulingMode，可以FIFO或FAIR，用于控制调度池内的Job是排队执行还是平均共享资源 weight，用于控制调度池相对于其他调度池的权重，是一个相对值。所有调度池的默认权重都是1，平均共享集群资源，如果某个调度池的是2，那么理论上它可以调用权重为1的资源池两倍的资源。设置一个非常高的权重，比如1k，相当于配置了各个调度池的优先级。实际上，权重为1k的调度池只要有一个Job就会优先启动。 minShare，设置最小资源值，默认值为0，公平调度器在按权限分配之前，会满足各个资源池的最小资源值，这样可以保证调度池总会获得一些资源，不会被其他高权重的调度池抢光。 。配置文件中没有出现的调度池都被设置为默认值（schedulingMode为FIFO，weight为1，minShare为0）。 &lt;allocations&gt; &lt;pool name=&quot;production&quot;&gt; &lt;schedulingMode&gt;FAIR&lt;/schedulingMode&gt; &lt;weight&gt;1&lt;/weight&gt; &lt;minShare&gt;2&lt;/minShare&gt; &lt;/pool&gt; &lt;pool name=&quot;test&quot;&gt; &lt;schedulingMode&gt;FIFO&lt;/schedulingMode&gt; &lt;weight&gt;2&lt;/weight&gt; &lt;minShare&gt;3&lt;/minShare&gt; &lt;/pool&gt; &lt;/allocations&gt; 内存管理 相比MR，spark具有巨大的性能优势，很大一部分原因是spark对内存的充分利用和缓存机制。 RDD持久化 如果一个RDD不止一次被用到，那就可以持久化它，将RDD缓存到内存中，内存不够时用磁盘顶上去，这样可以大幅提升程序性能，十倍甚至更多。 默认情况下，RDD只使用一次，用完即扔，再次使用需要重新计算得到，而持久化操作避免了重复计算，这就是Spark刚出现时被人称作内存计算的原因。 持久化的方法是调用persist()函数，RDD.unpersist()可以删除持久化。默认只持久化到内存中，可以在调用持久化函数时添加参数设置存储级别，有memory_and_disk，memory_only_ser，memore_and_disk_ser,disk_only，其中memory_only_ser类似默认的memory_only，格式是序列化后的数据，节省内存更消耗CPU 容错机制 分布式系统通常运行在一个机器集群，同时运行的几百台机器中有机器发生故障的概率大大增加，容错设计是分布式系统的重要能力。 Spark以前的集群容错机制，如MR，将计算转换为一个有向无环图(DAG)的任务集合，通过重复执行DAG里的一部分任务来完成容错。由于主要的数据存储在分布式文件系统中，没有提供其他存储的概念，容错过程需要在网络上进行数据复制，增加了大量的消耗，所以分布式编程经常需要做检查点，将某个时刻的中间数据写到存储中。 RDD处理过程也是一个DAG，每个RDD都会记住创建该数据集需要的操作，自己是由哪个RDD转化而来，这个继承关系叫lineage(血统)，由于创建RDD的操作是相对粗粒度的变换(如map，filter，join)，很多单一的操作应用于许多数据元素，不需要存储真正的数据，当RDD的某个分区丢失时，RDD有足够的信息记录其如何通过其他RDD进行计算，且只需要计算该分区。 lineage不是完美的，因为RDD之间的依赖有两种，父分区对应多个子分区的宽依赖和父分区对应一个子分区的窄依赖。 对于窄依赖，只需要重新计算丢失的那一块数据，如map，filter，union，分区相同的join，容错成本较小，但宽依赖容错重算分区时，就会有大量冗余计算。 所以不同的应用有时需要使用doCheckPoint适当设置数据检查点，RDD的只读特性使得它很容易做检查点。 某些场景中容错要求更高更复杂，比如计费服务要求零丢失，流计算应用场景系统上游不断产生数据，容错过程会造成数据丢失。为了解决这些问题，spark又提供了预写日志，先将数据写入支持容错的文件系统，然后对数据进行操作。 这样，容错机制包括从检查点重新计算恢复，从日志恢复，从数据源重发，实现了零丢失。 Master使用zookeeper容错，在spark-env.sh中添加选项，设置master的恢复模式为zookeeper，设置zookeeper的集群地址和用于恢复的zookeeper目录。或者采用一种更简单的方式，设置恢复模式为filesystem并设置一个恢复目录，该目录会存储必要的恢复信息，当master进程异常时，重启master进程即可。 slave节点运行着worker，执行器，Driver程序。 Worker异常停止时，会先将自己启动的执行器停止，Driver需要有相应的程序来重启Worker进程。 执行器异常退出时，Driver没有在规定时间内收到执行器的StatusUpdate，于是Driver会将注册的执行器移除，Worker收到LaunchExecutor指令，再次启动执行器。 Driver异常退出时，一般要使用检查点重启Driver，重新构造上下文并重启接收器。第一步，恢复检查点记录的元数据块。第二步，未完成作业的重新形成。由于失败而没有处理完成的RDD，将使用恢复的元数据重新生成RDD，然后运行后续的Job重新计算后恢复。 监控管理 可以用多种方式来监控Spark程序的运行状况:web界面，metrics，外部系统。 web:每个Driver的SparkContext都会启动一个Web界面，默认端口是4040，用于显示程序的许多有用信息，包括：调度器Stage、Task列表，RDD大小和内存使用统计概况，环境信息，正在运行的执行器信息。浏览器输入http://xxx.xxx.xxx.xxx:4040即可访问， 自定义:开发人员可以通过JSON接口自己开发可视化展示形式，历史服务器和正在运行的spark程序的web界面都支持rest api，所有入口地址都有严格的版本控制，spark保证: 这些地址永远不会在某个版本中删除，任何地址中提供的每个字段都不会被删除； 可能添加新的入口地址； 现有入口地址可能添加新的字段； 每个入口地址在未来都可能添加新版本的API（比如api/V2），新版本不要求向下兼容老版本； API版本可能被废弃，但前提是新的版本可以与之共存，而且新版本不只是发布主版本，至少有一个次版本发布。 metrics指标体系:spark支持基于Coda Hale Metrics Library的指标体系，可以主动将运行状态发送给其他系统，方便与其他监控系统进行集成，比如Ganglia。Spark支持的Metrics实例有Master、Applications、Worker、执行器和Driver，支持的接收者类型包含在org.apache.spark.metrics.sink包中，包括ConsoleSink、CSVSink、JmxSink、MetricsServlet、GraphiteSink和Slf4jSink。 其他监控工具:集群级别可以使用各种工具来监控各节点的CPU，网络，磁盘等负载情况，如zabbix，nagios，ganglia运维工具，操作系统级别可以使用dstat，iostat，iotop，top等linux工具对单点问题进行定位。 Spark程序配置管理 spark配置文件默认模式是${SPARK_HOME}/conf，可更改的配置项自行按需操作。 spark属性项的配置可以在三个地方进行设置，优先级依次是 sc对象，命令行参数，spark-defaults.conf。 可以在spark-defults.conf文件里面可以设置spark运行的诸多参数 可以在用命令行./bin/spark-submit提交程序时重新指定参数， ./bin/spark-submit --name &quot;My app&quot; --master local[4] --conf spark.shuffle.spill=false --conf &quot;spark.executor.extraJavaOptions=-XX:+PrintGCDetails -XX:+PrintGCTimeStamps&quot; myApp.jar 在开发spark程序时可以把参数写在代码里，sc对象的优先级最高，不过灵活性差了点，改参数就得改代码。 val conf = new SparkConf() .setMaster(&quot;local[2]&quot;) .setAppName(&quot;CountingSheep&quot;) .set(&quot;spark.executor.memory&quot;, &quot;1g&quot;) val sc = new SparkContext(conf) 在web界面的environment选项里可以看到所有配置的spark属性 日志配置是另一项单独的配置，使用配置文件目录下的log4j.properties作为配置文件。 2018/3/14 20:19:48 第四章 Spark内核讲解 深入Spark内核，并结合源码，介绍了核心结构RDD、RDD对象的Transformation和Action操作是如何实现的、SparkContext对象及初始化过程、DAG调度的工作流程。了解这些内容可以帮助读者编写出高质量的Spark程序代码。 第五章 Spark SQL和数据仓库 介绍Spark SQL，可以代替Hive，用于搭建一个企业级的数据仓库。案例基于淘宝的电商数据建立电商数据仓库，并以日常运营工作为例，通过电商数据库分析电商运营中的各类问题。 第六章 Spark 流式计算 章介绍Spark实时流式计算，类似于Storm，但吞吐量方面更有优势。案例是基于一个站点的Web日志建立一个类似百度统计的实时统计系统，是各种实时系统典型的参考例子。 第七章 Spark 图计算 介绍Spark的图计算。案例基于新浪微博2000万的关系链数据，讲解了如果利用图计算来实现社交关系链的挖掘，比如闺蜜的发现、粉丝群体的发现等。 第八章 Spark MLlib 介绍Spark的机器学习库。案例基于某个搜索引擎的点击日志，建立了一个搜索广告点击率预估系统。广告点击率预估是各家互联网系统的核心系统，公开的实战项目不多。 二级目录 三级目录]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>Book Summary-1</tag>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark深入]]></title>
    <url>%2F2018%2F02%2F04%2FSpark%E6%B7%B1%E5%85%A5%2F</url>
    <content type="text"><![CDATA[简介 Spark是一个解决大规模数据集运算分析的计算引擎，知其然更要知其所以然，深入了解编程模型，RDD原理，解析spark内核，能够让我们在一个更高的角度看程序运行，数据流动，而且现在对批处理有了其他的需求，比如sturcture stream实现的流处理，MLib实现的机器学习的分类聚类算法，对这种更高层次的spark应用的学习有助于更好的发挥spark的潜力，安装配置不值一提，具体应用才是画龙点睛。 spark 教程 讲的很透彻，已经刷了两遍，跟着做了一些实际操作，每一遍都有新的理解，学得越多越觉得自己懂得太少，奋斗吧少年！ 深坑待填，关键是理论性东西写起来真的墨迹啊 scala基础与实践 scala是一门多范式的编程语言，设计初衷是要集成面向对象编程和函数式编程的各种特性。scala用java开发，运行在java虚拟机上并兼容java程序，spark就是用scala开发的，用scala写spark的程序极为合适，其次是python和java。 不去学具体的使用技巧，不了解程序的内部原理，那么spark就只是一个软件，深入研究并自己写程序去测试spark的性能，它才会成为数据操控者手中的利剑，剑锋所指，一切真相无所遁形！额(⊙﹏⊙)，说人话就是:”用熟了之后做数据挖掘能手脚麻利一点”，hiahiahia~~~ 二级目录 三级目录 spark编程模型 深入spark内核 spark streaming spark上运行机器学习，算法实现 shark多语言编程]]></content>
      <categories>
        <category>Bigdata</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>Framework</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark实践]]></title>
    <url>%2F2018%2F02%2F02%2FSpark%E5%AE%9E%E8%B7%B5%2F</url>
    <content type="text"><![CDATA[简介Apache Spark 是专为大规模数据处理而设计的快速通用的计算引擎,一个用来实现快速而通用的集群计算的平台,扩展了广泛使用的MapReduce计算模型，能高效支持更多的计算模式，包括交互式查询和流处理。在处理大规模数据集的时候有优势，Spark的一个重要特点就是能够在内存中计算，因而比MapReduce更快，即使在磁盘上进行的复杂计算，Spark依然更加高效。 软件 下载地址 jdk-8u161-linux-x64.tar.gz 点击下载 hadoop-2.7.2.tar.gz 点击下载 scala-2.11.7.tgz 点击下载 spark-2.2.0-bin-hadoop2.7.tgz 点击下载 hadoop-native-64-2.7.0.tar 点击下载 服务器 节点 192.168.4.50 SparkMaster 192.168.4.237 SparkWorker1 192.168.4.238 SparkWorker2 192.168.4.48 SparkWorker3 192.168.4.49 SparkWorker4 Hadoop 运行环境配置 请务必关闭防火墙，包括iptables及selinux java 三台虚拟机均需配置java环境，下文所示环境是包括hadoop，scala，spark软件在内的环境变量最终形态，如果新玩家要入手，直接配置成这样纸，后面的软件注意存放在指定路径即可。 $ tar xvf jdk-8u161-linux-x64.tar.gz -C /usr/local/ $ vi /root/.bash_profile $ source /root/.bash_profile export PATH export JAVA_HOME=/usr/local/jdk1.8.0_161 export JRE_HOME=$JAVA_HOME/jre export CLASSPATH=$JAVA_HOME/lib:$JRE_HOME/lib export SCALA_HOME=/usr/local/scala-2.11.7 export SPARK_HOME=/usr/local/spark-2.2.0-bin-hadoop2.7 export PATH=$JAVA_HOME/bin:/usr/local/hadoop-2.7.2/bin:/usr/local/hadoop-2.7.2/sbin:$SCALA_HOME/bin:$SPARK_HOME/bin:$PATH export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native export HADOOP_OPTS=&quot;-Djava.library.path=$HADOOP_HOME/lib&quot; 主机名称 各节点均要更改，图为主节点示例，需要修改/etc/hosts，/etc/sysconfig/network两个文件，注意hosts文件各节点一致，network文件各节点需分别修改成自己的hostname ssh 在各虚拟机上执行命令生成密钥 # ssh-keygen ssh将各个机器的密钥拿来主节点（以下语句皆在SparkMaster上执行）合成authorized_keys，然后分发到各个节点 $ ssh root@192.168.4.237 cat /root/.ssh/id_rsa.pub &gt;&gt; /root/.ssh/authorized_keys 237替换成238，50，48，49依次执行，完成后会在主节点生成/root/.ssh/authorized_keys,是一份包含了五台机器公钥信息的文件 $ scp authorized_keys root@192.168.4.237:/root/.ssh/ 237替换成238，50，48，49依次执行，将包含了五台服务器的公钥文件分发到所有服务器，所有节点之间都可以无密码ssh操作 配置互信的原因:namenode(即主节点)如果没有对datanode(即数据节点)的ssh免登陆权限，那么namenode起每个datanode的服务都需要输入密码，多节点的情况极为影响效率，所以namenode与datanode间必须配置互信，而datanode之间没有ssh通信的需求，所以上文是一种绝对可行但相对麻烦和不必要的操作，如果多节点的大规模hadoop配置互信建议使用下面这种ssh自带的方法，可以使用类似ansible这样的工具批量执行。 $ ssh-keygen 生成密钥 $ ssh-copy-id -i /root/.ssh/id_rsa.pub root@192.168.4.50 把datanode生成的公钥发送到namenode，namedode拥有所有datanode的公钥即可，datanode不必持有其他datanode或者namenode的公钥 软件包解压 在namenode解压并修改配置文件，用rsync把修改后的/usr/local/hadoop-2.7.2文件夹同步到其他节点 $ tar xvf hadoop-2.7.2.tar.gz -C /usr/local/ $ mkdir -p {tmp,hdfs/{data,name}} tmp用来存储临时生成的文件 hdfs用来存储集群数据 hdfs/data用来存储真正的数据 hdfs/name用来存储文件系统元数据 修改配置文件 下列文件绝对路径:/usr/local/hadoop-2.7.2/etc/hadoop/ hadoop-env.sh 将 export JAVA_HOME=${JAVA_HOME} 改成 export JAVA_HOME=/usr/local/jdk1.8.0_161 core-site.xml &lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://SparkMaster:9000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;file:/usr/local/hadoop-2.7.2/tmp&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;io.file.buffer.size&lt;/name&gt; &lt;value&gt;131072&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; mapred-site.xml $ cp mapred-site.xml.template mapred-site.xml &lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt; &lt;value&gt;SparkMaster:10020&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt; &lt;value&gt;SparkMaster:19888&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; hdfs-site.xml &lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;file:/usr/local/hadoop-2.7.2/hdfs/name&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;file:/usr/local/hadoop-2.7.2/hdfs/data&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;2&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;SparkMaster:9001&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.webhdfs.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; yarn-site.xml &lt;configuration&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.address&lt;/name&gt; &lt;value&gt;SparkMaster:8032&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.scheduler.address&lt;/name&gt; &lt;value&gt;SparkMaster:8030&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.resource-tracker.address&lt;/name&gt; &lt;value&gt;SparkMaster:8031&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.admin.address&lt;/name&gt; &lt;value&gt;SparkMaster:8033&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.webapp.address&lt;/name&gt; &lt;value&gt;SparkMaster:8088&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; slaves SparkWorker1 SparkWorker2 SparkWorker3 SparkWorker4 所有节点的文件都需要是经过修改后的状态，利用rsync将namenode修改过的文件夹直接同步到其他节点 $ rsync -av /usr/local/hadoop-2.7.2 SparkWorker1:/usr/local/hadoop-2.7.2 SparkWorker1替换成slaves里其他节点依次执行，所有节点都有了修改过配置文件的hadoop文件夹后集群准备工作完成。 启动并验证服务 hadoop namenode -format 格式化 start-all.sh 或者 start-dfs.sh &amp;&amp; start-yarn.sh 每个节点都有完整的配置文件和命令，任何节点都可以执行集群的起停操作，都可以成功启动hdfs和yarn jps 查看各个节点进程状态 hadoop集群已经正常启动。 简单测试 $ hadoop fs -put /tmp/input.txt / 上传待处理文件到hdfs中 $ hadoop jar /usr/local/hadoop-2.7.2/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar wordcount /input.txt /output.txt 调用安装包自带jar包对hdfs中存储的待处理文件执行词频统计，这里就是诸位大数据开发工程师大显身手的地方，写好的程序打成jar包放在这里供hadoop调用，examples是hadoop自带练手用的测试包。 $ hadoop fs -ls /output.txt 可以看到已经生成了一个包含着若干文件的output.txt文件夹，存放着调用jar包函数后输出的处理结果，给文件夹起output.txt这种名字，皮一下就很开心 $ hadoop fs -text /output.txt/part-r-00000 Scala 安装包 $ tar xvf /tmp/scala-2.11.7.tgz -C /usr/local 上传解压安装包到指定目录 环境变量 $ vi /root/.bash_profile 添加 /usr/local/scala-2.11.7/bin 到PATH路径中 同步并验证 scala -version 一个词，rsync，有点灵性啊同学们! Spark 安装包 $ tar xvf /tmp/spark-2.2.0-bin-hadoop2.7.tgz -C /usr/local 环境变量及配置文件 $ vi /root/.bash_profile 添加 /usr/local/spark-2.2.0-bin-hadoop2.7/bin到环境变量 spark-env.sh export JAVA_HOME=/usr/local/jdk1.8.0_161 export SCALA_HOME=/usr/scala-2.11.7 export HADOOP_HOME=/usr/local/hadoop-2.7.2 export HADOOP_CONF_DIR=/usr/local/hadoop-2.7.2/etc/hadoop export SPARK_MASTER_IP=SparkMaster export SPARK_WORKER_MEMORY=4g export SPARK_WORKER_CORES=2 export SPARK_WORKER_INSTANCES=1 slaves 同步并验证 将修改过配置文件的文件夹全部同步到数据节点 $ start-dfs.sh spark只使用hdfs文件系统，并不用启动所有功能 $ /usr/local/spark-2.2.0-bin-hadoop2.7/sbin/start-all.sh 启动spark集群，建议使用绝对路径 成功启动后使用jps可以在namenode看到 30049 Jps 29655 ResourceManager 29501 SecondaryNameNode 29311 NameNode 可以在datanode看到 4208 Jps 3937 DataNode 4042 NodeManager 至此spark集群全部搭建完成 可以通过spark的webui界面访问控制台 可以通过spark-shell执行各种操作，执行transcation action任务，对存放在hdfs里的数据集进行分词，统计，排序 可以执行用python，scala，java等语言打包好的jar包程序 ENJOY IT! 遇到的坑 1.请务必关闭防火墙，包括iptables和selinux，否则会导致集群无法通信，页面无法访问等报错，如java.net.NoRouteToHostException:没有找到主机的路由 2.启动后有警告Unable to load native-hadoop library for your platform... using builtin-java classes where applicable，因为Apache提供的hadoop本地库是32位的，而在64位的服务器上就会有问题，因此需要自己编译64位的版本，解决方案是找个包，解压到目录并修改环境变量，操作后问题解决，原作者环境跟我的环境不一样，不确定是不是所有人都会碰到这个问题 $ tar -xvf hadoop-native-64-2.7.0.tar -C /usr/local/hadoop-2.7.2/lib/native $ tar -xvf hadoop-native-64-2.7.0.tar -C /usr/local/hadoop-2.7.2/lib 3.第一次启动后进程都正常，页面无法访问，查看监听状态发现监听到了ipv6的端口，hadoop的配置文件明明标注的是监听到ipv4的端口，解决方案如下。 vi /etc/sysctl.conf 添加以下内容并执行stop-all.sh和start-all.sh重启hadoop服务 #disable ipv6 net.ipv6.conf.all.disable_ipv6 = 1 net.ipv6.conf.default.disable_ipv6 = 1 net.ipv6.conf.lo.disable_ipv6 = 1 4.数据节点jps没有datanode进程的情况 多次format文件系统可能会出现datanode无法启动的情况，解决方案是删除数据节点的hdfs及tmp目录，重新启动即可。 原因是执行hadoop namenode -format后会在namenode的/usr/lcoal/hadoop-2.7.2/hdfs/name/current/文件夹下生成记录了namespaceID，clusterID及blockpoolID的VERSION文件，然后执行start-dfs.sh命令启动hdfs，这时datanode的/usr/local/hadoop-2.7.2/hdfs/data/文件夹下会生成记录了集群信息的文件。 hadoop启动关闭hdfs时需要读取namenode及datanode的集群信息，二次format的时候namenode的相关信息会自动更新，但datanode的原有信息不会自动删除，启动的时候namenode优先读取该节点原有信息，这样本来准备起datanode，一看有记录，要写入的信息不写了，按原有记录来，结果原有记录的信息是format之前集群的信息，hadoop以为那是其他集群的数据节点，自然不会带起来datanode进程。]]></content>
      <categories>
        <category>Bigdata</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>Framework</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NoSQL简介]]></title>
    <url>%2F2018%2F01%2F23%2FNoSQL%2F</url>
    <content type="text"><![CDATA[简介 本文主要参考菜鸟教程，原博文清晰明了，可以自行前往，写在这里是以手连心加深印象，同时把这些信息抓回来作为以博客为载体的个人技能图的重要组成部分。 NoSQL(NoSQL = Not Only SQL )，意即”不仅仅是SQL”。 现代计算系统上每天网络上都会产生庞大的数据量，这些数据很大一部分是由关系数据库RDBMS处理，实践证明关系模型非常适合于客户端服务器编程，是结构化数据存储在网络和商务应用的主导技术。 NoSQL是另一种数据组织形式，早期提出至09年趋势高涨，NoSQL相关的非关系型数据库在存储处理当前互联网快速产生，日益庞大，形式多样的数据方面有巨大优势，发展迅速。 NoSQL简史 NoSQL一词最早出现在1998年，是Carlo Strozzi开发的一个轻量，开源，不提供SQL功能的关系数据库； 2009年，有一次关于分布式开源数据库的讨论，有人再次提出了NoSQL的概念，主要指非关系型，分布式，不提供ACID的数据库设计模式； 2009年，亚特兰大举行的”no:sql(east)”讨论会是一个里程碑，其口号”select fun, profit from real_world where relational = false”，因此对NoSQL最普遍的解释是”非关系型的”，强调key-values stores和文档数据库的优点，而不是单纯的反对RDBMS。 时至今日，各种NoSQL数据库已经是大数据领域各种数据分布式存储的基石。 NoSQL数据库分类 类型 代表数据库 特点 列存储 Hbase/Cassandra/Hypertable 按列存储，方便存储结构化和半结构化数据，方便做数据压缩，在某一列或者某几列的查询中有巨大的IO优势 文档存储 MongoDB/CouchDB 用类似json的格式存储文档，可以对某些字段建立索引，实现关系数据库的某些功能 key-value存储 Redis/MemcacheDB 存储的是key-value对，不管value的格式，可以通过key快速查询value 图存储 Neo4J/FlockDB 图形关系的最佳存储方案 对象存储 db4o/Versant 通过对象的方式存储数据，用类似面向对象语言的语法操作数据库 xml数据库 BaseX/Berkeley XML DB 高效存储XML数据，支持XML内部查询语法，比如XQuery，Xpath RDBMS和NoSQL的规则 ACID 关系型数据库有一个很重要的概念:事务(transaction)，类似现实中的交易，遵循ACID: A-Atomicity 原子性 事务要么不做，要么做完，事务成功的条件是事务里所有操作均成功完成，只要有一个操作失败，整个事务便失败回滚。比如银行转账，A账户转B账号，分为两个步骤:1.从A账号取出100元2.往B账号转入100元银行的数据库处理该事务，要么两步一起完成，事务提交，任何一步出问题事务都会回滚，如果第一步未完成那么事务失败，第二步未完成，B没收到钱但A少了100元，事务会回滚到未转账状态。 C-Consistency 一致性 数据库一直处于一致的状态，事务的运行不会改变原本的一致性约束。例如某表上有完整性约束”a + b = 10”，如果一个事务改变了a，那么b必定会发生改变，使得事务结束后依然满足该约束，否则事务失败 I-Isolation 独立性 并发的事务之间不会互相影响，如果A事务访问的数据正在被B事务修改，只要B事务未提交，A访问的数据就不会受到影响 D-Durability 持久性 一旦事务提交，所有的修改将会永久保存在数据库上，即使宕机也不会丢失，除非进行数据库恢复 BASE Basically Available, Soft-state, Eventually Consistent Basically Availble –基本可用 Soft-state –软状态/柔性事务。 “Soft state” 可以理解为”无连接”的, 而 “Hard state” 是”面向连接”的 Eventual Consistency –最终一致性 最终一致性， 也是是 ACID 的最终目的。 CAP 计算机科学中， CAP定理 揭示了分布式系统的本质，并给出了设计准则，它指出任何分布式计算系统不能同时满足一致性(Consistency) - 所有节点在同一时间具有相同的数据可用性(Availability) - 保证每个请求都有响应，无论响应成功还是失败分区容错性(Partition tolerance) - 系统中任意信息的丢失或失败不会影响系统的继续运作 CA without P,如果不要求P(分区)，那么CA(强一致性和可用性)是可以保证的，但分区是始终存在的问题，因此CA系统更多的是允许分区后各子系统依然保证CA。 CP without A，如果不要求A，相当于每个请求都需要server之间强一致，而P会导致时间无限延长，因此CP也是可以保证的，很多传统的数据库分布式事务都属于这种模式 AP without C，要高可用并允许分区，则需放弃一致性，一旦分区发生，节点间可能失去联系，为了高可用，每个节点只能用本地数据提供服务，而这会导致全局数据的不一致性，众多NoSQL数据库都属于此类 因此，根据CAP原理将数据库分成满足CA,CP,AP的三大类: 为什么用NoSQL RDBMS 高度组织化结构化数据 结构化查询语言（SQL） (SQL) 数据和关系都存储在单独的表中。 数据操纵语言，数据定义语言 严格的一致性 基础事务 NoSQL 代表着不仅仅是SQL 没有声明性查询语言 没有预定义的模式 键 - 值对存储，列存储，文档存储，图形数据库 最终一致性，而非ACID属性 非结构化和不可预知的数据 CAP定理 高性能，高可用性和可伸缩性 优点: 1.高可扩展性 2.分布式计算 3.低成本 4.架构灵活，可以处理半结构化/非结构化数据 5.不用处理复杂的关系模型 缺点: 1.没有关系型数据库发展得那么成熟 2.查询功能有限 3.是一种最终一致性的系统，它们为了高的可用性牺牲了一部分的一致性 可以通过第三方平台（如：Google,Facebook等）可以很容易的访问和抓取数据。用户的个人信息，社交网络，地理位置，用户生成的数据和用户操作日志已经成倍的增加。我们如果要对这些用户数据进行挖掘，那SQL数据库已经不适合这些应用了, NoSQL数据库的发展也却能很好的处理这些大的数据。]]></content>
      <categories>
        <category>NoSQL</category>
      </categories>
      <tags>
        <tag>Theory</tag>
        <tag>Database</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mongodb补充]]></title>
    <url>%2F2018%2F01%2F19%2Fmongodb%E8%A1%A5%E5%85%85%2F</url>
    <content type="text"><![CDATA[简介MongoDB 是一个由 C++ 语言编写的基于分布式文件存储的数据库，旨在为 WEB 应用提供可扩展的高性能数据存储解决方案，这里简要介绍一下基本操作和存储原理，勉强能够应付日常工作。如果主要工作环境和业务都是mongo，那么就需要深入研究了，本文是远远不够的。 P1（基础阶段） 增 db.COLLECTION_NAME.insert(document) 例: db.col.insert({title: &apos;MongoDB 教程&apos;, description: &apos;MongoDB 是一个 Nosql 数据库&apos;, by: &apos;菜鸟教程&apos;, url: &apos;http://www.runoob.com&apos;, tags: [&apos;mongodb&apos;, &apos;database&apos;, &apos;NoSQL&apos;], likes: 100 }) 删 db.collection.remove(query,justOne) db.col.remove({&apos;title&apos;:&apos;MongoDB 教程&apos;}) db.col.remove({}) db.col.find() 改 db.collection.update( , , { upsert: , multi: , writeConcern: }) 例: db.col.update({‘title’:’MongoDB 教程’},{$set:{‘title’:’MongoDB’}}) db.col.update({‘title’:’MongoDB 教程’},{$set:{‘title’:’MongoDB’}},{multi:true}) db.col.save({ “_id” : ObjectId(“56064f89ade2f21f36b03136”), “title” : “MongoDB”, “description” : “MongoDB 是一个 Nosql 数据库”, “by” : “Runoob”, “url” : “http://www.runoob.com“, “tags” : [ “mongodb”, “NoSQL” ], “likes” : 110}) 查 MongoDB 查询文档使用 find() 方法,以非结构化的方式来显示所有文档。 db.collection.find(query, projection) 可以使用where，大于等于小于，与或非等条件进行查询，具体语句自行查找 排序 db.COLLECTION_NAME.find().sort({KEY:1}) db.col.find({},{&quot;title&quot;:1,_id:0}).sort({&quot;likes&quot;:-1}) 索引 db.COLLECTION_NAME.ensureIndex({KEY:1}) db.col.ensureIndex({&quot;title&quot;:1}) 聚合 MongoDB中聚合(aggregate)主要用于处理数据(诸如统计平均值,求和等)，并返回计算后的数据结果。有点类似sql语句中的 count(*)。 db.COLLECTION_NAME.aggregate(AGGREGATE_OPERATION) db.mycol.aggregate([{$group : {_id : &quot;$by_user&quot;, num_tutorial : {$sum : 1}}}]) mongo集群搭建 大概就是副本集，主从，sharding，目的是将数据同步到多个服务器，复制提供数据的冗余备份，从而提高了数据的可用性， 并保证数据的安全性，主节点记录在其上的所有操作oplog，从节点定期轮询主节点获取这些操作，然后对自己的数据副本执行这些操作，从而保证从节点的数据与主节点一致。 mongo备份恢复 在另一篇博客里有应用场景的命令格式，大概就是用mongodump和mongorestor两个命令实现数据导出到备份集和导入 mongo监控 mongostat命令是mongo自带的状态检测工具，可以间隔固定时间获取并输出mongodb的当前运行状态，数据库变慢或者有异常的首选操作 P2(稍高一点) mongodb map reduce Map-Reduce是一种计算模型，简单的说就是将大批量的工作（数据）分解（MAP）执行，然后再将结果合并成最终结果（REDUCE），MongoDB提供的Map-Reduce非常灵活，对于大规模数据分析也相当实用 以下是MapReduce的基本语法： &gt;db.collection.mapReduce( function() {emit(key,value);}, //map 函数 function(key,values) {return reduceFunction}, //reduce 函数 { out: collection, query: document, sort: document, limit: number } ) 使用 MapReduce 要实现两个函数 Map 函数和 Reduce 函数,Map 函数调用 emit(key, value), 遍历 collection 中所有的记录, 将 key 与 value 传递给 Reduce 函数进行处理。 Map 函数必须调用 emit(key, value) 返回键值对。 参数说明: map ：映射函数 (生成键值对序列,作为 reduce 函数参数)。 reduce 统计函数，reduce函数的任务就是将key-values变成key-value，也就是把values数组变成一个单一的值value。。 out 统计结果存放集合 (不指定则使用临时集合,在客户端断开后自动删除)。 query 一个筛选条件，只有满足条件的文档才会调用map函数。（query。limit，sort可以随意组合） sort 和limit结合的sort排序参数（也是在发往map函数前给文档排序），可以优化分组机制 limit 发往map函数的文档数量的上限（要是没有limit，单独使用sort的用处不大） 以下实例在集合 orders 中查找 status:”A” 的数据，并根据 cust_id 来分组，并计算 amount 的总和。 操作实例 准备数据集 db.posts.insert({ &quot;post_text&quot;: &quot;菜鸟教程，最全的技术文档。&quot;, &quot;user_name&quot;: &quot;mark&quot;, &quot;status&quot;:&quot;active&quot; }) WriteResult({ &quot;nInserted&quot; : 1 }) db.posts.insert({ &quot;post_text&quot;: &quot;菜鸟教程，最全的技术文档。&quot;, &quot;user_name&quot;: &quot;mark&quot;, &quot;status&quot;:&quot;active&quot; }) WriteResult({ &quot;nInserted&quot; : 1 }) db.posts.insert({ &quot;post_text&quot;: &quot;菜鸟教程，最全的技术文档。&quot;, &quot;user_name&quot;: &quot;mark&quot;, &quot;status&quot;:&quot;active&quot; }) WriteResult({ &quot;nInserted&quot; : 1 }) db.posts.insert({ &quot;post_text&quot;: &quot;菜鸟教程，最全的技术文档。&quot;, &quot;user_name&quot;: &quot;mark&quot;, &quot;status&quot;:&quot;active&quot; }) WriteResult({ &quot;nInserted&quot; : 1 }) db.posts.insert({ &quot;post_text&quot;: &quot;菜鸟教程，最全的技术文档。&quot;, &quot;user_name&quot;: &quot;mark&quot;, &quot;status&quot;:&quot;disabled&quot; }) WriteResult({ &quot;nInserted&quot; : 1 }) db.posts.insert({ &quot;post_text&quot;: &quot;菜鸟教程，最全的技术文档。&quot;, &quot;user_name&quot;: &quot;runoob&quot;, &quot;status&quot;:&quot;disabled&quot; }) WriteResult({ &quot;nInserted&quot; : 1 }) db.posts.insert({ &quot;post_text&quot;: &quot;菜鸟教程，最全的技术文档。&quot;, &quot;user_name&quot;: &quot;runoob&quot;, &quot;status&quot;:&quot;disabled&quot; }) WriteResult({ &quot;nInserted&quot; : 1 }) db.posts.insert({ &quot;post_text&quot;: &quot;菜鸟教程，最全的技术文档。&quot;, &quot;user_name&quot;: &quot;runoob&quot;, &quot;status&quot;:&quot;active&quot; }) WriteResult({ &quot;nInserted&quot; : 1 }) 在 posts 集合中使用 mapReduce 函数来选取已发布的文章(status:”active”)，并通过user_name分组，计算每个用户的文章数 db.posts.mapReduce( function() { emit(this.user_name,1); }, function(key, values) {return Array.sum(values)}, { query:{status:&quot;active&quot;}, out:&quot;post_total&quot; } ) 结果如下 结果表明，共有 5 个符合查询条件（status:”active”）的文档， 在map函数中生成了 5 个键值对文档，最后使用reduce函数将相同的键值分为 2 组。 用类似的方式，MapReduce可以被用来构建大型复杂的聚合查询。 Map函数和Reduce函数可以使用 JavaScript 来实现，使得MapReduce的使用非常灵活和强大。 以小见大，mongo集群的存储性能，可扩展性，对mapreduce的支持，使得分布式集群搭建后运行各种数据分析挖掘算法来处理海量数据有了可靠的保证。 mongodb 全文检索 全文检索对每一个词建立一个索引，指明该词在文章中出现的次数和位置，当用户查询时，检索程序就根据事先建立的索引进行查找，并将查找的结果反馈给用户的检索方式。 这个过程类似于通过字典中的检索字表查字的过程。 MongoDB 从 2.4 版本开始支持全文检索，目前支持15种语言(暂时不支持中文)的全文索引。 mongodb 正则表达式 正则表达式是使用单个字符串来描述，匹配一系列符合某个语法规则的字符串。 正则表达式在文本处理方面极为实用，很多语言都支持利用正则表达式进行字符串操作 mongodb使用$regex操作符来设置匹配字符串的正则表达式 可以设置 $options 为 $i 使正则匹配忽略大小写 如果你的文档中字段设置了索引，那么使用索引相比于正则表达式匹配查找所有的数据查询速度更快 mongodb 管理工具: Rockmongo rockmongo是php写的一个mongodb管理工具，可以用来管理服务，数据库，集合，文档，索引等，具体使用方法等用到了学就是，界面管理方法想来也是跟Plsqldeveloper和navicat那些差不多的页面版 P3(再高一点点) mongo是常用的面向文档非关系数据库，主要应用场景在微博，博客等消息存储业务，数据与金融等传统行业比起来没有那么重要，对事务要求相对不高，这时候用mongo比关系型数据库更合适，因为关系型数据库每次操作都有ACK，mongo省去了这一步，大大提高存储性能，同时设计时就考虑了廉价设备的容错和业务增长时的可扩展性。 在其他的博客内容对mongo的集群搭建有了一定的认识，这里通过一种副本集与分片混合部署的方案和数据存取，数据存储，对mongo的应用做进一步的介绍。 副本集中每个节点存储的数据都是相同的，相当于主备方式的数据冗余，分片是为了数据拓展，按照片键进行节点划分，数据根据片键存储到对应服务器上。 mongo的集群中有三类角色:实际数据存储节点，配置文件存储节点和路由介入节点。 ps:在另一个介绍集群搭建方案的博客里，副本集配置里除了存储节点还有一个arbiter节点，负责节点故障的时候仲裁主节点切换，实测有该节点时双节点中主节点停/起副节点会自动切主备，没有则主掉了副节点还是副节点，不会自动切换 客户端与路由节点连接，从配置节点上查询数据，根据查询结果到实际的存储节点上查询和存储数据。 客户端发起写操作请求，路由节点接到请求后查询配置服务器，得到有存储空间的服务器，路由节点把写入操作转发给由副本集组成的数据存储服务器，管理过程类似hdfs中master节点对chunk server的管理 客户端发起读操作请求，路由节点接到请求后查询配置服务器，根据存储服务器中的记录，返回目标数据的存储服务器路径大小等信息，路由节点转发读操作到存储服务器，获取信息后路由服务器将查询结果返回给客户端 副本集内部的写操作 写操作只写到主节点当中，由主节点以异步的方式同步到从节点 副本集内部的读操作 可以从任意节点读取数据，也可以指定具体到哪个节点 P4(可以说相当高了) 存储引擎:wiredTiger按照Mongodb默认的配置，WiredTiger的写操作会先写入Cache，并持久化到WAL(Write ahead log)，每60s或log文件达到2GB时会做一次Checkpoint，将当前的数据持久化，产生一个新的快照。Wiredtiger连接初始化时，首先将数据恢复至最新的快照状态，然后根据WAL恢复数据，以保证存储可靠性 所有write请求都基于”文档级别”的lock，因此多个客户端可以同时更新一个collection中的不同文档，这种更细颗粒度的lock，可以支撑更高的读写负载和并发量，因为对于production环境，更多的CPU可以有效提升wiredTiger性能，因为它是多线程IO。 架构模式Replica set：复制集，mongodb的架构方式之一 ，通常是三个对等的节点构成一个“复制集”集群 Sharding cluster：分片集群，数据水平扩展的手段之一,replica set这种架构的缺点就是“集群数据容量”受限于单个节点的磁盘大小，如果数据量不断增加，对它进行扩容将时非常苦难的事情，所以我们需要采用Sharding模式来解决这个问题。将整个collection的数据将根据sharding key被sharding到多个mongod节点上，即每个节点持有collection的一部分数据，这个集群持有全部数据，原则上sharding可以支撑数TB的数据。 系统配置： - 建议mongodb部署在linux系统上，较高版本，选择合适的底层文件系统（ext4），开启合适的swap空间 - 无论是MMAPV1或者wiredTiger引擎，较大的内存总能带来直接收益。 - 对数据存储文件关闭“atime”（文件每次access都会更改这个时间值，表示文件最近被访问的时间），可以提升文件访问效率。 - ulimit参数调整，这个在基于网络IO或者磁盘IO操作的应用中，通常都会调整，上调系统允许打开的文件个数（ulimit -n 65535） 数据文件存储原理 Data Filesmongodb的数据会保存在底层文件系统中，比如”dbpath=/usr/local/mongodb/master”，创建一个database为”runoob”，collection为”col”，然后插入若干documents，可以看到如下列表 可以看到数据文件所在目录”/usr/local/mongodb/master”下面生成了”runoob”文件夹，里面存放有若干collection文件，数据文件从16M开始，每次扩张一倍(16M，32M，64M，128M…)，默认情况下迟迟单个data file最大尺寸为2G，smallfile设置后限定512M，每个database最多支持16000个数据文件，约32T，smallfiles设置后单个database最大容量8T 一个database中所有的collections及索引信息会分散存储在多个数据文件中，没有像SQL数据库一样每个表的数据索引分别存储； 数据分块的单位是extent(范围，区域)，extent中可以保存collection数据或者index数据，每个extent包含多条document，大小不等，一个extent只能保存同一个collection的数据，不同collections数据分布在不同extents中，indexs也保存在各自的extent中，一个collection由一个或多个extent构成，最小size8K，最大2G，这些extent分散在多个datafile中，换句话说，一个datafile可能包含多个collection的数据，由不同collection的extent组成，但一个extent不会跨越两个datafile 举个栗子（吼吼），两个数据文件my-db.1和my-db.2里存放A和B两张表的数据，AB两张表上的document和index存放在一个个小的extent里面，这些extent并不是各自集中存放，而是散布在两个数据文件里面 在每个database的namespace文件中，比如test.ns文件中，每个collection只保存了第一个extent的位置信息，并不保存所有的extents列表，但每个extent都维护者一个链表关系，即每个extent都在其header信息中记录了此extent的上一个、下一个extent的位置信息，这样当对此collection进行scan操作时（比如全表扫描），可以提供很大的便利性。 由存储机制可以很容易想到，删除document会导致磁盘碎片，有些update也会导致磁盘碎片，比如update导致文档尺寸变大，进而超过原来分配的空间；当有新的insert操作时，mongodb会检测现有的extents中是否合适的碎片空间可以被重用，如果有，则重用这些fragment，否则分配新的存储空间。磁盘碎片对write操作有一定的性能影响，而且会导致磁盘空间浪费； 如果database已经运行一段时间，数据已经有很大的磁盘碎片（storageSize与dataSize比较），可以通过mongodump将指定database的所有数据导出，然后将原有的db删除，再通过mongorestore指令将数据重新导入 Namespace文件对于namespace文件，比如“test.ns”文件，默认大小为16M，此文件中主要用于保存“collection”、index的命名信息，比如collection的“属性”信息、每个索引的属性类型等，如果你的database中需要存储大量的collection（比如每一小时生成一个collection，在数据分析应用中），那么我们可以通过配置文件“nsSize”选项来指定，参见某博客。 journal文件journal日志保存在dbpath下“journal”子目录中，一般会有三个journal文件，journal文件中保存了write操作的记录，每条记录中包含write操作内容之外，还包含一个“lsn”（last sequence number），表示此记录的ID journal是一个预写事务日志，来确保数据持久性，存储引擎每隔60s(默认/可调)或者待写入数据达到2G，mongo将对journal文件提交一个checkpoint检测点，将内存中的数据变更flush到磁盘的数据文件中，并做一个标记点，表示此前的数据已经持久存储在数据文件中，检测点创建后，journal日志清空，后续数据更改存在于内存和journal日志。 例如write操作首先被写入journal日志，然后在内存中变更数据，数据量积累或者时间间隔条件满足后触发检测点，数据flush到数据文件。即检测点之前的数据只是在journal中持久存储，并没有写入数据文件，延迟持久化可以提高磁盘效率，如果在checkpoint之前mongo异常退出，再次启动可以根据journal恢复数据。 默认情况下，“journal”特性是开启的，特别在production环境中，我们没有理由来关闭它，当然也可以关闭journal，提高了性能，但单点mongo降低了数据安全性，如果有异常则丢失checkpoint之间的数据，副本集在所有节点同时退出的情况时有影响。 如果你希望数据尽可能的不丢失，可以考虑： 1）减小commitIntervalMs的值 2）每个write指定“write concern”中指定“j”参数为true 3）最佳手段就是采用“replica set”架构模式，通过数据备份方式解决，同时还需要在“write concern”中指定“w”选项，且保障级别不低于“majority”，最终我们需要在“写入性能”和“数据一致性”两个方面权衡，即CAP理论。 至此mongodb的基础内容介绍得差不多，具体实践和高级技巧等工作项目中遇到了再深入研究，理论指导实践，在实践中检验和调整升华理论，然后在下一次实践中表现得更加出色，理论和实践的螺旋式上升是不变的旋律。]]></content>
      <categories>
        <category>mongodb</category>
      </categories>
      <tags>
        <tag>Theory</tag>
        <tag>Database</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mongodb]]></title>
    <url>%2F2018%2F01%2F18%2FMongodb%2F</url>
    <content type="text"><![CDATA[简介 Mongodb是时下流行的Nosql数据库，存储方式是文档式存储，并不是key-value形式。 参考CSDN的博客，在本地使用三种方式实际搭建mongo集群: 1.replica set 2.sharding 3.master-slaver 万丈高楼平地起，无论业务多复杂，架构多庞大，归根到底还是数据库本身各种功能的应用，理解了数据库的设计原理，数据存储形式，掌握了基本的功能应用，集群构建，数据流动抽取备份，到了实际工作场景中无非数据量更大，应用形式多样化，架构更庞大而已。 replica set副本集，简单来说就是集群当中包含了多份数据，保证主节点挂掉，备节点能继续提供服务，备节点与主节点的数据一致，如图 Mongodb(M)表示主节点，Mongodb(S)表示备节点，Mongodb(A)表示仲裁节点，主备节点存储数据，仲裁节点不存储数据，客户端连接主备，不连接仲裁节点。 默认设置下，主节点提供所有增删改查服务，备节点不提供服务。可以设置read preference modes，让备节点提供查询服务来分担主节点的压力，客户端进行的数据查询请求会自动转到备节点。 仲裁节点本身不存储数据，主要作用是决定哪个备节点在主节点挂掉后提升为主节点。多个备节点仍然只需要一个仲裁节点，本文中即使一个备节点也需要仲裁节点，如果没有，主节点挂了备节点还是备节点。 集群设计 服务器 节点 192.168.3.7 主节点 192.168.3.8 备节点 192.168.3.9 仲裁节点 软件版本 下载地址 mongodb-linux-x86_64-3.4.10 下载地址 实验过程解压到指定目录，创建数据文件夹tar xvf mongodb-linux-x86_64-3.4.10.tgz -C /usr/local cd /usr/local mv mongodb-linux-x86_64-3.4.10/ mongodb mkdir -p /usr/local/mongodb/data/master mkdir -p /usr/local/mongodb/data/slaver mkdir -p /usr/local/mongodb/data/arbiter 三个目录分别对应主，备，仲裁节点 建立配置文件,起mongo 以下三个文件是三个节点的mongo启动参数文件，编辑好放在任何位置都可以，启动时按绝对路径指定即可 #master.conf dbpath=/usr/local/mongodb/data/master logpath=/usr/local/mongodb/log/master.log pidfilepath=/usr/local/mongodb/master.pid directoryperdb=true logappend=true replSet=testrs bind_ip=192.168.3.7 port=27017 oplogSize=10000 fork=true noprealloc=true #slaver.conf dbpath=/usr/local/mongodb/data/slaver logpath=/usr/local/mongodb/log/slaver.log pidfilepath=/usr/local/mongodb/slaver.pid directoryperdb=true logappend=true replSet=testrs bind_ip=192.168.3.8 port=27017 oplogSize=10000 fork=true noprealloc=true #arbiter.conf dbpath=/usr/local/mongodb/data/arbiter logpath=/usr/local/mongodb/log/arbiter.log pidfilepath=/usr/local/mongodb/arbiter.pid directoryperdb=true logappend=true replSet=testrs bind_ip=1192.168.3.9 port=27017 oplogSize=10000 fork=true noprealloc=true 参数解释： dbpath：数据存放目录 logpath：日志存放路径 pidfilepath：进程文件，方便停止mongodb directoryperdb：为每一个数据库按照数据库名建立文件夹存放 logappend：以追加的方式记录日志 replSet：replica set的名字 bind_ip：mongodb所绑定的ip地址 port：mongodb进程所使用的端口号，默认为27017 oplogSize：mongodb操作日志文件的最大大小。单位为Mb，默认为硬盘剩余空间的5% fork：以后台方式运行进程 noprealloc：不预先分配存储 ./mongod -f ../master.conf ./mongod -f ../slaver.conf ./mongod -f ../arbiter.conf 原博客这里并没有提到需要手动创建log文件夹，经过若干次各个节点若干次启动失败各种原因排查，最终确定在我搭建的环境里需要手动创建log文件夹，但是原博文和推送我博文的朋友他们搭建过程貌似都不需要，不得解，待议。 配置主，备，仲裁节点./mongo 192.168.3.7:27018 #ip和port是某个节点的地址 &gt;use admin &gt;cfg={_id:&quot;testrs&quot;,members:[ {_id:0,host:&apos;192.168.3.7:27017&apos;,priority:2}, {_id:1,host:&apos;192.168.3.8:27017&apos;,priority:1}, {_id:2,host:&apos;192.168.3.9:27017&apos;,arbiterOnly:true}] }; &gt;rs.initiate(cfg) #使配置生效 现在基本上已经完成了集群的所有搭建工作。 测试方案: 一个是往主节点插入数据，从备节点查询到之前插入的数据. 二是停掉主节点，让备节点变成主节点提供服务。 三是恢复主节点，备节点也能恢复其备的角色，而不是继续充当主的角色。二和三都可以通过rs.status()命令实时查看集群的变化 sharding和Replica Set类似，都需要一个仲裁节点，但是Sharding还需要配置节点和路由节点。就三种集群搭建方式来说，这种是最复杂的。部署图如下： 非常简单，懒得配了，看文档都能想到所有的具体操作和屏幕输出还有最后的结果，如果工作用到现搭就行了，有兴趣的自己去简介的原博客地址去撸一遍。 master-slaver 这个是最简答的集群搭建，严格来说不能算是集群，只能说是主备。并且官方已经不推荐这种方式，简单的介绍下，搭建方式相对简单。 ./mongod --master --dbpath /data/masterdb/ #主节点 ./mongod --slave --source &lt;masterip:masterport&gt; --dbpath /data/slavedb/ 备节点 只要在主节点和备节点上分别执行这两条命令，Master-Slaver就算搭建完成了。我没有试过主节点挂掉后备节点是否能变成主节点，不过既然官方已经不推荐了，了解即可。 三种集群搭建方式首选Replica Set，只有真的是大数据，Sharding才能显现威力，毕竟备节点同步数据是需要时间的。Sharding可以将多片数据集中到路由节点上进行一些对比，然后将数据返回给客户端，Replica Set的ips在数据达到1400w条时基本能达到1000左右，而Sharding在300w时已经下降到500ips，mongodb吃内存的问题，解决办法只能通过ulimit来控制内存使用量，但是如果控制不好的话，mongodb会挂掉 mongo数据库工作原理 MongoDB使用的是内存映射存储引擎，它会把磁盘IO操作转换成内存操作，如果是读操作，内存中的数据起到缓存的作用，如果是写操作，内存还可以把随机的写操作转换成顺序的写操作，总之可以大幅度提升性能。MongoDB并不干涉内存管理工作，而是把这些工作留给操作系统的虚拟缓存管理器去处理，这样的好处是简化了MongoDB的工作，但坏处是你没有方法很方便的控制MongoDB占多大内存，事实上MongoDB会占用所有能用的内存，所以最好不要把别的服务和MongoDB放一起。 出于某些原因，你可能想释放掉MongoDB占用的内存，不过前面说了，内存管理工作是由虚拟内存管理器控制的，所以 1.重启服务 2.调整内核参数drop_caches systemctl vm.drop_caches=3 3.使用MongoDB内置的closeAllDatabases命令 可以通过mongo命令行来监控MongoDB的内存使用情况 db.serverStatus().mem 这篇博客简单介绍下怎么使用，具体技术细节服务库架构应用场景等我深入研究回来补上。]]></content>
      <categories>
        <category>Mongodb</category>
      </categories>
      <tags>
        <tag>Database</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Leanote]]></title>
    <url>%2F2018%2F01%2F16%2FLeanote%2F</url>
    <content type="text"><![CDATA[简介 Leanote 蚂蚁笔记是一款国产的在线网页版云笔记软件， 集知识管理、笔记、分享、博客功能于一身，界面简约但功能不简单！它支持多笔记本、标签分类、笔记共享、添加保存附件等，而且还提供了免打扰写作模式、支持图片尺寸调整、并且支持 Markdown 语法写作，最重要的是，它还能完美支持代码高亮显示！ 该项目采用 Golang+MongoDB 开发，现已完全开源并能免费使用。普通用户可以直接使用 Leanote 提供的公共服务，也可以自行搭建属于自己或公司局域网内的私有云笔记平台。 而且，Leanote 可以让用户创建一个用户组，并将笔记共享到这个组里，所有组员都可以浏览、编辑笔记，可以非常方便地进行协作或知识共享。（题外话：团队内还可以通过 SeaFile 搭建私有的云存储配合使用） 搭建思路 1.上传mongodb软件包到服务器，解压，起服务 2.上传leanote软件包到服务器，解压，修改配置文件，向mongodb导入lenote文件夹下的原始库表 4.起leanote服务 软件包 软件包 下载地址 mongodb-linux-x86_64-3.0.1 点击下载 leanote-linux-amd64-v2.6 点击下载 leanote-desktop 点击下载 实验步骤 1.上传，解压，创建数据文件夹，起服务$ tar xf mongodb-linux-x86_64-3.0.1.tgz -C /usr/local $ mv mongodb-linux-x86_64-3.0.1/ mongodb $ mkdir -p /usr/local/mongodb/data $ /usr/local/mongodb/bin/mongod --dbpath /usr/local/mongodb/data/ --bind_ip 127.0.0.1 &amp;&gt;&gt; /tmp/mongo.log &amp; $ kill -4 xxx # xxx为mongo的进程号，安全关闭 2.上传，解压，修改配置文件，导原始库表$ tar xf leanote-linux-amd64-v2.6.bin.tar.gz -C /usr/local/ $ vi /usr/local/leanote/conf/app.conf 修改服务端口，加密字符串 $ /usr/local/mongodb/bin/mongorestore -h localhost -d leanote --dir /usr/local/leanote/mongodb_backup/leanote_install_data/ 3.起服务并验证可用$ cd /usr/local/leanote/bin $ nohup ./run.sh &amp; 进程正常 服务正常 至此以私人服务器为server，仅供个人及好友使用的leanote搭建完成。 能在线编辑保存普通文件 能在线编辑保存markdown文件并发布为个人博客等 效果演示 这是leanote登陆页面，访问服务器的9000端口，如该服务器ip为192.168.x.x，访问”http://192.168.x.x:9000&quot;即可进入该页面。 因私人服务器性能有限，且非盈利自用，管理员已经关闭注册功能，使用该私人服务的唯一途径就是向管理者申请，通过后台单独添加用户 这是在线编辑页面，可以设置头像，访问别名，自定义很多参数，可以创建普通文本文件并保存，进行上传附件，插入图片，网页链接，版本管理等操作，类似有道云，为知等在线笔记 可以将写好的markdown文件公布为博客，任何人都可以通过链接访问该博客，添加评论。 访问该网站就是对server服务器的9000端口发起访问请求，leanote将数据库的数据展示出来，用户编辑的普通文件和博客文件在保存后都会经过leanote的加密字符串运算加密后存放在数据库，本人对自己的文章具有最高权限，admin管理员具有后台添加删除用户并设置一些参数等权限，对普通用户的博客文章没有修改删除等权限。 有没有发现软件包有三个文件至此只用了两个，没错，第三个是leanote桌面版，下载解压就能用，不用安装，登陆的时候选择登陆自建服务，进去后可以像在线一样编辑文件，完成后保存，公布为博客，软件内即可访问自己的博客，而这所有的都是开源免费，如此干净高效，良心得我有点感动。 解锁更多姿势 1.数据库备份策略备份 $ /usr/local/mongodb/bin/mongodump -h 127.0.0.1 -d leanote -o /~（备份集存放路径） 恢复 $ /usr/local/mongodb/bin/mongorestore -h 127.0.0.1 -d leanote --drop --dir ~/leanote （备份集所在路径） 用linux系统自带的crontab定时执行备份脚本，实现每天三次备份，数据备份集可以保证即使服务挂了，甚至服务器挂了，被回收了，随时找个腾讯云或者阿里云的主机，分分钟重新搭建一套，导入数据后恢复服务，对普通用户来说除了期间无法使用之外，没有区别，所有数据都在。 $ crontab -l 0 6,14,22 * * * /usr/local/mongodb/backup/back.sh $ more /usr/local/mongodb/backup/back.sh #!/bin/sh #该脚本通过crontab定时执行，实现每天6，14，22整点共三次备份mongo数据，保留两周内的备份集 #备份目录为BASEDIR，将带时间戳文件夹的备份集压缩，最后检测是否有14天前的文件夹，有则删除 BASEDIR=&quot;/usr/local/mongodb/backup&quot; CURDIR=$BASEDIR/`date &quot;+%Y-%m-%d_%H.%M.%S&quot;` /usr/local/mongodb/bin/mongodump -h 127.0.0.1 -d leanote -o $CURDIR cd $CURDIR tar -zcvf leanote.tar.gz leanote rm -rf leanote/ find $BASEDIR -mtime +14 -exec rm -rf {} \; 2.docker部署1.制作docker镜像 docker是个很好用的工具，将系统，应用，程序制作成docker镜像，分发部署使用都会特别方便，这里是leanote的docker镜像，服务器有docker可以直接导入然后使用，没有需要先安装docker。导入制作好的docker镜像并启动，这时所有前面配置过程的安装包，备份策略，配置文件更改都已经解决，使用效果一致。 2.一键部署使用$ docker load -i leanote_2.6-image.tar.gz $ docker history 69e799db2e66 $ docker run -tid –name leanote -p 80:80 –restart always -v /data/leanote:/data/ leanote:2.6 $ docker exec -ti cbc598f37d50 sh $ tar xf leanote.tar.xz $ mongorestore -h 127.0.0.1 -d leanote –drop –dir ./leanote 3.使用技巧详细介绍1.普通用户傻瓜攻略 普通用户不用管上述搭建过程，有兴趣的可以研究下，没兴趣知道怎么使用就足够了。 首先，访问。要在线写文章并保存以待后来回顾，写博客发布出去给别人看，得知道在哪里写，先访问我搭建的私人leanote的网页； 然后，登陆。你需要一个账号密码来登陆站点使用各种功能，我已经关闭了注册权限，新用户只能沟通后由我后台手动建立账号。因为服务器存储计算网络资源有限，这个服务也不是盈利性质，就是个使用方便小范围共享的利器，所以使用者那必须是有限制的。 最后，编辑。登陆成功就可以为所欲为了，具体功能看本文中间段落效果演示。所有编写的文章和markdown（后缀为.md的博客文件）保存在我的服务器上，每天备份，保留两周的备份集，所以就可劲作吧。 最最后，可以写什么？ 什么都可以写，在线写的文章和博客如果不发布，没有任何人可以看到，包括管理员，所有文章加密后保存在数据库；可以发布出去，比如这样，发布的内容所有人都可以通过网络访问。 你可以写一些技术文档分类保存不发布，当作一个在线笔记存储，可以写一些游戏攻略经验发布出去装逼，可以写一些抒情的文章然后把链接给别人看（说真的，这个有点骚包，我绝逼干不出来这种事），基本功能就是这些，具体的骚操作自己开发，enjoy it！！！ 2.深度用户高级攻略]]></content>
      <categories>
        <category>Software</category>
      </categories>
      <tags>
        <tag>service</tag>
        <tag>blog</tag>
        <tag>community</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HBase入门]]></title>
    <url>%2F2018%2F01%2F10%2FHBase%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[简介 HBase – Hadoop Database，是一个高可靠性、高性能、面向列、可伸缩的分布式存储系统，利用HBase技术可在廉价 PC Server 上搭建起大规模结构化存储集群。 面向可扩展性的分布式数据库，如hbase，与面向高性能并发读写的key-value数据库，如Redis，面向海量数据访问的文档数据库，如MongoDB共同构成的应用场景，是用于解决在当前互联网发展阶段产生的巨量数据分布式处理，大量非结构化数据处理，各种社交平台日常生活中产生的的文本信息处理的重要解决方案。 Hbase是什么 HBase 是 Google Bigtable 的开源实现，类似 Google Bigtable 利用 GFS 作为其文件存储系统，HBase 利用 Hadoop HDFS 作为其文件存储系统；Google运行 MapReduce 来处理 Bigtable 中的海量数据，HBase 利用 Hadoop MapReduce 来处理 HBase 中的海量数据；Google Bigtable 利用 Chubby 作为协同服务，HBase 利用 ookeeper 作为对应。 HBase 是一个分布式的、面向列的开源数据库,是 Apache 的 Hadoop 项目的子项目。HBase 不同于一般的关系数据库，它是一个适合于非结构化数据存储的数据库,另一个不同是 HBase 基于列而不是基于行的模式。 如图,HBase 位于结构化存储层，Hadoop HDFS 为 HBase 提供了高可靠性的底层存储支持，Hadoop MapReduc 为 HBase 提供了高性能的计算能力，Zookeeper 为 HBase 提供了稳定服务和 failover 机制。 Pig 和 Hive 还为 HBase 提供了高层语言支持，使得在 HBase 上进行数据统计处理变的非常简单。 Sqoop 则为 HBase 提供了方便的 RDBMS 数据导入功能，使得传统数据库数据向 HBase 中迁移变的非常方便。 spark的构成 如何访问Hbase 1. Native Java API，最常规和高效的访问方式，适合Hadoop MapReduce Job并行批处理HBase表数据 2. HBase Shell，HBase的命令行工具，最简单的接口，适合HBase管理使用 3. Thrift Gateway，利用Thrift序列化技术，支持C++，PHP，Python等多种语言，适合其他异构系统在线访问HBase表数据 4. REST Gateway，支持REST 风格的Http API访问HBase, 解除了语言限制 5. Pig，可以使用Pig Latin流式编程语言来操作HBase中的数据，和Hive类似，本质最终也是编译成MapReduce Job来处理HBase表数据，适合做数据统计 6. Hive，当前Hive的Release版本尚没有加入对HBase的支持，但在下一个版本Hive 0.7.0中将会支持HBase，可以使用类似SQL语言来访问HBase Hbase数据模型 HBase的特点 大：一个表可以有上亿行，上百万列。 面向列：面向列表（簇）的存储和权限控制，列（簇）独立检索。 稀疏：对于为空（NULL）的列，并不占用存储空间，因此，表可以设计的非常稀疏。 无模式：每一行都有一个可以排序的主键和任意多的列，列可以根据需要动态增加，同一张表中不同的行可以有截然不同的列。 数据多版本：每个单元中的数据可以有多个版本，默认情况下，版本号自动分配，版本号就是单元格插入时的时间戳。 数据类型单一：HBase中的数据都是字符串，没有类型。 Table &amp; Column Family Row Key: 行键，Table的主键，Table中的记录按照Row Key排序访问 HBase table 中的行，只有三种方式： 1)通过单个 Row Key 访问。 2)通过 Row Key 的 range 全表扫描。 3)Row Key 可以使任意字符串（最大长度是64KB，实际应用中长度一般为 10 ~ 100bytes），在HBase 内部，Row Key 保存为字节数组。 Timestamp: 时间戳，每次数据操作对应的时间戳，可以看作是数据的version number Column Family：列簇，Table在水平方向有一个或者多个Column Family组成，一个Column Family中可以由任意多个Column组成，即Column Family支持动态扩展，无需预先定义Column的数量以及类型，所有Column均以二进制格式存储，用户需要自行进行类型转换。 逻辑数据模型中空白cell在物理上是不存储的，因为根本没有必要存储，因此若一个请求为要获取t8时间的contents:html，他的结果就是空。相似的，若请求为获取t9时间的anchor:my.look.ca，结果也是空。但是，如果不指明时间，将会返回最新时间的行，每个最新的都会返回 Table &amp; Region 当Table随着记录数不断增加而变大后，会逐渐分裂成多份splits，成为regions，一个region由[startkey,endkey)表示，不同的region会被Master分配给相应的RegionServer进行管理 HBase中有两张特殊的Table，-ROOT-和.META. .META.：记录了用户表的Region信息，.META.可以有多个regoin -ROOT-：记录了.META.表的Region信息，-ROOT-只有一个region Zookeeper中记录了-ROOT-表的location MapReduce on HBase 在HBase系统上运行批处理运算，最方便和实用的模型依然是MapReduce，如图所示，HBase Table和Region的关系，比较类似HDFS File和Block的关系，HBase提供了配套的TableInputFormat和TableOutputFormat API，可以方便的将HBase Table作为Hadoop MapReduce的Source和Sink，对于MapReduce Job应用开发人员来说，基本不需要关注HBase系统自身的细节。 HBase系统架构 Client HBase Client使用HBase的RPC（Remote Procedure Call – 远程过程调用）机制与HMaster和HRegionServer进行通信，对于管理类操作，Client与HMaster进行RPC；对于数据读写类操作，Client与HRegionServer进行RPC Zookeeper Zookeeper Quorum中除了存储了-ROOT-表的地址和HMaster的地址，HRegionServer也会把自己以Ephemeral方式注册到 Zookeeper中，使得HMaster可以随时感知到各个HRegionServer的健康状态。此外，Zookeeper也避免了HMaster的 单点问题 HMaster HMaster没有单点问题，HBase中可以启动多个HMaster，通过Zookeeper的Master Election机制保证总有一个Master运行，HMaster在功能上主要负责Table和Region的管理工作： 1.管理用户对Table的增、删、改、查操作 2.管理HRegionServer的负载均衡，调整Region分布 3.在Region Split后，负责新Region的分配 4.在HRegionServer停机后，负责失效HRegionServer 上的Regions迁移 HRegionServer HRegionServer主要负责响应用户I/O请求，向HDFS文件系统中读写数据，是HBase中最核心的模块。 如图，HRegionServer内部管理了一系列HRegion对象，每个HRegion对应了Table中的一个Region，HRegion中由多个HStore组成。每个HStore对应了Table中的一个Column Family的存储，可以看出每个Column Family其实就是一个集中的存储单元 HStore存储是HBase存储的核心,由两部分组成，一部分是MemStore，一部分是StoreFiles，MemStore是 Sorted Memory Buffer，用户写入的数据首先会放入MemStore，当MemStore满了以后会Flush成一个StoreFile（底层实现是HFile）， 当StoreFile文件数量增长到一定阈值，会触发Compact合并操作，将多个StoreFiles合并成一个StoreFile，合并过程中会进 行版本合并和数据删除，因此可以看出HBase其实只有增加数据，所有的更新和删除操作都是在后续的compact过程中进行的，这使得用户的写操作只要 进入内存中就可以立即返回，保证了HBase I/O的高性能。当StoreFiles Compact后，会逐步形成越来越大的StoreFile，当单个StoreFile大小超过一定阈值后，会触发Split操作，同时把当前 Region Split成2个Region，父Region会下线，新Split出的2个孩子Region会被HMaster分配到相应的HRegionServer 上，使得原先1个Region的压力得以分流到2个Region上。 在理解了上述HStore的基本原理后，还必须了解一下HLog的功能，因为上述的HStore在系统正常工作的前提下是没有问题的，但是在分布式 系统环境中，无法避免系统出错或者宕机，因此一旦HRegionServer意外退出，MemStore中的内存数据将会丢失，这就需要引入HLog了。 每个HRegionServer中都有一个HLog对象，HLog是一个实现Write Ahead Log的类，在每次用户操作写入MemStore的同时，也会写一份数据到HLog文件中（HLog文件格式见后续），HLog文件定期会滚动出新的，并 删除旧的文件（已持久化到StoreFile中的数据）。当HRegionServer意外终止后，HMaster会通过Zookeeper感知 到，HMaster首先会处理遗留的 HLog文件，将其中不同Region的Log数据进行拆分，分别放到相应region的目录下，然后再将失效的region重新分配，领取 到这些region的HRegionServer在Load Region的过程中，会发现有历史HLog需要处理，因此会Replay HLog中的数据到MemStore中，然后flush到StoreFiles，完成数据恢复。 HBase存储格式 HBase中的所有数据文件都存储在Hadoop HDFS文件系统上，主要包括上述提出的两种文件类型： 1.HFile， HBase中KeyValue数据的存储格式，HFile是Hadoop的二进制格式文件，实际上StoreFile就是对HFile做了轻量级包装，即StoreFile底层就是HFile 2.HLog File，HBase中WAL（Write Ahead Log） 的存储格式，物理上是Hadoop的Sequence File HFile 如图HFile的存储格式，首先HFile是不定长的，长度固定的只有其中两块:Trailer和Fileinfo。 Trailer中有指针指向其他数据块的起始点； Fileinfo则记录了文件的部分meta信息，如AVG_KEY_LEN, AVG_VALUE_LEN, LAST_KEY, COMPARATOR, MAX_SEQ_ID_KEY等; Data Index和Meta Index块记录了每个Data块和Meta块的起始点。 Data Block是HBase I/O的基本单元，为了提高效率，HRegionServer中有基于LRU的Block Cache机制。每个Data块的大小可以在创建一个Table的时候通过参数指定，大号的Block有利于顺序Scan，小号Block利于随机查询。 每个Data块除了开头的Magic以外就是一个个KeyValue对拼接而成, Magic内容就是一些随机数字，目的是防止数据损坏。后面会详细介绍每个KeyValue对的内部构造。 LRU:Least Recently Used ,内存管理的一种页面置换算法，对于在内存中但又不用的数据块（内存块）叫做LRU，操作系统会根据哪些数据属于LRU而将其移出内存而腾出空间来加载另外的数据。 HFile里面的每个KeyValue对就是一个简单的byte数组。但是这个byte数组里面包含了很多项，并且有固定的结构。具体结构如图:开始时两个固定长度的数值，分别表示key的长度和value的长度，紧接着是key，开始是固定长度的数值，表示rowkey的长度，紧接着是rowkey，然后是固定长度的数值，表示Family的长度，然后是Family，接着是Qualifier，然后是两个固定长度的数值，表示Time Stamp和Key Type（Put/Delete）。Value部分没有这么复杂的结构，就是纯粹的二进制数据。 HLogFile 如图是HLog文件的结构，其实HLog文件就是一个普通的Hadoop Sequence File，Sequence File 的Key是HLogKey对象，HLogKey中记录了写入数据的归属信息，除了table和region名字外，同时还包括 sequence number和timestamp，timestamp是“写入时间”，sequence number的起始值为0，或者是最近一次存入文件系统中sequence number。 HLog Sequece File的Value是HBase的KeyValue对象，即对应HFile中的KeyValue HBase的高并发和实时处理数据 Hadoop是一个高容错、高延时的分布式文件系统和高并发的批处理系统，不适用于提供实时计算；HBase是可以提供实时计算的分布式数据库，数据被保存在HDFS分布式文件系统上，由HDFS保证其高容错性。 在生产环境中，HBase是如何基于hadoop提供实时性呢？ HBase上的数据是以StoreFile(HFile)二进制流的形式存储在HDFS上block块儿中；但是HDFS并不知道的HBase存的是什么，它只把存储文件视为二进制文件，也就是说，HBase的存储数据对于HDFS文件系统是透明的。 从根本上说，HBase能提供实时计算服务主要原因是由其架构和底层的数据结构决定的，即由LSM-Tree + HTable(region分区) + Cache决定——客户端可以直接定位到要查数据所在的HRegion server服务器，然后直接在服务器的一个region上查找要匹配的数据，并且这些数据部分是经过cache缓存的。 具体数据访问流程如下： 1. Client会通过内部缓存的相关的-ROOT-中的信息和.META.中的信息直接连接与请求数据匹配的HRegion server； 2. 然后直接定位到该服务器上与客户请求对应的Region，客户请求首先会查询该Region在内存中的缓存——Memstore(Memstore是一个按key排序的树形结构的缓冲区)； 3. 如果在Memstore中查到结果则直接将结果返回给Client； 4. 在Memstore中没有查到匹配的数据，接下来会读已持久化的StoreFile文件中的数据，StoreFile也是按 key排序的树形结构的文件——并且是特别为范围查询或block查询优化过的，HBase读取磁盘文件是按其基本I/O单元(即 HBase Block)读数据的。 hbase的具体配置使用跟hadoop，spark有密切关系，后面会有专门的博客来介绍openstack和hadoop，包括原理和配置，那里面会包含hbase的安装配置使用方法，这个坑交给后面的文章来填，就这么愉快得决定了！]]></content>
      <categories>
        <category>HBase</category>
      </categories>
      <tags>
        <tag>Database</tag>
        <tag>Original</tag>
        <tag>Part-transported</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Google三大论文]]></title>
    <url>%2F2018%2F01%2F03%2FGoogle%E4%B8%89%E5%A4%A7%E8%AE%BA%E6%96%87%2F</url>
    <content type="text"><![CDATA[简介 Google File System、MapReuce以及Bigtable三驾马车可以说是大数据算法的起源，虽然Google没有公布这三个产品的源码，但是他发布了这三个产品的详细设计论文，奠定了风靡全球的大数据算法的基础！ 软件 下载地址 Google云计算三大论文英文版 点击下载 Google-File-System中文版 点击下载 Google-MapReduce中文版 点击下载 Google-Bigtable中文版 点击下载 那些年google发过的论文 1.按时间算第一篇的论文应该2003年公布的 Google File System，这是一个可扩展的分布式文件系统，用于大型的、分布式的、对大量数据进行访问的应用。文件被分割成很多块，使用冗余的方式储存于商用机器集群上(基本上Google每篇论文都是关于“商用机型”)。 2.2004年发布的的 MapReduce现在基本上可以代表大数据。主要思想是将任务分解然后在多台处理能力较弱的计算节点中同时处理，将结果合并从而完成大数据处理，传说中Google使用它计算他们的搜索索引。Mikio L. Braun(柏林工业大学机器学习学博士后，TWIMPACT联合创始人兼首席数据科学家)认为其工作模式应该是：Google把所有抓取的页面都放置于他们的集群上，并且每天都使用MapReduce来重算。 3.Bigtable发布于2006年，启发了无数的NoSQL数据库，比如：Cassandra、HBase等等。Cassandra架构中有一半是模仿Bigtable，包括了数据模型、SSTables以及提前写日志（另一半是模仿Amazon的Dynamo数据库，使用点对点集群模式）。 Google并没有止步于MapReduce，事实上，随着Internet的指数增长，从零开始重算所有搜索索引变得不切实际,他们在MapReduce不适用的地方开发新方法,对于大数据领域来说这是个福音。MapReduce不是万能的。当然，你可以更深入一步，比如说将磁盘数据移入内存，然而同样还存在一些任务的内部结构并不是MapReduce可以扩展的。 2010年发表的 Percolator的论文中，Google展示了其网络搜索是如何保持着与时俱进。Percolator建立于已存类似Bigtable的技术，但是加入了事务以及行和表上的锁和表变化的通知。这些通知之后会被用于触发不同阶段的计算。通过这样的方式，个体的更新就可以“渗透”整个数据库。 在2010年，Google还公布了 Dremel论文。一个为结构化数据设计，并拥有类SQL语言的交互式数据库。然而取代SQL数据库使用字段填补的表格，Dremel中使用的是类JSON格式数据（更准确的说，使用Google Protocol buffer格式，这将加强对允许字段的限制）。内部，数据被使用特殊格式储存，可以让数据扫描工作来的更高效。查询被送往服务器，而优秀的格式可以最大性能的输出结果 Google还需要挖掘图数据，比如在线社交网络的社交图谱；所以他们开发了 Pregel，并在2010年公布其论文。论文陈述了许多算法的实现，比如Google的PageRank、最短路径、二分图匹配等。Mikio L. Braun认为，对比MapReduce或SPF，Pregel需要更多实现的再思考。 Google在2009年提出了Spanner远景计划，并在2012年对外公布Spanner–全球分布式数据库论文。Spanner的公布可以说是Google向大数据技术中添的又一把火，Spanner具有高扩展性、多版本、全球级分布以及同步复制等特性，跨数据中心的高扩展性及全球分布会对一致性保障提出苛刻的需求,读写的外部一致性和基于时间戳的全局读一致性。为了保障这一点，Google引入了TrueTime API。TureTime API可以同步全球的时间，拥有一个TT.now（）的方法，将获得一个绝对时间，同时还能得到时间误差。为了保证万无一失，TrueTime API具有GPS和原子钟双保险。也只有这样的机制才能让全球范围内的并发处理得到保障。 在Google思路以及论文的启发下，同样涌现出一些开源项目，比如：Apache Drill、Apache Giraph、斯坦福GPS等等。 Google近年来每篇论文都有着深远的影响，同时大数据领域内有很多人必然在翘首以盼Google的下一篇论文。 Google-File-System(2003年) 文件被分割成很多块，使用冗余的方式储存于商用机器集群上 Google-MapReduce (2004年)Mapreduce是针对分布式并行计算的一套编程模型 Google-Bigtable(2006年)Bigtable发布于2006年，启发了无数的NoSQL数据库，比如：Cassandra、HBase等等 为了管理巨大的Table，把Table根据行分割，这些分割后的数据统称为：Tablets。每个Tablets大概有 100-200 MB，每个机器存储100个左右的 Tablets。底层的架构是：GFS。 由于GFS是一种分布式的文件系统，采用Tablets的机制后，可以获得很好的负载均衡。比如：可以把经常响应的表移动到其他空闲机器上，然后快速重建。 总结 MapReduce 和 BigTable都是以GFS为基础，三大基础核心技术构建出了完整的分布式运算架构。 大数据的出现是互联网技术发展的大势所趋，随着越来越多的智能化数字化应用，社交媒体信息爆炸，超级大公司的业务数据不断膨胀，需要从海量数据中挖掘发现有价值的信息来进行商务决策，企业管理，产品调整，支撑各种互联网＋的公司，传统的高性能服务器加oracle等结构化数据库方案已经不足以满足需求。 谷歌本身就是一个体量巨大的全球性科技公司，旗下youtube，twitch，twitter，搜索业务，承载着互联网数据总量相当比重的处理压力，主动或者被动都要面对这个问题，从无到有地建立了一套技术体系之后，没有依靠技术垄断获取更大的利益，而是拿出了一整套解决方案的理论基础，可以说当前的这些分布式系统，框架，非结构化数据库，大部分都基于此，谷歌为互联网进入下个时代做出了极大的贡献，再次膜拜。 这是基于当前可观测事实的合理推测，事实上谷歌作为资本控制下的科技公司，做出这种大公无私的事可能性有但很小，大概率有自己的目的在里面，比如已经有了其他的解决方案，或许是另一个技术方向，用这些论文误导业界的发展方向，结果发现业界依然如火如荼┑(￣Д ￣)┍；比如是技术上又有了突破，把淘汰下来的技术拿出去给你们这些战五渣用(╯‵□′)╯︵┻━┻;又或者是有什么别的考量，遇上了瓶颈，把这些拿出来准备接受业界的反哺啦，跟某些团体有什么不可描述的py交易啦。这些都无所谓，论迹不论心，论心世上无完人，不管当时抱着什么目的，客观上确实极大地促进了大数据这个领域的发展，我宁愿相信谷歌出于公心出于科技界一员的责任感出于促进人类科技发展，主动自愿地发起了推动大数据领域前进的一系列行为，科技宅天下第一！！！ 最后说一句，估计有巨多的人都听说过谷歌的三篇传奇论文推进一个领域的故事，但是看过论文并仔细研究过的没多少，毕竟术业有专攻，该领域的从业人员限定就能划掉大部分人，领域内分工不同，也不是人人都需要看这些，又划掉一部分，比如我就没看过，写这篇博客是因为最近又开始玩hbase和mongo，查资料越查越深，决定单独把这三篇奠基性的论文拿出来过一遍，顺便mark一下。 最最后说一句，论文是最为精确信息量最大的，但这种专业度极高的文献看起来超级麻烦，所以我推荐b站的一个教学视频【大数据系统基础】.MOOC.清华大学，可以说把GFS和MapReduce讲的相当透彻。 PS:b站超良心的，up主把视频搬过来排版调好放在那没有广告免费观看，还可以选择2倍速，资源贼多，真真的“我在b站看纪录片”。]]></content>
      <categories>
        <category>Bigdata</category>
      </categories>
      <tags>
        <tag>Theory</tag>
        <tag>Paper</tag>
        <tag>Google</tag>
        <tag>Transported</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[安装配置]]></title>
    <url>%2F2018%2F01%2F02%2FOracle%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[简介 Oralce体系结构主要可以分为三个部分，对其分别进行深入理解可以很好得帮助理解oracle软件的运行和结构 内存结构 进程结构 物理存储结构 一级目录###二级目录### 三级目录]]></content>
      <categories>
        <category>Oracle</category>
      </categories>
      <tags>
        <tag>Database</tag>
        <tag>Original</tag>
        <tag>Part-transported</tag>
        <tag>Operate</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[oracle大纲]]></title>
    <url>%2F2017%2F12%2F29%2FOracle%E5%A4%A7%E7%BA%B2%2F</url>
    <content type="text"><![CDATA[简介 Oracle Database，又名Oracle RDBMS，或简称Oracle。是甲骨文公司的一款关系数据库管理系统。它是在数据库领域一直处于领先地位的产品。可以说Oracle数据库系统是目前世界上流行的关系数据库管理系统，系统可移植性好、使用方便、功能强，适用于各类大、中、小、微机环境。它是一种高效率、可靠性好的 适应高吞吐量的数据库解决方案。 oracle的起源oracle是甲骨文公司的拳头产品，是一个功能完善，商业应用程度极高的关系型数据库软件，是计算机行业发展的阶段性产物。 数据库是什么数据库是按照数据结构组织，存储和管理数据的仓库，可视为电子化的文件柜，用户可以对文件中的数据进行新增、截取、更新、删除等操作。 它将数据以一定方式储存在一起、能为多个用户共享、具有尽可能小的冗余度，是与应用程序彼此独立的数据集合。 数据库中数据的存储形式数据模型是数据库中数据的存储方式，是数据库系统的基础 数据模型经历了： 1.层次模型：层次模型发展最早，它以树结构为基本结构，典型代表是IMS模型。由于多数实际问题中数据间关系不简单地是树型结构，层次型数据模型渐被淘汰。 2.网状模型：网状数据模型通过网状结构表示数据间联系，开发较早且有一定优点，目前使用仍较多，典型代表是 DBTG模型。 3.关系型：关系模型开发较晚，它是通过满足一定条件的二维表格来表示实体集合以及数据间联系的一种模型，具有坚实的数学基础与理论基础，使用灵活方便，适应面广，发展十分迅速 4.非关系型 ： NoSQL( Not Only SQL )，用于指代那些非关系型的，分布式的，且一般不保证遵循ACID原则的数据存储系统。 关系型数据库是指采用了关系模型来组织数据的数据库，如SQLite,Oracle,Mysql,SQLserver，最大特点就是事务的一致性（ACID），简单来说，关系模型指的就是二维表格模型，关系型数据库就是由二维表及其之间的联系所组成的一个数据组织。 优点: 1、容易理解：二维表结构是非常贴近逻辑世界一个概念，关系模型相对网状、层次等其他模型来说更容易理解； 2、使用方便：通用的SQL语言使得操作关系型数据库非常方便； 3、易于维护：丰富的完整性(实体完整性、参照完整性和用户定义的完整性)大大减低了数据冗余和数据不一致的概率； 4、支持SQL，可用于复杂的查询。 缺点: 1、为了维护一致性所付出的巨大代价就是其读写性能比较差； 2、固定的表结构； 3、高并发读写需求； 4、海量数据的高效率读写； 非关系型数据库提出了另一种理念，随之产生的面向高性能并发读写的key-value数据库，如Redis,Tokyo Cabinet,Flare ，面向海量数据访问的面向文档数据库，如MongoDB以及CouchDB，面向可扩展性的分布式数据库,如hadoop的hbase，用于解决在当前互联网发展阶段产生的巨量数据分布式处理，大量非结构化数据处理，各种社交平台日常生活中产生的的文本信息处理。 优点： 1）成本：nosql数据库简单易部署，基本都是开源软件，不需要像使用oracle那样花费大量成本购买使用，相比关系型数据库价格便宜。 2）查询速度：nosql数据库将数据存储于缓存之中，关系型数据库将数据存储在硬盘中，自然查询速度远不及nosql数据库。 3）存储数据的格式：nosql的存储格式是key,value形式、文档形式、图片形式等等，所以可以存储基础类型以及对象或者是集合等各种格式，而数据库则只支持基础类型。 4）扩展性：关系型数据库有类似join这样的多表查询机制的限制导致扩展很艰难。 缺点： 1）维护的工具和资料有限，因为nosql是属于新的技术，不能和关系型数据库10几年的技术同日而语。 2）不提供对sql的支持，如果不支持sql这样的工业标准，将产生一定用户的学习和使用成本。 3）不提供关系型数据库对事物的处理。 数据库技术的发展未来不确定，但是可预测，基于事物发展的规律进行合理推测，更大的信息量带来的是更精确的预测结果。 数据库就是用来存储数据的一个容器，信息时代万物皆数据，当前科技的热门发展方向，智能汽车，无人机，机器学习，人工智能，物联网等等，都是基于对描述物质世界的巨量数据进行处理后得到期望的结果 人工智能和深度学习不管是分类聚类算法，决策树还是神经网络，监督学习还是非监督学习，都要有经过预处理干净的数据集；物联网存在的基础就是存在于所有要监控和可监控的硬件中的嵌入式集成芯片，对这些芯片进行数据采集和操作来实现物联网控制一切的目的；智能汽车无人机这种基于数字技术和传感器的技术，最基础的技术很大部分都是对车辆行为数据集的学习和对行驶状态中数据的快速处理算法。 数据是根本，对数据安全一致的存放管理和以合理形式进行组织应用的需求将会是永久的课题，不管将来信息科技如何发展，数据组织形式如何革新，数据库这一类对数据进行管理的软件永远有一席之地，而且必将是信息时代基石般的存在。 oracle作为关系型数据库领域登峰造极的产品，安全性稳定性可扩展性都已经高度成熟，从1977年IBM提出“关系数据库”的论文，埃里森以此做出oracle产品，并在之后的不断发展，到2013年甲骨文已经超越IBM,成为继MICORSOFT后全球第二大软件公司。oracle的各代产品在全世界各个行业，电信，电力，金融，政府及大量制造业扮演着很重要的角色，互联网总数据量的不断增长，现在的发展趋势是大数据处理，对非关系型，文本数据的处理需求使nosql类数据库和分布式处理框架越来越火，发展势头迅猛，然而在传统电信电力金融政府等领域，要处理的还是高度关系化，稳定性安全性需求极高的数据，oracle作为业界大佬，即使有了强力的挑战者，基本盘依然巨大，从业人员的需求短时间内也是不会衰退太多，而且作为如此成熟的一个产品，它的设计思路，工作方式都是值得深入学习，SQL和PL/SQL如此朴实通用的数据操作语言也是必须掌握。 从软件本身，从业，学习其他数据技术等各个角度来说，学习oracle都是一件拓宽眼界有益身心的事情。 oracle知识谱系 上图知识谱系的内容基本能够覆盖诠释oracle的产品特性和使用，我会按照图中的思路来介绍oracle的相关技术，本文章是大纲，是oracle技能树的框架，后面我会每个分枝都用一篇博客来介绍。 (～￣(OO)￣)ブ，本来处于兴趣搭了个博客，又觉得空荡荡的写点东西好了，然后发现想到哪写到哪的写作方式效率太低，就大概规划了一下。 初步目标是由系列文章构成的oracle技能树，mysql技能树，mongodb技能树，python的一些相关内容，然后是研究生课程学的大数据相关内容，商务智能一篇，算法一片，数据仓库一篇，数据挖掘一篇，再加上自学的hadoop，spark这些，貌似不知不觉给自己挖了个巨坑啊（微笑挥手），加油填坑，跟妹子吹的牛跪着也要圆回来ORZ~~~ 体系结构（√） 内存结构 进程结构 物理存储结构 安装配置SQL和PL/SQL数据加载性能调优数据迁移和备份恢复安全加固和故障处理]]></content>
      <categories>
        <category>Oracle</category>
      </categories>
      <tags>
        <tag>Database</tag>
        <tag>Original</tag>
        <tag>Part-transported</tag>
        <tag>Framework</tag>
        <tag>theory</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[体系结构]]></title>
    <url>%2F2017%2F12%2F28%2FOracle%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84%2F</url>
    <content type="text"><![CDATA[简介 Oralce体系结构主要可以分为三个部分，对其分别进行深入理解可以很好得帮助理解oracle软件的运行和结构 内存结构 进程结构 物理存储结构 Oracle服务器由数据库实例和数据库文件构成 数据库 = 数据文件 + 控制文件 + 日志文件 实例 = 内存池 + 后台进程 Oracle实例就是由一些内存区和后台进程构成的一个逻辑概念。要访问数据库，先启动实例,分配内存区，然后启动后台进程 内存结构 SGA+PGA SGA – system global area由所有服务进程和后台进程共享 shared pool缓存了各用户间可共享的各种结构 数据字典，执行计划数据块相关性高，版本转换产生bug的几率更大SQL执行计划，硬解析和软解析语法语句检查，软解析去shared pool调用缓存的执行计划，硬解析，分析统计信息，生成执行计划执行，得到数据，缓存到buffer cache，执行计划缓存到shared pool，发送给客户，网络问题产生的等待事件当客户端进程，将SQL语句通过监听器发送到Oracle时, 会触发一个Server process生成，来对该客户进程服务。Server process得到SQL语句之后，对SQL语句进行Hash运算，然后根据Hash值到library cache中查找，如果存在，则直接将library cache中的缓存的执行计划拿来执行，最后将执行结果返回该客户端，这种SQL解析叫做软解析；如果不存在，则会对该SQL进行解析parse，然后执行，返回结果，这种SQL解析叫做硬解析。 硬解析的步骤： 1）对SQL语句进行语法检查，看是否有语法错误。如果存在语法错误，则退出解析过程； 2）通过数据字典(row cache)，检查SQL语句中涉及的对象和列是否存在，检查SQL语句的用户是否对涉及到的对象是否有权限。3）通过优化器创建一个最优的执行计划。这个过程会根据数据字典中的对象的统计信息，来计算多个执行计划的cost，从而得到一个最优的执行计划。这一步涉及到大量的数据运算，从而会消耗大量的CPU资源；(library cache最主要的目的就是通过软解析来减少这个步骤)；4）将该游标所产生的执行计划，SQL文本等装载进library cache中的heap中。 软解析：就是因为相同文本的SQL语句存在于library cache中，所以本次SQL语句的解析就可以去掉硬解析中的一个或多个步骤，从而节省大量的资源的耗费。软软解析：就是不解析。 buffer cache缓存了从磁盘上检索的数据块 灌数据 内存或者进程读数据，server process排数据 dbwr某个表经常使用，放在keep池中，防止冲出去 user I/O 等待事件，都是往buffer cache里灌的时候引起的，system I/O 等待事件，都是往出排的时候 redo log buffer缓存了写到磁盘之前的重做信息 重做信息（用于实例恢复）在写入磁盘中存储的物理重做日志文件 之前，将缓存在此处数据，undo的改变，会产生redo日志，写进logbuffer触发事件后 lgwr 进程写出去，生成归档日志 PGA – process global area由每个服务进程、后台进程专有；每个进程都有一个PGA 1.Private SQL area：包含绑定信息、运行时的内存结构。每个发出sql语句的会话，都有一个private SQL area（私有SQL区） 2.Session memory：为保存会话中的变量以及其他与会话相关的信息，而分配的内存区。 pga不足的时候使用临时表空间 SGA和PGA分配 Oracle官方文档推荐: MEMORY_TARGET=物理内存 x 80% MEMORY_MAX_SIZE=物理内存 x 80% 对于OLTP系统： SGA_TARGET=(物理内存 x 80%) x 80% SGA_MAX_SIZE=(物理内存 x 80%) x 80% PGA_AGGREGATE_TARGET=(物理内存 x 80%) x 20% 对于DSS系统： SGA_TARGET=(物理内存 x 80%) x 50% SGA_MAX_SIZE=(物理内存 x 80%) x 50% PGA_AGGREGATE_TARGET=(物理内存 x 80%) x 50% 进程结构五个主要后台进程 SQL&gt; select name,description from v$bgprocess where paddr&lt;&gt;&apos;00&apos;; SMON – System monitor 系统监控进程 SMON启动后会自动的用于在实例崩溃时进行数据库实例自动恢复。 清除作废的排序临时段，回收整理碎片，合并空闲空间，释放临时段，维护闪回的时间点。 在老数据库版本中，当我们大量删除表的时候，会观测到SMON进程很忙，直到把所有的碎片空间都整理完毕。 PMON – Process monitor 进程监控 PMON在后台进程执行失败后负责清理数据库缓存和闲置资源，是Oracle的自动维护机制。 清除死进程 重新启动部分进程（如调度进程） 监听的自动注册 回滚事务 释放锁 释放其他资源 DBWR 数据写进程 Server process连接Oracle后，通过数据库写进程(DBWn)将数据缓冲区中的“脏缓冲区”的数据块写入到存储结构(数据文件、磁盘文件) 只做一件事，将数据写到磁盘。就是将数据库的变化写入到数据文件。该进程最多20 个，即使你有36 个CPU 也只能最多有20 个数据库写进程。进程名称DBW0-DBW9 DBWa-DBWj LGWR 日志写进程 主要用于记录数据库的改变和记录数据库被改变之前的原始状态，所以应当对其作多重备份，用于恢复和排错。 激活LGWR的情况： 提交指令 日志缓冲区超过1/3 每三秒 每次DBWn执行之前 CKPT 校验点进程 主要用户更新数据文件头，更新控制文件和触发DBWn数据库写进程。Ckpt 进程会降低数据库性能，但是提高数据库崩溃时，自我恢复的性能。我们可以理解为阶段性的保存数据，一定的条件满足就触发，执行DBWn存盘操作。 物理结构oracle数据库是个运行在操作系统上最终目的是存储和管理相关数据的软件 每一个Oracle数据库都是由三种类型的文件组成：数据文件（Data File）、日志文件（Log File）和控制文件（Control File）。数据库的文件为数据库信息提供真正的物理存储。 一. 控制文件为二进制文件，初始化大小由CREATE DATABASE指定,可以使用RMAN备份 记录了当前数据库的结构信息,同时也包含数据文件及日志文件的信息以及相关的状态,归档信息等等 在参数文件中描述其位置，个数等等。通常采用分散放开，多路复用的原则。在mount阶段被读取，open阶段一直被使用 维护数据库一致性(数据库启动时会比较控制文件与联机日志文件中的ckpt,即起始scn号，如相等则正常启动，否则需要介质恢复) 一个控制文件只能属于一个数据库 控制文件的任意修改将写入到初始化参数中指定的所有控制文件中，读取时则仅读取第一个控制文件 控制文件只能连接一个数据库，控制文件的大小一般不要超过MB,最多为个，最少一个，互为镜像 控制文件中包含的内容 数据库的名字、ID、创建的时间戳 表空间的名字 联机日志文件、数据文件的位置、个数、名字 联机日志的Sequence号码 检查点的信息 撤销段的开始或结束 归档信息 备份信息 一、何时创建新的控制文件 a、在控制文件发生永久性的损坏且之前未对控制文件进行备份 b、需要修改控制文件中的某些内容 二、多路复用控制文件 Oracle推荐最好将控制文件分布在不同的物理磁盘上。如果由于磁盘故障导致控制文件发生损坏，与之相关联的实例应被关闭。一旦磁盘被修复，可以通过其他磁盘上的控制文件恢复损坏的控制文件。待恢复完成后实例就可以重新启动，不需要进行介质恢复。多路复用控制文件是如何进行工作的呢？1、数据库启动时，仅读取control_file中第一个参数文件的信息2、数据库打开时，将会向control_file中所有的控制文件更新信息3、在数据库操作期间，如果任意一个控制文件不可用，实例将无法操作。 三、备份与恢复控制文件1、何时需要备份控制文件： a、添加、删除或重命名数据文件 b、添加、删除一个表空间或修改表空间的读写状态 //在添加，删除或重命名的前后都需要进行备份吗？文档没有说明，我认为应该这样。 c、添加、删除重做日志文件或组 2、备份控制文件的方法如下： a、以二进制文件的形式备份控制文件：alter database backup controlfile to ‘/u02/backup’; b、以SQL语句的形式备份控制文件便于以后可以重建：alter database backup controlfile to trace;该备份的存放位置可以通过查看alert告警日志获知。 3、恢复控制文件的方法如下： a、假设control_file参数中的一个参数文件发生损坏，但是控制文件的存放目录仍然可以访问，这时可以：关闭数据库–&gt;cp控制文件另一副本覆盖损坏控制文件–startup b、假设control_file参数中的一个参数文件发生发生介质损坏，这时可以：关闭数据库 –在新介质上恢复控制文件 –修改control_file参数 –startup4、删除控制文件 关闭数据库 –&gt; 修改control_file参数(删除对于信息) –&gt; startup，该系列操作不会删除操作系统上的物理文件，需要手动删除。 实验:rac+asm控制文件的多路复用 SQLshow parameter control_files; $ srvctl stop database -d CHAOS #关闭集群数据库 SQL&gt; startup nomount; # 单节点启动到nomount状态 使用rman连接nomount状态的数据库并且备份控制文件到指定路径 $ rman target / RMAN&gt; restore controlfile to &#39;+ORADATA/chaos/controlfile/current.260&#39; from &#39;+ORADATA/chaos/controlfile/current.260.958910433&#39;; 进入asmcmd，可以看到已经在目标路径生成备份 su - grid $ export ORACLE_SID=+ASM $ asmcmd ASMCMD&gt; cd oradata/chaos/controlfile ASMCMD&gt; ls SQL&gt; alter system set control_files=&#39;+oradata/chaos/controlfile/current.270.963936137&#39;,&#39;+oradata/chaos/controlfile/current.260.958910433&#39; scope=spfile sid=&#39;*&#39;; #自动生成了两个备份文件curren.260和current.270.963936137，用哪个都一样 $ srvctl start database -d CHAOS #这个时候已经完成了对rac集群所有节点控制文件的多路复用 二.数据文件 每个数据库有一个或多个物理的数据文件。逻辑数据库结构（如表、索引等）的数据物理地存储在数据库的数据文件中，数据文件通常为*.dbf格式。数据文件包含数据库中的实际数据，是数据库操作中数据的最终存储位置数据文件有下列特征： 1、一个数据文件仅与一个数据库联系； 2、一旦建立，数据文件只增不减； 3、一个表空间（数据库存储的逻辑单位）由一个或多个数据文件组成。 Oracle数据库在逻辑上是由多个表空间组成的，表空间在物理上包含一个或多个数据文件。而数据文件大小是块大小的整数倍；表空间中存储的对象叫段，比如数据段，索引段和回退段。段由区组成，区是磁盘分配的最小单位。段的增大是通过增加区的个数来实现的。每个区的大小是数据块大小的整数倍，区的大小可以不相同；数据块是数据库中的最小的I/O单位，同时也是内存数据缓冲区的单位，及数据文件存储空间单位。块的大小由参数DB_BLOCK_SIZE设置，其值应设置为操作系统块大小的整数倍。 最后再来说一下Oracle的用户、表空间和数据文件之间的关系： 一个用户可以使用一个或多个表空间，一个表空间也可以供多个用户使用。用户和表空间没有隶属关系，表空间是一个用来管理数据存储的逻辑概念，表空间只和数据文件存在关系，数据文件是物理的，一个表空间可以包含多个数据文件，而一个数据文件只能隶属一个表空间。 解释数据库、表空间、数据文件、表、数据的最好办法，就是想象一个装满东西的柜子，数据库其实就是柜子，柜中的抽屉是表空间，抽屉中的文件夹是数据文件，文件夹中的纸是表，写在纸上的信息就是数据。 表空间：是一个或多个数据文件的逻辑集合表空间逻辑存储对象： –永久段:如表与索引 –临时段:如临时表数据与排序段 –回滚段:用于事物回滚或闪回内存的撤销数据 表空间分类：系统表空间(system、sysaux)，非系统表空间,一个表空间至少包含一个数据文件，一个数据文件只能属于一个表空间。 –SYSTEM :字典表空间，不能被损坏 –UNDO :dml,ddl把数据快照到此，数据提交即消失（用于恢复) –SYSAUX :10g 高并发系统繁忙时，会造成system争用，将工具放到SYSAUX,减轻system的压力，SYSAUX不影响系统（影响性能） –TEMP :临时数据相关的内容 –USERS :10g 用户数据从system拨离出来 逻辑结构是Oracle内部管理数据库中对象的方式database数据库—&gt;tablespace表空间—&gt; segment段—&gt;extent区间—-&gt; block块 Schema: 用户—&gt;创建相关对象、表、视图、序列、函数、存储过程、包等 举例描述scott用户创建对象的组织方式,scott用户创建一张emp表，数据定义于system，数据逻辑存储于user表空间: 表段: 区间: 内存块/索引段: 区间: 内存块,user表空间物理存储于user01.dbf数据文件，采用本地管理，包含头部信息，可用，已用等位图信息，buffer cache满或者commit之后dbwr进程将数据从内存写到物理文件中 `SQL&gt; select username,default_tablespace,temporary_tablespace from dba_users where username=&apos;SCOTT&apos;;` SQL&gt; select t1.name tbname,t2.name from v$tablespace t1,v$datafile t2 where t1.ts# = t2.ts#;#查看当前数据库所有表空间及其数据文件路径 SQL&gt; select file_name,tablespace_name from dba_data_files; # 某人竟然嘲讽我，哼哼 表空间管理创建表空间的条件 1.具有create tablespace权限，dba，sysdba，sysoper拥有改权限，可授予 2.创建的是bigfile/smallfile，超过T级应考虑bigfile 3.新建的表空间的I/O，是否会导致磁盘I/O不够用 4.oracle需要具有创建表空间时指定datafile路径的写权限 SQL&gt; select PROPERTY_NAME,PROPERTY_VALUE from database_properties where PROPERTY_NAME like &#39;%TBS%&#39;; #查看表空间创建缺省状态时bigfile还是smallfile SQL&gt; alter database set default bigfile/smallfile tablespace; #修改创建表空间缺省为大/小文件状态 大表文件（bigfile)最大可以存放个T的容量。头文件的大小达到了G－－＞block,普通的头文件大小为M—-&gt;block。好处：减少了数据文件的个数，管理方便，大的对象的存放得到了优化。减少了control文件的信息，控制文件定义了datafile的个数。bigfile只能存在一个数据文件，所以要保证分配的的磁盘具有足够的空间。 SQL&gt; create tablespace TBS1 datafile &apos;+ORADATA/chaos/datafile/tbs1.dbf&apos; size 100m; #创建数据文件路径为 &apos;~tbs1.dbf&apos;的名为TBS1的表空间 SQL&gt; create temporary tablespace TMP1 tempfile &apos;+ORADATA/chaos/datafile/tmp1.dbf&apos; size 10m; 创建数据文件路径为 &apos;~tmp1.dbf&apos;的名为TBS2的临时表空间，临时表空间文件不能设置为只读，不能重命名数据文件，日志方式总是nologing，主要用途是在数据库进行排序运算，管理索引，访问视图等操作时提供临时的运算空间，当运算完成之后系统会自动清理，因为用途不同所以才有了默认表空间和临时表空间区分，实际上数据库都是有默认临时空间的，但实际应用中很难满足需求，所以才需要自己创建临时空间。 SQL&gt; alter database tempfile &apos;+ORADATA/chaos/tempfile/tmp1.dbf&apos; resize 15m; #重置大小 SQL&gt; alter database tempfile &apos;+ORADATA/chaos/tempfile/tmp1.dbf&apos; autoextend on next 10m maxsize 100m;#打开自动扩展 SQL&gt; alter tablespace tmp1 add tempfile &apos;+ORADATA/chaos/datafile/tmp2.dbf&apos; size 10m; #给临时表空间增加临时文件，增加到表空间中的数据文件不能直接从表空间中删除，除非删掉整个表空间，增加数据文件将有助于均衡I/O SQL&gt; alter database default temporary tablespace tmp1; #设置tmp1为默认临时表空间，如果没有指定默认临时表空间，那么将使用system表空间作为排序区 SQL&gt; SELECT dbms_metadata.get_ddl(&apos;TABLESPACE&apos;,&apos;SYSTEM&apos;) FROM dual; #查看创建表空间语句 SQL&gt; CREATE UNDO TABLESPACE tablespace_name DATAFILE &apos;+ORADATA/chaos/datafile/undotbs2.dbf&apos; size 10m; #创建undo表空间 SQL&gt; ALTER SYSTEM SET UNDO_TABLESPACE=tablespace_name; #修改默认undo表空间 SQL&gt; alter tablespace TABLESPACE_NAME rename to TBS2; # 表空间改名 undo表空间扩容，重置表空间大小，添加数据文件，自动扩展，跟临时表空间操作没有区别 SQL&gt; drop tablespace tbs2； #删除表空间，注意，system，sysaux，user表空间强烈不建议删除，会崩，默认表空间和当前undo表空间无法删除 表空间的管理方式 SQL&gt; select TABLESPACE_NAME,EXTENT_MANAGEMENT,BLOCK_SIZE,STATUS,CONTENTS,FORCE_LOGGING,BIGFILE from dba_tablespaces; 字典管理:字典管理表空间-DMT，指oracle的表空间分配和回收是通过数据库中的数据字典表来记录和管理，用于管理的两个数据字典分别是 UET$（used extents)和FET$(freeextents),其工作方式是当建立一个新的段或者表空间时，oracle通过一系列SQL语句来完成这个工作，更新上述两个字点的信息，在繁忙系统中会造成竞争和等待（另一个DMT会带来的问题是空间碎片） 本地管理:LMT的表空间数据文件头部加入了一个位图区域，在其中记录每个extent的使用状况，当extent被使用或者被释放，oracle会更新头部的记录来反映这个变化，不产生回滚信息，因为仅仅操作数据文件头部的几个数据块，不用操作数据字典，LMT比DMT要快，尤其是数据库繁忙的时候更明显 通过使用PL/SQL 完成表空间转换 exec dbms_space_admin.tablespace_migrate_to_local(&apos;USERS&apos;); exec dbms_space_admin.tablespace_migrate_from_local(&apos;USERS&apos;); 表空间的四种状态：online offline read only read write 一个表空间的正常状态是联机（online），有时需要将某表空间进行脱机，以进行数据库维护 如：在数据打开的状态下移动数据文件，恢复一个表空间或数据文件,执行表空间的脱机备份，在数据库正常工作情况下使数据库的某部分不可访问 SQL&gt; alter tablespace tbs1 offline; SQL&gt; alter tablespace tbs1 online; read only状态不能执行DML语句，可以使用DDL，DQL语句 SQL&gt; alter tablespace tbs1 read only; SQL&gt; alter tablespace tbs1 read write; system 必须online 必须read write sysaux 可以offline 不能read only undo 不能offline 不能read only SQL&gt; select tablespace_name,file#,v.status,v.enabled from dba_data_files d,v$datafile v where d.file_id = v.file#; #查看表空间的状态 表空间重定位 open且archive状态，先offline，移动数据文件位置，修改控制文件该数据文件路径，online 表空间相关视图表空间数据字典视图:DBA_TABLESPACES 表空间动态性能视图:V$TABLESPACE 数据字典视图:DBA_DATA_FILES 动态性能视图:V$DATAFILE 数据字典视图:DBA_TEMP_FILES 动态性能视图:V$TEMPFILE 被段分配的区:DBA_EXTENTS 没有被段分配的区: DBA_FREE_SPACE 系统表空间默认设置:database_properties 三.归档文件 在重做日志分成2部分，一个是在线重做日志文件，另外一个就是归档日志文件。在线重做日志大小毕竟是有限的，当都写满了的时候，就面临着2个选择，第一个就是把以前在线重做日志从头擦除开始继续写，第二种就是把以前的在线重做日志先进行备份，然后对被备份的日志擦除开始写新的在线Redo File。这种备份的在线重做日志就是归档日志。而数据库如果采用这种生成归档日志的模式的话，就是归档日志模式(ARCHIVELOG模式)，反之如果不生成归档日志，就是非归档日志模式(NOARCHIVELOG模式)。 归档日志(Archive Log)是非活动的重做日志备份.通过使用归档日志,可以保留所有重做历史记录,当数据库处于ARCHIVELOG模式并进行日志切换式,后台进程ARCH会将重做日志的内容保存到归档日志中.当数据库出现介质失败时,使用数据文件备份,归档日志和重做日志可以完全恢复数据库. 实验: rac环境下修改归档模式rac环境的归档模式切换和单实例稍有不同，主要是共享存储所产生的差异，将rac数据库切换到非集群状态下，在一个实例上实施归档模式切换即可完成rac数据库归档模式转换问题，非切归与归切非为镜像操作，这里仅描述切成归档模式的具体操作 1.主要步骤 备份spfile，防止参数修改失败导致数据库无法启动 修改集群参数cluster_database为false 关闭集群数据库 启动单实例到mount状态 修改该实例为归档模式（alter database archivelog/noarchivelog） 修改集群参数cluster_database为true 关闭单实例，启动集群数据库 2.操作 查看归档模式 SQL&gt; archive log list; SQL&gt; select instance_name,host_name,status from gv$instance; SQL&gt; show parameter cluster； SQL&gt; create pfile=&apos;/u01/app/oracle/spfileback.ora&apos; from spfile; #备份spfile SQL&gt; alter system set cluster_database=false scope=spfile sid=&apos;*&apos;; #修改为非集群数据库，该参数为静态参数，需要使用scope=spfile 退出sql命令行在linux命令行下面执行集群和单节点的起停操作 $ srvctl stop database -d CHAOS#停集群数据库 $ srvctl start instance -d CHAOS -i CHAOS1 -o mount #起单个实例到mount状态 SQL&gt; select instance_name,status from v$instance; #查看启动的单实例的数据库的状态，这个时候应该是mounted状态 SQL&gt; alter database archivelog; #在mount状态下设置该单实例为归档模式 SQL&gt; alter system set cluster_database=true scope=spfile sid=&apos;*&apos;; #修改为集群数据库 $ srvctl stop instance -d CHAOS -i CHAOS1#关闭当前操作的单实例数据库 $ srvctl start database -d CHAOS#起集群数据库 以上操作按顺序执行，已经在本机实验环境中实际操作验证，执行完最后的起集群操作之后双节点上数据库都处于open状态，并成功修改为归档模式。双节点实验环境是这样，实际生产环境多节点操作也是同理，多节点rac修改归档模式笨办法就是挨个节点启动到mount状态并修改，好处是不影响其他节点的工作，缺点是工作量略大，并且得执行节点数*2的起停数据库操作，风险稍大。这样修改集群状态，更改单实例归档模式，让其他节点自动同步的方式不管有多少节点都一次到位，并且只要操作一次集群的起停操作和一次单实例的起停操作，安全高效 一些归档文件的操作 SQL&gt; alter system switch logfile; # 手工切归档 SQL&gt; select inst_id,name,thread#,sequence#,status from gv$archived_log; #查看归档日志 SQL&gt; select * from v$log;#查看v$log视图里的归档文件信息 SQL&gt; alter system set log_archive_dest = &apos;/u01/app/oracle/arch1/&apos; scope = spfile; #修改归档文件路径 SQL&gt; alter system set log_archive_duplex_dest = &apos;/u01/app/oracle/arch2&apos; scope = spfile; #归档到本机且大于等于两个归档位置 SQL&gt; alter system set log_archive_format = &apos;arch_%t_%s_%r.arc&apos;; #修改归档文件命名格式 归档日志相关视图v$archived_log #从控制文件中获得归档的相关信息 v$archive_dest #归档路径及状态 v$log_history #控制文件中日志的历史信息 v$archive_processes #归档相关的后台进程信息 rac起停操作进程正常如图，双节点RAC进程正常，端口监听正常，可以对外提供服务 停止HAS(High Availability Services)必须以root用户操作，每个rac节点都得执行，在执行过程中vip会不断漂移到正常节点，直到所有节点服务都被关闭，执行完成后所有节点1521端口不再监听，所有相关进程全部清除，集群无法被访问 # cd /u01/app/11.2/grid/bin # ./crsctl stop has -f 启动HASroot在每个rac节点执行起has的命令，可以在单一节点启动集群数据库 # crsctl start has $ srvctl status database -d racdb oracle用户 可以看到所有的rac相关进程都已经启动，可以通过sqlplus本地登陆，用开发工具访问vip也正常，这是一次完整的rac起停过程]]></content>
      <categories>
        <category>Oracle</category>
      </categories>
      <tags>
        <tag>Theory</tag>
        <tag>Database</tag>
        <tag>Original</tag>
        <tag>Part-transported</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[搭建个人博客]]></title>
    <url>%2F2017%2F11%2F14%2F%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%2F</url>
    <content type="text"><![CDATA[简介 Hexo 是一个快速、简洁且高效的博客框架。Hexo 使用 Markdown（或其他渲染引擎）解析文章，在几秒内，即可利用靓丽的主题生成静态网页。看官档是个好习惯，戳这里跟着官方的文档走清晰明确又全面。 很久前搭建过一个基于wordpress的博客，然而那个时候并没有写东西的兴趣，完成了环境搭建之后确定博客可用，实验过程可复现就没再管理。 最近一直在看世界历史，越发感觉到生产关系和生产资料在人类文明史发展过程中的核心地位，可以清楚得看到历史的脉络跟生产力的发展之间的关系. 做个对社会有贡献的人真的不是一句套话。生产资料的占有者，生产关系中的生产者，社会财富的创造者,这些人才真的有机会去在社会变革的浪潮中大幅度改变既定的人生轨迹。 在信息时代，在下一次科技革命的前夕，在即将到来注定是数字化信息化的世界里，一切皆数据。数据就是新的生产资料，用数据去形成各种特定功能的产品，解决问题，就是新世界的生产关系，就如同石器时代会做石斧，铁器时代掌握了冶金技术，化石能源时代掌握了开采石油并加工成产品，核能时代掌握了可控核聚变技术一样，在数据时代中掌握了跟数据相关的姿势，从数据的收集和传递，存储，到在线分析，机器学习，人工智能、从数据支持的企业商务决策到各种炫酷到不行的网页设计，前端展示。就算站不到浪潮之巅，苟全性命于盛世绰绰有余。 这个博客第一是把我的学习状态和进度拿出来晒一晒，即使没人看，也是对我自己的一种鼓励（ps：万一有人看到了呢），现在处于大数据职业技能树生成阶段，既然走在正确的路上，用点手段督促自己走快点。再一个就是分享一些技术文档，刚从菜鸟阶段提升了一点，踩过的坑，做过的事都摆出来，看看能不能帮到一起前进的同道中人，也算是一种展现自己价值的方式。 思路 要搭建一个自己的博客，可以选择类似CSDN之类的网站，网站提供现成服务和模板，作者编辑内容后提交，网站审核发布。也可以在自己的服务器上用博客模板搭建服务，编辑内容后发布，游客通过访问服务器来访问作者的博客。我在某服务商那里常年租借VPS，所以选择自己搭建。 搭建的时候可以选择wrodpress，hugo，jekyll,hexo等博客框架和生成静态网页的工具，这里选择使用的是hexo。确定了要做什么和怎么做，之后的事情就简单了。用xshell或CRT终端连接VPS，部署服务，编辑内容，测试，发布，博客就算搭建完成。搭建完成后得提供访问地址，总不能给别人一个IP和端口说“这是我的博客，跑在这个服务器这个端口上，欢迎访问”。一不专业，二不安全，建议申请域名进行域名解析，可以选择国内的域名，申请备案然后使用。笔者这里选择在freenom申请一个免费域名并设置解析VPS，达到通过域名直接访问博客的目的，同时因为是境外的域名，所以不涉及到备案，可以直接访问。还可以通过更改配置文件，添加公钥等步骤将博客设置托管到github上，通过github提供的服务来访问博客，好处是所有的内容都会推送一份到github上，哪怕服务器挂了也算有个备份。 环境 软件 下载地址 node.js 点击下载 git yum安装即可 hexo npm安装即可 nginx 点击下载 步骤一 安装 1. 安装node.js1rpm -ivh node-8.4.0-el6.x86_64.rpm 2. 安装git1yum install git 3. 安装hexo1npm install -g hexo-cli 4. 创建工作文件夹12hexo init myblogcd /myblog; npm install 5. 启动服务1hexo s (完整命令hexo server，可简写) 访问4000端口即可看到已经提供服务 二 使用 1.推一篇文章1hexo new &quot;My first post with hexo&quot; 该命令会在存放博客内容的文件夹（/myblog/source/_posts）内生成一个My first post with hexo.md文件,写入下文并使用hexo g(hexo generate)命令使hexo根据.md文件生成网页静态文件，hexo s 开启测试服务，访问4000端口即可看到新增的名为”My first post with hexo”的博客文章 123456789101112131415161718---title: My Fist Post with hexodate: 2016-09-25 20:03:25tags:---This my first post using [Hexo](https://hexo.io/)! ## First title### a first subtitile $ hexo new &quot;My New Post&quot; More info: [Writing](https://hexo.io/docs/writing.html) 2.将博客托管在github上github创建的仓库可以提供一个链接，将静态页面文件以网页的形式展示出来。在git里面创建该特殊的仓库，然后通过修改hexo的配置文件将托管形式改成git，使用hexo d(hexo deployment)将本地生成博客文件推送到仓库，便可以通过github提供的服务访问博客，大概可以分为以下三步 * 添加密钥在服务器生成密钥，添加到github里，使服务器和github可以进行文件的读写,具体操作步骤可以参考我的另一篇博客Build your own blog with hexo内容中相关部分 * 安装可以实现将hexo文件部署到git的组建1npm install hexo-deployer-git --save * 修改配置文件并将内容推送到git 修改配置文件 12将博文推送到github上hexo d 至此，在本地部署一个博客网站，托管到gitbhub上，可以通过链接 https://uxtuo.github.io 访问，get！ 3.设置域名并用nginx代理 将博客托管在github上和通过域名直接访问服务器的博客不冲突。hexo作为生成静态文件的工具，编辑内容并生成博客文件,推送并访问位于git上的文件，亦或者通过ip或者域名访问服务器上的文件没有区别，这里申请一个域名对vps的ip进行解析，使游客可以通过域名直接访问服务器上的博客，并用nginx做代理处理对服务器的请求。 * 申请域名并设置解析地址 在freenom申请免费域名，设置域名解析为vps地址 * 设置nginx做代理以应付可能的高并发访问安装nginx（下载压缩包后解压到/usr/local/目录即可使用），修改配置文件，使nginx代理对该服务器80端口的所有访问，设置nginx解析位于/myblog/public目录hexo生成的静态网页文件 这样就可以实现通过域名访问部署在VPS上的博客，get！ 本博客就是通过上文方法搭建，所有操作均通过实验，有兴趣的筒子们可以跟着做，绝对轻松愉快可操作，有问题可以发邮件给我（yung241088@126.com）,我一定会看但不一定会回/滑稽]]></content>
      <categories>
        <category>env</category>
      </categories>
      <tags>
        <tag>blog</tag>
        <tag>env</tag>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Build your own blog with hexo]]></title>
    <url>%2F2017%2F11%2F10%2Fwhat-is-hexo%2F</url>
    <content type="text"><![CDATA[What is Hexo? Hexo is a fast, simple and powerful blog framework. You write posts in Markdown (or other languages) and Hexo generates static files with a beautiful theme in seconds.Installing Hexo is quite easy. However, you do need to have Nodejs &amp; Git installed first.In order to install Nodejs you can see Install &amp; run your first application Nodejs.In order to install Git you can see https://git-scm.com/ . Install Hexo Once all the requirements are installed,you can install Hexo. $ npm install -g hexo-cli Create a blog Now that hexo is installed run the following commands to initialise Hexo project $ hexo init myblog $ cd myblog $ npm install You can modify site settings in _config.yml. for the sake of simplicity we�re only modify the Title and author name . Run the Blog Run the server: $ hexo server launch your browser and navigate to http://localhost:4000.Voila your first blog is working! Create a new post Create a new post is very simlpe all what you have to do is : $ hexo new “My Fist Post with hexo” Update the file using Markdown language: 123456789101112131415161718---title: My Fist Post with hexodate: 2016-09-25 20:03:25tags:---This my first post using [Hexo](https://hexo.io/)! ## First title### a first subtitile /usr/bin/bash $ hexo new "My New Post" ## Second titleMore info: [Writing](https://hexo.io/docs/writing.html) Run the server again: $ hexo server Deployment on Github Now what about Deployment, it�s exactly what we are going to do, first Create new Github repository : Click settings Then install hexo-deployer-git: $ npm install hexo-deployer-git –save Click clone or download button: Update _config.yaml file : It�s time for deployement : $ hexo deploy To preview launch your browser. You can get see the blog on https://malektrainer.github.io/.You can find source code on https://github.com/malektrainer/myblog.]]></content>
      <categories>
        <category>blog</category>
      </categories>
      <tags>
        <tag>blog</tag>
        <tag>server</tag>
        <tag>environment</tag>
        <tag>mark</tag>
      </tags>
  </entry>
</search>
