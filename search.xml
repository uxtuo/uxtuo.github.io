<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title><![CDATA[我们的时代]]></title>
      <url>/2018/03/15/%E6%88%91%E4%BB%AC%E7%9A%84%E6%97%B6%E4%BB%A3/</url>
      <content type="html"><![CDATA[<h1 id="简介"><a href="#简介" class="headerlink" title=" 简介 "></a><font color="#5CACEE"> 简介 </font></h1><blockquote>
<p>夫人之相与，俯仰一世，仰观宇宙之大，俯察品类之盛，所以游目骋怀，足以极视听之娱，信可乐矣！</p>
<p>这是最好的时代，也是最坏的时代，这是，我们的时代！</p>
</blockquote>
<a id="more"></a>
<h1 id="写作背景"><a href="#写作背景" class="headerlink" title=" 写作背景 "></a><font color="#5CACEE"> 写作背景 </font></h1><p>知名物理学家史蒂芬·霍金去世，享年76岁，（Stephen William Hawking，1942年1月8日–2018年3月14日），英国物理学家和宇宙学家，曾任职剑桥大学理论宇宙学中心研究主任。</p>
<p>霍金做出很多重要贡献，最主要的是他与罗杰·彭罗斯合作提出广义相对论框架内的彭罗斯-霍金奇性定理，以及他关于黑洞会发射辐射的理论预测(霍金辐射)。</p>
<p>1979年至2009年是剑桥大学的<a href="https://baike.baidu.com/item/%E5%8D%A2%E5%8D%A1%E6%96%AF%E6%95%B0%E5%AD%A6%E6%95%99%E6%8E%88%E5%B8%AD%E4%BD%8D/6830722?fr=aladdin" target="_blank" rel="external"><font color="#AAAAAA">卢卡斯数学教授</font></a>，撰写了多本阐述自己理论和宇宙猜想的科普著作，如《时间简史》，《果壳中的宇宙》等。</p>
<p>霍金患有肌萎缩性脊髓侧索硬化症，随着年月愈加严重，后半生全身瘫痪，在轮椅中度过半生。曾有过两次婚姻，育有三名子女，情感经历和病情发展可以看看2014年的电影<a href="https://baike.baidu.com/item/%E4%B8%87%E7%89%A9%E7%90%86%E8%AE%BA/14472057?fr=aladdin" target="_blank" rel="external"><font color="#AAAAAA">《万物理论》</font></a>，科学上的成就么，普通人知道很牛就是，至于有多牛，我也不是很清楚┑(￣Д ￣)┍</p>
<p>他<strong>一直</strong>没有停止思考！一个延伸到宇宙尽头时间终点的灵魂被困在了比正常人更小的牢笼里动弹不得，虽然对于他们这些思考范围横跨无尽时间空间的存在而言，肉体本身就是一个小得不能再小的牢笼，但是全身瘫痪一辈子也未免太残酷，连日常生活仅有的一点乐趣都被剥夺。或许他根本不在乎，肉体的享受跟思考宇宙的奥秘比起来不值一提，或许他在乎，在乎得不得了，但是没办法，只能靠绝大的毅力降伏心魔，将自己的所有才华，精力都投入无尽的思考，不得而知，我们看到的只有霍金身残志坚，全身瘫痪做出了令人瞩目的成就，得到了社会的认同和尊重，励志到不行，然而不是本人，永远无法体会他的痛苦，挣扎，直面一切地勇气。</p>
<p>事实上在我看来霍金先生已经很幸运了，他对人类在宇宙认识方面做出了巨大的贡献，值得尊敬，虽然身体残缺，但是这一生也算是事业有成，功成名就。历史上有更多奋斗在科学前线，穷尽一生为人类科学开疆拓土的先贤或者是走错了方向，或者是运气不好，或者是受到埋没，最终献出了所有光和热却无法得到大众的认同，有很多如果单论科技成果毫不逊色于霍金，然而这就是现实，霍金注定成为一个传奇，而那些先贤生前不被认同，死后不被大众所知。他们后悔吗，我觉得没有，总有人负重前行，总有人会站在文明的高度发着光，总有人不计利益地站出来。他们奉献一切，我们享受着成果，可以不知道，但要对这一切心存感激，心存敬畏。有一篇我特别特别喜欢的作者刘慈欣写的特别特别喜欢的科幻小说《伤心者》，相信我，你会哭的。</p>
<p>刷CSDN看到有篇文章提到霍金在一次演讲中提到对人工智能的看法，就联想到前阵子学过的自然辩证法里科技发展的部分，还有些有的没的，上班路上看着博客想了一路，一时兴起整理出来，可以肯定的是未来的我再次看到这篇文章的时候一定别有一番滋味，估计会疯狂吐槽现阶段的我思考角度太少太局限，思想幼稚吧哈哈哈，不管了，未来的我，你好(@^0^@)/</p>
<h1 id="我的思考"><a href="#我的思考" class="headerlink" title=" 我的思考 "></a><font color="#5CACEE"> 我的思考 </font></h1><p>这是最好的时代，生产力随着科技水平的提升指数增长，之前的250万年产生的财富占3%，而97%的财富是在工业革命后的这250年间产生的，人类这个物种在科技的辅助下飞速进步，</p>
<p>能源的使用从化学能，机械能，电能到现在的核能，可控核聚变，氢元素占整个宇宙质量的99.999%，可控核聚变是两个较轻的原子核聚合为一个较重的原子核并释放能量的过程，最容易实现的聚变反应是氢的同位素–氘和氚的聚变，这种反应在太阳上持续了50亿年，现在科学家们创造了一个个小太阳来为人类持续稳定的输出能量，当前还不成熟，但是已经有持续90s的可控核聚变，相信人类用上廉价稳定无废料无限的能量那天迟早到来，那时候人类就不用明明已经进入电力时代，还为古生物尸油–石油打生打死了，能源问题解决之后，人类的征途是宇宙群星！</p>
<p>互联网出现不到半个世纪，整个人类物种前所未有地成为了一个整体，计算机网络技术地出现使社会资源地分配效率，信息的沟通速度，世界上人类真正形成大规模的分工合作。整个人类文明如果单纯看作是地球上对各种资源进行整合利用维持自身生存的一个物种的话，那么计算机和互联网出现后这个物种对资源的使用和改造效率提升了无数倍，利用外界一切可用资源提升自身生存能力，发展文化，科技的能力得到极大提升。人类文明发展史如果从诞生到现在画个时间跨度万年记的曲线，那就是一条折线，前面几百万年近乎平行于x轴向前，最后一个小格直线蹿升。</p>
<p>量子通信，人工智能，物联网，智能汽车，各种用途的机器人，无人机，整个社会在享受了几百年科技进步的红利之后，对于新生科技几乎是无条件接受的状态，每时每刻都在发生着变革，信息化的步子越来越快，手机，个人电脑，包括控制各种设施的智能终端，人类的生活从未如此紧密地连接在一起。</p>
<p>前段时间学习了自然辩证法， 其中讲到了人类这个生物个体和不断提升的技术掌控的矛盾，人体很脆弱，很低效，技术发展，人类无法掌控，要么电子化，主动拥抱人工智能，将人工智能发展的所有资粮化为人类进化的养分，要么半机械化，用辅脑协助处理，用外接存储扩展大脑存储，用技术实现人对机械的绝对掌控，突然有点期待全民上传和脑后插管的那天是怎么回事Σ(っ °Д °;)っ！</p>
<p>如果只是一直发展信息化，自动化，发展人工智能，发展科技，总有一天人类会成为整个科技世界运行发展的瓶颈，失控的结果可能是毁灭性的。这就是霍金演讲中对人工智能的思考，不能一直致力于发展人工智能替代人类，完善机器人格，要多挖掘利用人工智能的现实应用来让促进社会和技术的发展，人工智能是为人类服务的，不是一群人类最精英的团体发展研究出电子生命来替代人类自身的！</p>
<p>这是最坏的时代，或许我们都看不到人工智能毁灭人类的那天(按最坏的猜想)，人类本身国家社会发展就有极多的矛盾，人类可以说是这个星球最嗜血的物种，整个人类史就是一部战争史，除了人类，没有任何一个物种会毫无理由单纯为了毁灭而去杀戮。核扩散，伊朗，朝鲜，美国，俄罗斯，有核武器的国家互相仇视，二战后的核毁灭压力造就了美国”毁掉的一代”，多国签订《核不扩散条约》后都积极控制核武器数量和拥有的国家，同时稳定的大国关系，都知道底线就是不能打核战争。现在呢，朝鲜有了核弹，日本有能力造出核弹，伊朗核问题又开始恶化，中东那边打成一锅粥，希望是我想多了吧。中东难民问题导致欧洲那边恐怖袭击不断，<strong>英国这两天跟俄国翻脸</strong>，大使馆旗都拔了，美国贸易保护，满世界经济制裁，人工智能的出现必然导致大量的岗位需求变化，创造大量就业人口同时会毁掉更多的岗位需求，这种科技变更引起的社会动荡是有先例的，参考<a href="http://baike.sogou.com/v70457882.htm?fromTitle=%E5%8B%92%E5%BE%B7%E5%88%86%E5%AD%90" target="_blank" rel="external"><font color="#AAAAAA">勒德分子</font></a>，那一天不远，希望引发的社会动荡不会引发太严重的后果。</p>
<p>作为一个普通人，看到了也是没有半点办法的，时代前进的车轮没人挡着住，连拥有站在前进道路上资格的人都少之又少。作为普通人只能说希望人类文明万古长青，走向星辰，不会被中途一些蝇营狗苟的事情毁掉。没办法，从历史发展的时间跨度来，个人的生老病死只是一瞬间，连朝生暮死都算不上，如果做不出极大的成就，就只是地球上已经死掉的几十亿无名人士之一，留不下痕迹，没有人记得。</p>
<p>我的存在只对我有意义，人生就是一种体验，活在当下，去吃去玩，去爱，有自己的目标，有想做的事，一边前进一遍享受路上的风景，珍惜吃的每个苹果，珍惜爱我的人，享受看书时的愉悦，跟朋友开黑的快乐，享受奋斗带来的压力，享受每一种让我觉得自己还活着的感觉。毕竟，如果不活得敏感点，多珍惜眼下拥有的，哪天不知道出了啥事，比如下一秒核弹爆炸，战争开打之类的。</p>
<p><strong>夫人之相与，俯仰一世，仰观宇宙之大，俯察品类之盛，所以游目骋怀，足以极视听之娱，信可乐矣！</strong></p>
<h1 id="附录-霍金在-GMIC-2017-开幕式上的主题演讲"><a href="#附录-霍金在-GMIC-2017-开幕式上的主题演讲" class="headerlink" title=" 附录-霍金在 GMIC 2017 开幕式上的主题演讲 "></a><font color="#5CACEE"> 附录-霍金在 GMIC 2017 开幕式上的主题演讲 </font></h1><blockquote class="blockquote-center"><p>让人工智能造福人类及其赖以生存的家园 </p>
</blockquote>
<p>在我的一生中，我见证了社会深刻的变化。其中最深刻的，同时也是对人类影响与日俱增的变化，是人工智能的崛起。简单来说，我认为强大的人工智能的崛起，要么是人类历史上最好的事，要么是最糟的。我不得不说，是好是坏我们仍不确定。但我们应该竭尽所能，确保其未来发展对我们和我们的环境有利。我们别无选择。我认为人工智能的发展，本身是一种存在着问题的趋势，而这些问题必须在现在和将来得到解决。</p>
<p>人工智能的研究与开发正在迅速推进。也许我们所有人都应该暂停片刻，把我们的研究重复从提升人工智能的能力转移到最大化人工智能的社会效益上面。基于这样的考虑，美国人工智能协会（AAAI）于 2008 至 2009 年成立了人工智能长期未来总筹论坛，他们近期在目的导向的中性技术上投入了大量的关注。但我们的人工智能系统须要按照我们的意志工作。跨学科研究是一种可能的前进道路：从经济、法律、哲学延伸至计算机安全、形式化方法，当然还有人工智能本身的各个分支。</p>
<p>潜在的威胁：计算机智能与我们没有本质区别</p>
<p>文明所提产生的一切都是人类智能的产物，我相信生物大脑可以达到的和计算机可以达到的，没有本质区别。因此，它遵循了“计算机在理论上可以模仿人类智能，然后超越”这一原则。但我们并不确定，所以我们无法知道我们将无限地得到人工智能的帮助，还是被藐视并被边缘化，或者很可能被它毁灭。的确，我们担心聪明的机器将能够代替人类正在从事的工作，并迅速地消灭数以百万计的工作岗位。</p>
<p>在人工智能从原始形态不断发展，并被证明非常有用的同时，我也在担忧创造一个可以等同或超越人类的事物所导致的结果：人工智能一旦脱离束缚，以不断加速的状态重新设计自身，人类由于受到漫长的生物进化的限制，无法与之竞争，将被取代，这将给我们的经济带来极大的破坏。未来，人工智能可以发展出自我意志，一个与我们冲突的意志。尽管我对人类一贯持有乐观的态度，但其他人认为，人类可以在相当长的时间里控制技术的发展，这样我们就能看到人工智能可以解决世界上大部分问题的潜力。但我并不确定。</p>
<p>2015 年 1 月份，我和科技企业家埃隆·马斯克，以及许多其他的人工智能专家签署了一份关于人工智能的公开信，目的是提倡就人工智能对社会所造成的影响做认真的调研。在这之前，埃隆·马斯克就警告过人们：超人类人工智能可能带来不可估量的利益，但是如果部署不当，则可能给人类带来相反的效果。我和他同在“生命未来研究所（Future of Life Institute）”的科学顾问委员会，这是一个为了缓解人类所面临的存在风险而设立的组织，而且之前提到的公开信也是由这个组织起草的。这个公开信号召大家展开可以阻止潜在问题的直接研究，同时也收获人工智能带给我们的潜在利益，并致力于让人工智能的研发人员更关注人工智能安全。此外，对于决策者和普通大众来说，这封公开信内容翔实，并非危言耸听。人人都知道人工智能研究人员们在认真思索这些担心和伦理问题，我们认为这一点非常重要。比如，人工智能是有根除疾患和贫困的潜力的，但是研究人员必须能够创造出可控的人工智能。那封只有四段文字，题目为《应优先研究强大而有益的人工智能》（“Research Priorities for Robust and Beneficial Artificial Intelligence”）的公开信，在其附带的十二页文件中对研究的优先次序作了详细的安排。</p>
<p>如何从人工智能中获益并规避风险</p>
<p>在过去的 20 年里，人工智能一直专注于围绕建设智能代理所产生的问题，也就是在特定环境下可以感知并行动的各种系统。在这种情况下，智能是一个与统计学和经济学相关的理性概念。通俗地讲，这是一种做出好的决定、计划和推论的能力。基于这些工作，大量的整合和交叉被应用在人工智能、机器学习、统计学、控制论、神经科学、以及其它领域。共享理论框架的建立，结合数据的供应和处理能力，在各种细分的领域取得了显著的成功，例如语音识别、图像分类、自动驾驶、机器翻译、步态运动和问答系统。</p>
<p>随着这些领域的发展，从实验室研究到有经济价值的技术形成良性循环。哪怕很小的性能改进，都会带来巨大的经济效益，进而鼓励更长期、更伟大的投入和研究。目前人们广泛认同，人工智能的研究正在稳步发展，而它对社会的影响很可能扩大，潜在的好处是巨大的，既然文明所产生的一切，都是人类智能的产物。由于这种智能是被人工智能工具放大过的，我们无法预测我们可能取得什么成果。但是，正如我说过的那样，根除疾病和贫穷并不是完全不可能，由于人工智能的巨大潜力，研究如何（从人工智能中）获益并规避风险是非常重要的。</p>
<p>现在，关于人工智能的研究正在迅速发展，这一研究可以从短期和长期两个方面来讨论。</p>
<p>短期的担忧主要集中在无人驾驶方面，包括民用无人机、自动驾驶汽车等。比如说，在紧急情况下，一辆无人驾驶汽车不得不在小概率的大事故和大概率的小事故之间进行选择。另一个担忧则是致命性的智能自主武器。他们是否该被禁止？如果是，那么“自主”该如何精确定义；如果不是，任何使用不当和故障的过失应该如何问责。此外还有一些隐忧，包括人工智能逐渐可以解读大量监控数据引起的隐私问题，以及如何掌控因人工智能取代工作岗位带来的经济影响。</p>
<p>长期担忧主要是人工智能系统失控的潜在风险。随着不遵循人类意愿行事的超级智能的崛起，那个强大的系统会威胁到人类。这样的结果是否有可能？如果有可能，那么这些情况是如何出现的？我们又应该怎样去研究，以便更好地理解和解决危险的超级智能崛起的可能性？</p>
<p>当前控制人工智能技术的工具（例如强化学习）以及简单实用的功能，还不足以解决这个问题。因此，我们需要进一步研究来找到和确认一个可靠的解决办法来掌控这一问题。</p>
<p>近来（人工智能领域的）里程碑，比如说之前提到的自动驾驶汽车，以及人工智能赢得围棋比赛，都是未来趋势的迹象。巨大的投入正在倾注到这一领域。我们目前所取得的成就，和未来几十年后可能取得的成就相比，必然相形见绌。而且当我们的头脑被人工智能放大以后，我们更不能预测我们能取得什么成就。也许在这种新技术革命的辅助下，我们可以解决一些工业化对自然界造成的损害问题，关乎到我们生活的各个方面也即将被改变。简而言之，人工智能的成功有可能是人类文明史上最大的事件。</p>
<p>但是人工智能也有可能是人类文明史的终结，除非我们学会如何避免危险。我曾经说过，人工智能的全方位发展可能招致人类的灭亡，比如最大化使用智能性自主武器。今年早些时候，我和一些来自世界各国的科学家共同在联合国会议上支持其对于核武器的禁令，我们正在焦急的等待协商结果。目前，九个核大国可以控制大约一万四千个核武器，它们中的任何一个都可以将城市夷为平地，放射性废物会大面积污染农田，而最可怕的危害是诱发核冬天，火和烟雾会导致全球的小冰河期。这一结果将使全球粮食体系崩塌，末日般动荡，很可能导致大部分人死亡。我们作为科学家，对核武器承担着特殊的责任，因为正是科学家发明了它们，并发现它们的影响比最初预想的更加可怕。</p>
<p>现阶段，我对灾难的探讨可能惊吓到了在座的各位，很抱歉。但是作为今天的与会者，重要的是，你们要认清自己在影响当前技术的未来研发中的位置。我相信我们会团结在一起，共同呼吁国际条约的支持或者签署呈交给各国政府的公开信，科技领袖和科学家正极尽所能避免不可控的人工智能的崛起。</p>
<p>去年10月，我在英国剑桥建立了一个新的机构，试图解决一些在人工智能研究快速发展中出现的尚无定论的问题。“利弗休姆智能未来中心（The Leverhulme Centre for the Future of Intelligence）”是一个跨学科研究所，致力于研究智能的未来，这对我们文明和物种的未来至关重要。我们花费大量时间学习历史，深入去看，大多数是关于愚蠢的历史，所以人们转而研究智能的未来是令人欣喜的变化。虽然我们对潜在危险有所意识，但我内心仍秉持乐观态度，我相信创造智能的潜在收益是巨大的。也许借助这项新技术革命的工具，我们将可以削减工业化对自然界造成的伤害。</p>
<p>确保机器人为人类服务</p>
<p>我们生活的每一个方面都会被改变。我在研究所的同事休·普林斯（Huw Price）承认，“利弗休姆中心”能建立，部分是因为大学成立了“存在风险中心（Centre for Existential Risk）”。后者更加广泛地审视了人类潜在问题，“利弗休姆中心”的重点研究范围则相对狭窄。</p>
<p>人工智能的最新进展，包括欧洲议会呼吁起草一系列法规，以管理机器人和人工智能的创新。令人感到些许惊讶的是，这里面涉及到了一种形式的电子人格，以确保最有能力和最先进的人工智能的权利和责任。欧洲议会发言人评论说，随着日常生活中越来越多的领域日益受到机器人的影响，我们需要确保机器人无论现在还是将来，都为人类而服务。一份向欧洲议会议员提交的报告明确认为，世界正处于新的工业机器人革命的前沿。报告分析了是否给机器人提供作为电子人的权利，这等同于法人（的身份）。报告强调，在任何时候，研究和设计人员都应确保每一个机器人设计都包含有终止开关。在库布里克的电影《2001 太空漫游》中，出故障的超级电脑哈尔没有让科学家们进入太空舱，但那是科幻，我们要面对的则是事实。奥斯本·克拉克（Space Odyssey）跨国律师事务所的合伙人——洛纳·布拉泽尔（Lorna Brazell）在报告中说，我们不承认鲸鱼和大猩猩有人格，所以也没有必要急于接受一个机器人人格，但是担忧一直存在。报告承认在几十年的时间内，人工智能可能会超越人类智力范围，进而挑战人机关系。报告最后呼吁成立欧洲机器人和人工智能机构，以提供技术、伦理和监管方面的专业知识。如果欧洲议会议员投票赞成立法，该报告将提交给欧盟委员会，它将在三个月的时间内决定要采取哪些立法步骤。</p>
<p>我们还应该扮演一个角色，确保下一代不仅仅有机会还要有决心，能够在早期阶段充分参与科学研究，以便他们继续发挥潜力，并帮助人类创造一个更加美好的的世界。这就是我刚谈到学习和教育的重要性时所要表达的意思。我们需要跳出“事情应该如何（how things should be）”这样的理论探讨，并且采取行动，以确保他们有机会参与进来。我们站在一个美丽新世界的入口，这是一个令人兴奋的、同时充满了不确定性的世界，而你们是先行者，我祝福你们。</p>
]]></content>
      
        <categories>
            
            <category> 思考 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 霍金 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[module]]></title>
      <url>/2018/03/14/%E6%A0%BC%E5%BC%8F/</url>
      <content type="html"><![CDATA[<h1 id="简介"><a href="#简介" class="headerlink" title=" 简介 "></a><font color="#5CACEE"> 简介 </font></h1><blockquote>
<p>Oracle Database，又名Oracle RDBMS，或简称Oracle。是甲骨文公司的一款关系数据库管理系统。它是在数据库领域一直处于领先地位的产品。可以说Oracle数据库系统是目前世界上流行的关系数据库管理系统，系统可移植性好、使用方便、功能强，适用于各类大、中、小、微机环境。它是一种高效率、可靠性好的 适应高吞吐量的数据库解决方案。</p>
</blockquote>
<a id="more"></a>
<table>
<thead>
<tr>
<th>软件</th>
<th>下载地址</th>
</tr>
</thead>
<tbody>
<tr>
<td>Google云计算三大论文英文版</td>
<td><a href="https://pan.baidu.com/s/1dFOyXOX" target="_blank" rel="external"><font color="#AAAAAA">点击下载</font></a></td>
</tr>
<tr>
<td>Google-File-System中文版</td>
<td><a href="https://pan.baidu.com/s/1eShdCKa" target="_blank" rel="external"><font color="#AAAAAA">点击下载</font></a></td>
</tr>
<tr>
<td>Google-MapReduce中文版</td>
<td><a href="https://pan.baidu.com/s/1jIOoRaA" target="_blank" rel="external"><font color="#AAAAAA">点击下载</font></a></td>
</tr>
<tr>
<td>Google-Bigtable中文版</td>
<td><a href="https://pan.baidu.com/s/1geX8jLL" target="_blank" rel="external"><font color="#AAAAAA">点击下载</font></a></td>
</tr>
</tbody>
</table>
<h1 id="一级目录"><a href="#一级目录" class="headerlink" title=" 一级目录 "></a><font color="#5CACEE"> 一级目录 </font></h1><h2 id="二级目录"><a href="#二级目录" class="headerlink" title=" 二级目录 "></a><font color="green"> 二级目录 </font></h2><h3 id="三级目录"><a href="#三级目录" class="headerlink" title=" 三级目录 "></a><font color="purple"> 三级目录 </font></h3><h4 id="四级目录"><a href="#四级目录" class="headerlink" title="四级目录"></a>四级目录</h4><table>
<thead>
<tr>
<th>软件</th>
<th>下载地址</th>
</tr>
</thead>
<tbody>
<tr>
<td>Google云计算三大论文英文版</td>
<td><a href="https://pan.baidu.com/s/1dFOyXOX" target="_blank" rel="external"><font color="#AAAAAA">点击下载</font></a></td>
</tr>
</tbody>
</table>
<blockquote class="blockquote-center"></blockquote>
<font face="黑体">我是黑体字</font>

<font face="微软雅黑">我是微软雅黑</font>

<font face="STCAIYUN">我是华文彩云</font>

<font color="#0099ff" size="7" face="黑体">color=#0099ff size=72<br>face=”黑体”</font>

<font color="#00ffff" size="72">color=#00ffff</font>

<font color="red">color=gray</font>

<p><font color="#5CACEE">What is Hexo?</font></p>
]]></content>
      
        <categories>
            
            <category> Database </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Oracle </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Book Summary系列写作计划]]></title>
      <url>/2018/03/01/Book-Summary%E5%86%99%E4%BD%9C%E8%AE%A1%E5%88%92/</url>
      <content type="html"><![CDATA[<h1 id="简介"><a href="#简介" class="headerlink" title=" 简介 "></a><font color="#5CACEE"> 简介 </font></h1><blockquote>
<p>“Book Summary”-书的概述，预期是写成一个系列，主要内容有研究生课程学习的课本，对我的思想有重大启发的纪录片和文章书籍，包括不限于历史，宇宙，文化，才艺等等，工作中自学或用到的技术书籍，当前买了很多实体书，oracle/mysql/python/SQL/Spark/网络，还有”多看”上的电子书，没有人督促来读，也没有指标，进度很慢，准备在通读之后把书的内容整理出来，一是记录，二是分享。</p>
</blockquote>
<a id="more"></a>
<h1 id="规划"><a href="#规划" class="headerlink" title=" 规划 "></a><font color="#5CACEE"> 规划 </font></h1><p>信息社会，知识已经不像以前那么宝贵，茫茫多的免费资料，各个机构分享出来的技术资料俯拾皆是，<strong>学习能力</strong>反而成了个人提升的瓶颈，就比如欧洲黑暗中世纪和我国的封建时代，皇权和神权不约而同奉行愚民政策，封锁知识流动，将社会极大部分人的知识层次和眼光局限在较低层次对社会和权力的稳定有奇效，然而近代科技爆炸，知识和信息在全社会无障碍流通，假设有一个人有无上限的学习能力和精力，那么就跟电影<code>《永无止境》</code>里一样，很容易就能突破一切局限，乐器，搏击，商业，政治，社交，任何方面都能极快地获得超凡成就。然而很遗憾，人力有穷，现实中的我们困在百十来斤的肉体中，困了要睡，伤了会痛，饿了要吃，高兴会笑伤心会哭，肉体的吃喝拉撒和情绪的喜怒哀乐这种本能的存在是生活多姿多彩的因素，是人类欲望的根源，是不可缺少的部分。</p>
<p>但对个人的自我学习来说，极为不利，当前我的解决方案是持<a href="https://baike.baidu.com/item/%E4%B8%AD%E5%BA%B8%E4%B9%8B%E9%81%93/873892" target="_blank" rel="external"><font color="#AAAAAA">中庸之道</font></a>，乐而不淫，哀而不伤，形成稳定的人生观和世界观，冷静的处理生活，工作和生活节奏，同时<code>&quot;stay hungry,stay curious&quot;</code>,保持旺盛的好奇心和学习的心态，做一个<strong>终身学习者</strong>。结果上来说，能让人生轨迹的弧线往上翘的几率大那么一丢丢，而且过程中的压力，动力，成就，痛苦，不都是让人生更充实的资粮么，学习的过程本就是一种享受啊！</p>
<p>原书作者在写书的时候付出了大量的时间和脑力劳动，出版商负责印刷，排版，出版发行，最后根据销量收入来调整书的发行数量和再版次数，付出不菲，然而大部分市面上的技术书籍价格跟价值比起来简直就是<strong>白菜价</strong>，如果销量还是一片惨淡，那可能负反馈太强烈以至于作者觉得写了书没人看，出版社赚不到钱不发行，结果就是好书越来越少，所以我建议手头不是仓促到下一顿饭不知道在哪的朋友尽量买正版书支持一下，让作者有动力写更多更优质的内容出来，这样就能学到更多干货吖\^o^/，当然也可以自己去网上找资源，pdf和免费资源多得是，只要认真看从中学到东西了，作者应该也会高兴地，不用有看盗版的心理负担，没关系的。</p>
<p>笔者主要阅读途径有三：实体书，”多看阅读”，bilibili纪录片。比如让我兴起写这个读书笔记系列念头的《Spark最佳实践》就是在“多看阅读”上趁着折扣30块钱买的，实体书阅读体验好但会稍贵，大概40-50，京东淘宝亚马逊等平台都有售，而一些python，oracle，爬虫等实体书大部分是在亚马逊买的，bilibili的纪录片资源很多，各个方面层次都有，比如清华的大数据课程，小象集团的spark课程，宇宙如何运行等等琳琅满目，免费无广告摆放整齐，力荐！</p>
<p>总之，有条件就支持下作者，免费看了书专门去买书支持一把作者这种事我也干过不少，量力而行吧诸君^(*￣(oo)￣)^</p>
<p>愿前进的途中，光照在你我的身上，早日实现生命的<strong>大和谐</strong>~~</p>
]]></content>
      
        <categories>
            
            <category> 写作计划 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Book Summary </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Spark最佳实践]]></title>
      <url>/2018/02/28/Book-Spark%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/</url>
      <content type="html"><![CDATA[<h1 id="简介"><a href="#简介" class="headerlink" title=" 简介 "></a><font color="#5CACEE"> 简介 </font></h1><p>本书是Spark的实战指南，全书共8章，前四章介绍Spark部署，工作机制和内核，后四章通过实战项目介绍Spark SQL，Spark Streaming，Spark GraphX和Spark MLlib功能模块。</p>
<p>通过阅读，读者可以接收的信息和获取的能力包括:</p>
<blockquote>
<p>Spark单机及集群部署，一键部署大规模集群的脚本；</p>
<p>对Spark运行过程，数据结构，内核函数进行深入解析；</p>
<p>hdfs解决了数据的分布式存储，Spark作为优秀的计算框架解决了数据处理的效率问题，那么把数据屯起来处理，想要干啥？这就是本书第三个重要的部分，对大量数据进行查询支持的Spark SQL、应对网站实时统计，数据流处理的Spark Streaming、在社交网络关系挖掘中举足轻重的图处理 Spark GraphX、在当前最火的机器学习上提供了Spark MLlib模块，实现了分类聚类回归等多种算法，这些可以处理实际问题的<strong>模块</strong>是Spark 业内热度飙升的重要原因。</p>
</blockquote>
<p>作者是陈欢和林世飞，在腾讯的社交和效果广告部负责大数据处理和分析的相关工作，掌握着海量的服务器资源，利用成规模的Spark集群来捣腾海量数据。</p>
<p>成书时间2016年2月，技术迭代很快又很慢，随着市场发展新的功能需求不断产生，技术的深度和使用技巧快速演进，慢则体现在技术的稳定性，五年前十年前的应用和概念多得是，很多十多年前的技术书籍现在拿来看依然唇齿留香，经典绝伦。Spark作为已经相当成熟的大数据领域工业应用级框架，以后肯定会不断发展，但根基整个推翻是不太可能了，那意味着整个行业所有跑在Spark上面的应用全部废掉，本书作为资深人士<strong>刚写出来</strong>结合行业内<strong>当前发展</strong>的<strong>基础性</strong>技术书籍，预期3-5年不会过时，作为入门书籍那是相当靠谱。</p>
<a id="more"></a>
<h1 id="前言"><a href="#前言" class="headerlink" title=" 前言 "></a><font color="#5CACEE"> 前言 </font></h1><p>互联网快速发展，最近几年全球数据量以每年50%的速度递增，大数据在事实上改变着我们的思维和生产方式。2015年9月国务院印发《促进大数据发展行动纲要》，大数据与各行各业的结合是大势所趋，大数据相关的企业创业和从业者的需求眼看着就是一波大发展。</p>
<p>03年开始谷歌陆续发表三大论文，在Apache软件基金会和雅虎等互联网公司的支持下，实现了大量了开源服务框架，包括hadoop，hive等重量级产品，hadoop拉开了IT行业使用大数据技术的序幕，后续发展如火如荼，数据量持续增长技术不断演化，10年美国加州伯克利分校陆续提出了多份RDD（Resilient Distributed Dataset，弹性分布式数据集）相关论文，随之推出了开源的Spark框架，对比传统的Hadoop，拥有深厚学术背景的Spark将以往的MapReduce，流式计算，机器学习等全部统合，提供了一站解决方案，让数据挖掘和机器学习的门槛大大降低，加速了大数据技术在各个行业的普及。</p>
<p>大数据热门之后出现了非常多的大数据技术，有Hadoop、Hive、HBase、Flume、Kafka、Lucene、Spark、Storm等，还有很多NoSQL技术可以归入大数据，比如MongoDB、CouchDB、Cassandra等。有人列举过大数据技术族谱，其中至少上百种技术。面对如此多的技术，我们往往会产生疑惑：到底该如何选择？这里有一个建议：在充分理解自身需求的基础上，挑选<strong>最合适</strong>的。</p>
<h1 id="第一章-Spark和大数据"><a href="#第一章-Spark和大数据" class="headerlink" title=" 第一章 Spark和大数据 "></a><font color="#5CACEE"> 第一章 Spark和大数据 </font></h1><h2 id="概述"><a href="#概述" class="headerlink" title=" 概述 "></a><font color="green"> 概述 </font></h2><blockquote class="blockquote-center"><p><strong>数据只是工具，最终还是要用于创造价值，大数据只是一种新的实践。</strong></p>
</blockquote>
<p>介绍大数据的发展状况，以及Spark的起源、特点、优势、未来</p>
<h2 id="大数据是什么"><a href="#大数据是什么" class="headerlink" title="  大数据是什么 "></a><font color="green">  大数据是什么 </font></h2><blockquote class="blockquote-center"><p>大数据大势所趋 </p>
</blockquote>
<p>“大数据”(big data)这个词最早出现在UNIX用户协会的会议上，来自SGI公司的科学家在其文章”大数据与下一代基础架构”中用它描述数据的快速增长，具有”4V”，大量（volume），多样（variety），快速（velocity），价值（value）的特征，从数据量上来看，通常认为当一个计算单元容纳不下要处理的数据，那就是大数据的场景。</p>
<p>电脑发明之前做人口统计，全国耕地面积普查，气象预报等；使用普通pc，服务器的时候碰到了互联网级别数据统计，如搜索引擎需要把网上的多数网页抓取来建立索引，这样我们搜索的时候才能毫秒级返回结果。假设每个网页平均大小20KB，大概有价值的中文网页有200亿，大概总共有400TB大小，那么一台计算机以30MB~40MB的速度从硬盘读写，大概需要4个月，还不包括做复杂的分析工作，这时候就需要计算机集群和大数据技术来存储处理。</p>
<p>大数据的两大问题：<strong>存储</strong>和<strong>计算</strong>。</p>
<p>X表示一台服务器是否正常工作，X=0表示正常工作，X=1表示不正常工作。服务器一个月内发生故障的概率是ε，P(X= 0)= 1 -ε，那么1000台电脑都正常工作的概率是(1-ε)^1000。假设故障率ε=0.001，所有1000台服务器1个月内正常工作的概率只有0.37，不到一半，所以随着数据的增长，机器会不断增加，数据存储和运行过程中都会产生宕机丢数据网络波动等意外状况。hadoop的hdfs从设计上解决了数据的分布式存储，集群的扩展性和容错性都非常完善。</p>
<p>计算框架包括很多优秀的平台，如hadoop的mapreduce做批处理任务，storm做流式处理，hive进行分布式的SQL查询，还有很多如mahout，mpi等，这些可以分别满足不同场景的优秀框架解决了海量数据的处理问题，使得从中进行数据挖掘，实现机器学习成为可能。</p>
<p>大数据发展的技术要素都已经存在，2015年9月5日国务院印发了《促进大数据发展行动纲要》，纲要提出要建设公共数据资源开放的统一开放平台，“2020年底前，逐步实现信用、交通、医疗、卫生、就业、社保、地理、文化、教育、科技、资源、农业、环境、安监、金融、质量、统计、气象、海洋、企业登记监管等民生保障服务相关领域的政府数据集向社会开放”，以及“到2020年，培育10家国际领先的大数据核心龙头企业，500家大数据应用、服务和产品制造企业”，大数据的浪潮已经不可阻挡！</p>
<p>这里引出一个概念，”云计算”，云计算是一种按使用量付费的模式，这种模式提供可用的、便捷的、按需的网络访问， 进入可配置的计算资源共享池（资源包括网络，服务器，存储，应用软件，服务）,有不同层级的服务模式，IaaS(Infrastructure as Service)：基础设施即服务，PaaS(Platform as Service)：平台即服务，SaaS(Software as Service)：软件即服务。其中IaaS支持按天付费，而且可以动态按需扩容。在云计算服务商的帮助下，如今一个小创业公司都能够快速开发和部署大数据应用。云计算服务商的国外领导者是AmazonAWS，国内主要是阿里云、腾讯云和UCloud。</p>
<h2 id="大数据的重要解决方案hadoop"><a href="#大数据的重要解决方案hadoop" class="headerlink" title="  大数据的重要解决方案hadoop "></a><font color="green">  大数据的重要解决方案hadoop </font></h2><blockquote class="blockquote-center"><p>Hadoop开天辟地 </p>
</blockquote>
<p>谷歌的<a href="http://lovepanda.tk/2018/01/03/Google%E4%B8%89%E5%A4%A7%E8%AE%BA%E6%96%87/" target="_blank" rel="external"><font color="#AAAAAA">三大论文</font></a>引发了人们在大数据领域的大量研究，直接导致了Hadoop的出现——MapReduce范式的开源实现。如今Hadoop，包括MapReduce与分布式文件系统（HDFS），已经成为数据处理的事实标准。大量的工业界应用，例如腾讯、百度、阿里巴巴、华为、迪斯尼、沃尔玛、AT&amp;T都已经有自己的Hadoop集群。</p>
<p><img src="http://p09u6sy9g.bkt.clouddn.com/1520846059390.png" alt=""></p>
<p>如图，Hadoop框架的核心是HDFS和MapReduce，其中HDFS是分布式文件系统，MapReduce是分布式数据处理模型和执行环境。</p>
<p>各个组件的功能互相配合使得hadoop能够完成很多场景的大数据任务，本文主角是Spark，这里不再赘述，有兴趣了解的可以<a href="http://blog.csdn.net/u010270403/article/details/51493191" target="_blank" rel="external"><font color="#AAAAAA">移步</font></a>。</p>
<p>但MapReduce不是唯一的大数据分析范式，有一些场景是不适合使用MapReduce的，比如处理网状的数据结构时，这要求能够处理顶点和边的增加和减少操作，并在所有节点进行运算，比如进行机器学习算法需要高速多次迭代的时候因为硬盘读写瓶颈会显得效率稍低，比如不擅长实时计算，需要使用storm，s4，akka等系统。</p>
<h2 id="为什么要用Spark"><a href="#为什么要用Spark" class="headerlink" title="  为什么要用Spark "></a><font color="green">  为什么要用Spark </font></h2><blockquote class="blockquote-center"><p>Spark应运而生 </p>
</blockquote>
<p>Spark替代hadoop不太准确，如上面生态系统图，确切来说spark替代的是MapReduce，凭借自身的内存计算，RDD数据结构在批处理方面比MapReduce表现得更加出色，同时Spark本身拥有Spark SQL，streaming，MLlib，GraphX这些模块，使得诸多功能用Spark一站搞定，不用像之前那样根据需要分别使用MapReduce，hive，storm等，这让Spark在使用门槛，维护成本和性能表现上具有明显优势。</p>
<p>机器学习算法通常需要对同一个数据集合进行多次迭代计算，而MapReduce中每次迭代都会涉及HDFS的读写，以及缺乏一个常驻的MapReduce作业，因此每次迭代需要初始化新的MapReduce任务，效率不高，基于MR的HIVE，PIG等技术也有类似问题。</p>
<p>Spark作为一个研究项目，诞生于加州大学伯克利分校AMP实验室（Algorithms, Machines, and People Lab），为了迭代算法和交互式查询两种典型的场景，Matei Zaharia和合作伙伴开发了Spark系统的最初版本。2009年Spark论文发布，Spark项目正式诞生，在某些任务表现上，Spark相对于Hadoop MapReduce有10～20倍的性能提升。2010年3月Spark开源，且在开源社区下发展迅速。2014年5月，Spark 1.0正式发布，如今已经是Apache基金会的顶级项目了。</p>
<p>如下图在官方博客的<a href="https://databricks.com/blog/2014/11/05/spark-officially-sets-a-new-record-in-large-scale-sorting.html" target="_blank" rel="external"><font color="#AAAAAA">排序大赛</font></a>可以有很清晰的对比，可以看看英文，还顺便创造了个世界纪录2333。最新的结果可以戳<a href="http://sortbenchmark.org/" target="_blank" rel="external"><font color="#AAAAAA">这里</font></a>。</p>
<p><img src="http://p09u6sy9g.bkt.clouddn.com/1520847824598.png" alt=""></p>
<p>Spark一直寻求保持Spark引擎小而紧凑。Spark 0.3版本只有3900行代码，其中1300行为Scala解释器，600行为示例代码，300行为测试代码。即使在今天，Spark核心代码也只有约50 000行，因此更容易为许多开发人员所理解和供我们改变和提高。</p>
<h2 id="Spark未来发展"><a href="#Spark未来发展" class="headerlink" title="  Spark未来发展 "></a><font color="green">  Spark未来发展 </font></h2><p>Spark在过去的5年里发展迅速，社区活跃程度一点儿不亚于Hadoop社区。</p>
<p><a href="https://databricks.com/sparkaisummit/north-america" target="_blank" rel="external"><font color="#AAAAAA">Spark峰会</font></a>是社区分享Spark应用的重要交流会，在Spark Summit 2015上，来自Databricks、UC Berkeley AMPLab、百度、阿里巴巴、雅虎、英特尔、亚马逊、Red Hat、微软等的数十个机构共分享了近100个精彩的报告。</p>
<p>目前Databricks、英特尔、雅虎、加州大学伯克利分校是Spark主要的贡献者。</p>
<p>已经大规模应用在生产环境，2015年Spark最大的集群来自腾讯，一共有8000个节点，单个Job最大分别是阿里巴巴和Databricks的1 PB，国内的BAT都在用Spark，可以从<a href="http://spark.apache.org/powered-by.html" target="_blank" rel="external"><font color="#AAAAAA">这里</font></a>查看完整的用户列表，Spark作为一种优秀的大数据处理技术已经得到广泛认可，在如此多的应用场景中承担着任务，逐渐会积累庞大的用户经验和文档。</p>
<p>道家看事物分为”道，法，术，器”四个层次，拿到大数据领域来看，大概可以这么理解，Spark是一种工具-器，掌握用Spark去解决实际问题的方法-术，能够根据场景和需求熟练选择合适的技术，不滞于某种技术，这就是掌握了大数据领域解决问题的基本规则-法，而道，可能就是那些在十几年前就已经从计算机技术和社会发展的层次看到了数据膨胀和物理发展的冲突，那个时候就开始提出理论试图拿出解决方案的人，是那些将理论转为现实，设计出分布式存储和各种算法的人，是站在大数据发展的最前端，了解技术走向并引领潮流的人所处的层次吧，难望项背，对先行者和开拓者唯有敬意！</p>
<h1 id="第二章-Spark基础"><a href="#第二章-Spark基础" class="headerlink" title=" 第二章 Spark基础 "></a><font color="#5CACEE"> 第二章 Spark基础 </font></h1><blockquote class="blockquote-center"><p>君子藏器于身，待时而动 </p>
</blockquote>
<p>本章介绍Spark部署和编程。作者首先带领读者在本地单机下体验Spark的基本操作，然后部署一个包括ZooKeeper、Hadoop、Spark的可实际应用的高可用集群。并且，这一章还介绍了作者开发的一个自动化部署工具，最后介绍了Spark编程的基础知识，以及如何打包提交至集群上运行。</p>
<h2 id="Spark集群部署"><a href="#Spark集群部署" class="headerlink" title=" Spark集群部署 "></a><font color="green"> Spark集群部署 </font></h2><blockquote>
<p>Spark真实的部署环境绝大部分都在Linux下，运行依赖JDK，大致思路就是在linux系统里配置JDK，将Spark的预编译包解压到指定路径即可使用。</p>
<p>笔者之前已经琢磨过Spark的安装部署，在本博客里另一篇博文<a href="http://lovepanda.tk/2018/02/02/Spark%E5%AE%9E%E8%B7%B5/" target="_blank" rel="external"><font color="#AAAAAA">Spark实践</font></a>有详细介绍，所有搭建步骤都经过两遍以上实际操作，验证无误。本地已经有了现成的环境，如下。</p>
<p><img src="http://p09u6sy9g.bkt.clouddn.com/1520911920198.png" alt=""></p>
<p>这些基本操作没有什么太多的变化，建议按照笔者博文的步骤实际操作，亲手搭建环境后再深入学习原理，手头有环境操作原理并行学习效率会高很多。</p>
</blockquote>
<p>本章里精华部分是作者对生产环境各种集群的配合部署和自动化脚本的详细介绍，看起来复杂，各种几千台服务器的初始化，自动化集群部署，实际上在自己实践并理解了整个流程之后会发现不是很难，下面介绍这部分内容，我猜想，作者写作的时候预定阅读群体是对Spark没有认识和实践经验的，所以过程会比较详细，而笔者是对Spark有了了解并且通读全书之后第二遍阅读并整理思路，所以假定本博文的阅读对象已经按照前文提到的博客内容有了实践经验，在此基础上不会那么详细（但保证不会出错！）。</p>
<h3 id="集群总览"><a href="#集群总览" class="headerlink" title=" 集群总览 "></a><font color="purple"> 集群总览 </font></h3><p><img src="http://p09u6sy9g.bkt.clouddn.com/1520929895755.png" alt=""></p>
<p>整个集群除了核心的Spark之外，还有多个子集群。为了实现集群的高可用性，引入zookeeper集群进行主备切换。Spark没有存储能力，引入HDFS。引入YARN让集群具备非常好的扩展性。</p>
<p>首先是zookeeper集群，集群机器数量为2n+1台，可以抵御n台机器宕机的风险，如果n取1，总共三台，可以抵御1台机器宕机的风险，所有节点都是相同角色，但运行中会自动选择一个作为leader，zookeeper集群担负终极仲裁者的角色，spark集群的master节点通过于zookeeper集群协商完成主备切换，hdfs文件系统的namenode及yarn中的ResourceManager也是通过ZooKeeper集群来协助完成主备切换。</p>
<p><img src="http://p09u6sy9g.bkt.clouddn.com/1520931669151.png" alt=""></p>
<p>其次是Spark集群，该集群有两类节点，master和slave。</p>
<p>master负责集群的资源管理，只要有一台正常工作即可，一般至少两台，一台主节点，当主节点异常下线时，其他master节点通过zookeeper仲裁竞选成为主节点</p>
<p>slave节点用于执行计算任务，机器数量可以是一台以上，几十几百或上千台。</p>
<p>hadoop集群从2.x开始把存储和计算分离开，分成hdfs和yarn两个子集群。</p>
<p><img src="http://p09u6sy9g.bkt.clouddn.com/1520931701760.png" alt=""></p>
<p>hdfs是一个分布式文件系统，为集群提供大文件的存储能力，namenode管理所有的文件信息，一主一备，通过zookeeper实现容灾切换，QJM节点数量为2n+1，一般三台即可，负责记录namenode的流水日志，确保数据不丢失，datanode担负集群的存储负载，用于数据存储。</p>
<p><img src="http://p09u6sy9g.bkt.clouddn.com/1520931715000.png" alt=""></p>
<p>yarn为MapReduce计算提供调度服务，也可为符合yarn编程接口的集群提供调度服务，比如Spark，该集群有两类节点，ResourceManager和NodeManager，RM一主多备，NM通常与datanode部署在一起，实现最高效的数据访问。</p>
<p>集群的设计理念就是利用普通服务器协同工作来实现大数据的计算，因此集群可以部署在任何普通服务器个人计算机甚至个人计算机的多个虚拟机上，但集群各个节点的工作性质不同，对硬件资源的消耗也不同，合理的搭配能更充分地发挥潜力。</p>
<p><img src="http://p09u6sy9g.bkt.clouddn.com/1520931929898.png" alt=""></p>
<p>“移动计算而不要移动数据”，计算离数据约近移动数据的开销越小，大部分场景移动数据的开销都会大于计算的开销，所以计算节点和存储节点一般会选择混合部署。</p>
<p><strong>节点部署方面</strong>建议Spark slave，HDFS datanode，YARN NodeManager三类节点在一起，这些节点在集群中数量最为庞大，最消耗资源，CPU，硬盘，内存资源都可以倾斜。</p>
<p>其他节点的主要任务是管理，除了namenode对内存要求稍高，其他对硬件消耗都不大。可以考虑混合部署，比如笔者用了五台虚拟机，master，namenode，ResourceManager在一台服务器，其他四台服务器作为spark的slave，hdfs的datanode，yarn的nodemanager。机器资源充足的话也可以单独部署，毕竟这些管理节点的数目都不多，将存储，资源管理，计算的主节点按单节点容灾设计，分开部署消耗最多也不过12台服务器，当然会格外麻烦。建议选择稳定性较高的配置，比如双电源，raid，冗余。</p>
<p><img src="http://p09u6sy9g.bkt.clouddn.com/1520941014372.png" alt=""></p>
<p>具体到<strong>硬件配置</strong>作者结合Spark官方和实际项目经验，有如下建议</p>
<blockquote>
<p>存储 : Spark Slave节点尽可能靠近存储系统（比如HDFS和HBase），如果不能部署在相同节点上，那么请尽可能让它们在物理位置上更近一些以减少网络IO的开销，比如尽可能在相同机架上、同一个路由器下、相同机房里。而且每个节点建议使用4块或更多独立挂载的硬盘，并且不要配置RAID，这样可以提升Spark读写本地缓存数据时的效率。</p>
<p>内存 : Spark节点可以适应几百吉字节大小的内存。更多的内存可以缓存更多的数据，在一些迭代计算场景下性能提升效果非常明显。另外，建议分配最多节点内存总量的75%给Spark使用，其他的留给操作系统。</p>
<p>网络 : 一些需要进行shuffle的byKey类操作，比如group-ByKey、reduceByKey、join，它们的瓶颈一般都在网络上，所以建议使用10 Gbit/s以上网络。在Spark程序的Web界面上，可以看到shuffle对于网络使用的状况。</p>
<p>CPU : Spark程序可以适应几十个内核的CPU，而且其计算性能与CPU核数量基本成正比。一般建议8~16个核，可以结合实际负载与成本综合考虑。一般情况下，如果数据已经在内存中了，CPU和网络IO会成为瓶颈。</p>
</blockquote>
<pre><code>单机硬件配置会严重影响集群性能，实际生产环境需要根据负载在成本和性能之间取得平衡。
</code></pre><h3 id="部署思路"><a href="#部署思路" class="headerlink" title=" 部署思路 "></a><font color="purple"> 部署思路 </font></h3><p>假设，有几千台服务器，刚刚装好系统配好ip，需要搭建一个Spark环境。</p>
<p>首先，需要初始化环境，包括创建账号，安装JDK，设置时间同步；</p>
<p>然后，依次部署zookeeper，hadoop，spark集群并启动服务。</p>
<p>上述操作在机器少的时候可以手动完成，但是需要在所有机器上操作，既要改配置文件，传软件包，还要执行一些命令，必须使用远程操作工具，笔者之前用的是rsync，还有ansible等等，工具只是实现目标的手段，殊途同归，高效解决问题就行，作者在这里用的是python的fabric。</p>
<h4 id="创建账号"><a href="#创建账号" class="headerlink" title="创建账号"></a>创建账号</h4><blockquote>
<p>给集群所有机器创建账号<br><img src="http://p09u6sy9g.bkt.clouddn.com/1520937088011.png" alt=""></p>
</blockquote>
<h4 id="安装JDK"><a href="#安装JDK" class="headerlink" title="安装JDK"></a>安装JDK</h4><blockquote>
<p>分发jdk软件包到指定路径并设置环境变量</p>
</blockquote>
<p><img src="http://p09u6sy9g.bkt.clouddn.com/1520937335233.png" alt=""></p>
<p><img src="http://p09u6sy9g.bkt.clouddn.com/1520937353458.png" alt=""></p>
<h4 id="设置时间同步"><a href="#设置时间同步" class="headerlink" title="设置时间同步"></a>设置时间同步</h4><blockquote>
<p>配置ntp服务器后设置集群所有机器时间同步</p>
</blockquote>
<p><img src="http://p09u6sy9g.bkt.clouddn.com/1520937486227.png" alt=""></p>
<h4 id="部署zookeeper集群"><a href="#部署zookeeper集群" class="headerlink" title="部署zookeeper集群"></a>部署zookeeper集群</h4><blockquote>
<p>笔者部署自己集群的时候没有用到zookeeper，zookeeper主要是对master，namenode，ResourceManager节点进行仲裁，主备切换，属于容灾措施，生产环境必须要有，没有也不影响正常运行。笔者自己搭着玩所以当时没弄，有时间可以玩一玩(<em>^_^</em>)</p>
</blockquote>
<p><img src="http://p09u6sy9g.bkt.clouddn.com/1520937718173.png" alt=""></p>
<p><img src="http://p09u6sy9g.bkt.clouddn.com/1520937736023.png" alt=""></p>
<p><img src="http://p09u6sy9g.bkt.clouddn.com/1520937752204.png" alt=""></p>
<h4 id="部署hadoop集群"><a href="#部署hadoop集群" class="headerlink" title="部署hadoop集群"></a>部署hadoop集群</h4><blockquote>
<p>hadoop的部署没什么说的，解压安装包到指定路径，修改配置文件并同步，fabric，ansible，rsync都能搞定</p>
</blockquote>
<p><img src="http://p09u6sy9g.bkt.clouddn.com/1520938006164.png" alt=""></p>
<h4 id="部署spark集群"><a href="#部署spark集群" class="headerlink" title="部署spark集群"></a>部署spark集群</h4><p>Spark集群的部署跟hadoop大同小异，解压，改配置文件，起服务，多了一个配置ssh免密码登陆，用ssh-keygen和ssh-copy命令就可以轻松搞定，在所有机器上执行这些命令可以使用fabric这些远程执行工具</p>
<p><img src="http://p09u6sy9g.bkt.clouddn.com/1520938551673.png" alt=""><br><img src="http://p09u6sy9g.bkt.clouddn.com/1520938581585.png" alt=""></p>
<p><img src="http://p09u6sy9g.bkt.clouddn.com/1520938626259.png" alt=""></p>
<p><img src="http://p09u6sy9g.bkt.clouddn.com/1520938667020.png" alt=""></p>
<h3 id="一键部署高可用hadoop-Spark集群"><a href="#一键部署高可用hadoop-Spark集群" class="headerlink" title=" 一键部署高可用hadoop+Spark集群 "></a><font color="purple"> 一键部署高可用hadoop+Spark集群 </font></h3><blockquote>
<p>相信认真看了的朋友发现了，这个一键部署就是将所有的软件分发，批量修改配置文件，顺序执行各种命令都写在了<code>install.sh</code>这个脚本里面，用到的材料就是准备做集群的那些服务器，具体的内容分解开来就是上述所有步骤，只是用脚本实现批量执行，大大提高效率并且减少人工失误，完美，这就是成熟的运维人员该干的事，然而很遗憾，作者给的链接失效了，笔者没有拿到这个脚本┑(￣Д ￣)┍</p>
</blockquote>
<p><img src="http://p09u6sy9g.bkt.clouddn.com/1520938858370.png" alt=""></p>
<p><img src="http://p09u6sy9g.bkt.clouddn.com/1520938875708.png" alt=""></p>
<p><img src="http://p09u6sy9g.bkt.clouddn.com/1520938884410.png" alt=""></p>
<h2 id="编程指南"><a href="#编程指南" class="headerlink" title=" 编程指南 "></a><font color="green"> 编程指南 </font></h2><h3 id="集群工作方式"><a href="#集群工作方式" class="headerlink" title=" 集群工作方式"></a><font color="purple"> 集群工作方式</font></h3><blockquote>
<p>Spark用scala语言开发，scala语言的函数式编程特性让代码的可读性非常高，做spark开发和看spark源码都需要懂scala，Spark也支持Python，java和R，本节用scala语言为示例讲解运行在spark上的程序的编写。</p>
</blockquote>
<p><img src="http://p09u6sy9g.bkt.clouddn.com/1520939651613.png" alt=""></p>
<blockquote>
<p>交互式编程，以wordcount举例，执行<code>spark-shell</code>这个命令进入spark命令行，在屏幕滚动日志中可以看到<code>Spark context available as sc</code>，解释器已经初始化了一个名为<code>sc</code>的<code>Spark context</code>对象，可以直接使用。</p>
</blockquote>
<p><img src="http://p09u6sy9g.bkt.clouddn.com/1520911920198.png" alt=""></p>
<blockquote>
<p>spark-shell只适合调试，正式环境下Spark程序在调试好之后，需要编译，链接，打成jar包，最后提交给集群运行</p>
</blockquote>
<p><img src="http://p09u6sy9g.bkt.clouddn.com/1520939761088.png" alt=""></p>
<p><img src="http://p09u6sy9g.bkt.clouddn.com/1520939786440.png" alt=""></p>
<p><img src="http://p09u6sy9g.bkt.clouddn.com/1520940075655.png" alt=""></p>
<p><img src="http://p09u6sy9g.bkt.clouddn.com/1520940206450.png" alt=""></p>
<p><img src="http://p09u6sy9g.bkt.clouddn.com/1520940215345.png" alt=""></p>
<h3 id="编程核心思想"><a href="#编程核心思想" class="headerlink" title=" 编程核心思想 "></a><font color="purple"> 编程核心思想 </font></h3><blockquote class="blockquote-center"><p>心之所向，一往无前 </p>
</blockquote>
<p>Spark工作的目的是调用经过清洗转换存储好的数据，用各种方法进行处理，实现查询，数据挖掘，流处理，机器学习等项目需求。</p>
<p>Spark怎么做这件事的？</p>
<p>第一步，将数据拿进来生成RDD。Spark可以从很多数据源拿到数据，最终都是生成RDD，只是类型不同；</p>
<p>第二步，Spark调用各种函数语法对RDD进行操作计算，transcation定义RDD上的操作，action触发操作，最终结果也是一个RDD，可以将结果按需要保存到指定位置或者进行下一次迭代处理。</p>
<p>RDD是Spark最核心的东西，hadoop的hdfs是为生成RDD的数据提供分布式存储，没有hdfs还能从其他渠道得到，hdfs是一种比较好的解决方案而已，yarn是资源调度，起到Spark触发计算的时候各个节点分配调用计算资源的作用，zookeeper是可以说是冗余容错机制，为了保证spark集群/hdfs存储/计算调度主节点的高可用，他们都是为”Spark生成RDD并调用函数进行处理最终得到预期结果”这一目标而服务的。</p>
<p>那么问题来了，RDD是什么，都可以对RDD进行什么操作？理解了这个问题，Spark可以说已经初窥门径，<strong>RDD的本质</strong>在后面Spark内核章节进行深入解析，循序渐进，这里先教大家怎么用，怎么创建RDD，怎么对它进行操作。</p>
<h4 id="RDD创建"><a href="#RDD创建" class="headerlink" title="RDD创建"></a>RDD创建</h4><p>RDD生成方式总结起来两大类：由Driver程序的数据集生成，由外部数据集生成，如共享文件系统，HDFS文件和HBase等</p>
<p>1.从Driver程序的数据集生成</p>
<blockquote>
<p>从Driver数据集生成RDD的直接方法是使用SparkContext对象的parallelize方法，其参数是一个Seq对象，或者其他可以被自动转换成Seq的对象也可以作为参数，比如Array，List，scala支持方便的隐式转换。</p>
<p>SparkContext对象还提供了range方法，通过指定长整数的开始值，结束值及步长，生成一定范围的长整型，最终调用parallelize生成RDD</p>
<p>这种方式的数据源收到Driver所在节点的资源限制，不适合处理特别大的数据，多适用于交互式环境下的程序调试，小数据量的数据集测试。</p>
</blockquote>
<p><img src="http://p09u6sy9g.bkt.clouddn.com/1520994908729.png" alt=""></p>
<p>2.从外部数据集生成RDD</p>
<p>Spark可以从任何Hadoop支持的存储类型的数据源生成RDD，包括本地文件系统，HDFS，HBase，Amazom S3等，还支持文本文件，Sequence文件，以及其他任何Hadoop InputFormat格式的数据。这使得RDD从生成到计算全过程都是分布式的，不会形成资源瓶颈。</p>
<p>文本文件可以使用SparkContext对象的textFile方法生成，其参数是文件的地址，可以是本地文件系统的完整路径，也可以是以hdfs://、s3n://等开头的URL。</p>
<p><img src="http://p09u6sy9g.bkt.clouddn.com/1520911920198.png" alt=""></p>
<p>除了最基本的文本文件外，Spark还支持其他类型的文件。   </p>
<p>•wholeTextFiles可以一次读取一个目录下的许多小文件，并返回&lt;文件名、内容&gt;二元组，而不是像textFile那样每条记录是一行文本，不保留文件信息。  </p>
<p>•对于SequenceFile格式的文件（Hadoop针对大数据计算使用的一种文件格式），可以使用SparkContext对象的se-quenceFile[K, V]方法，其中K和V分别代表文件中key和value的类型，这些类型必须继承自Hadoop的Writable接口，比如IntWritable和Text。不过Spark提供了便利，可以直接使用Scala基本类型，比如Int和String，它们已经实现了Writable接口，所以我们可以直接这样使用：se-quenceFile[Int, String]。</p>
<p>对于<strong>HBase文件</strong>，因为HBase使用HDFS作为存储，也是一种Hadoop InputFormat，所以也是用hadoopRDD或newAPIHadoopRDD来读取数据的</p>
<p><img src="http://p09u6sy9g.bkt.clouddn.com/1520997293217.png" alt=""></p>
<p>此外，Spark还提供了RDD对象的序列化和反序列化功能，通过RDD.savaAsObjectFile保存的RDD序列化对象，可以使用SparkContext.objectFile重新加载进来，非常简单实用。</p>
<p>SparkContext对象还提供了binaryFiles方法，以二进制的形式直接读取Hadoop Mapreduce计算的结果文件。</p>
<p>所有RDD创建方法，都在SparkContext对象中提供，完整的信息可以参考官方API文档，值得注意的一点，从外部创建RDD并不会马上读进内存进行计算，只是保存读取它们的信息，计算时通过DAG就近分配计算资源，避免移动数据，基本不用担心单点资源瓶颈。</p>
<p>以上就是RDD创建的几种方法，这里只是让读者有个概念，知道有这些个方法即可，真正用到的时候根据实际场景采用合适的方法，命令什么的都是细节。</p>
<h4 id="RDD操作"><a href="#RDD操作" class="headerlink" title="RDD操作"></a>RDD操作</h4><p>生成了RDD之后对其进行调用各种操作经过计算后得到预期的结果。这些操作分为两类:</p>
<pre><code>Transformation(转换)
</code></pre><p><img src="http://p09u6sy9g.bkt.clouddn.com/1520998392297.png" alt=""></p>
<pre><code>Action(动作)
</code></pre><p><img src="http://p09u6sy9g.bkt.clouddn.com/1520998410039.png" alt=""></p>
<p>Spark下所有的Transformation都是一种lazy模式，计算不会马上进行，而是先记录下计算方式，Action被触发时才会启动真正的计算向Driver程序返回结果。这种模式的好处是高效，不需要将每次Transformation的非常大的结果返回Driver，只需要记录下要对RDD进行的操作，当Action触发后将计算完成的最终小很多的结果集返回给Driver，避免了大量的网络IO，数据流动，反观mapreduce，每次迭代都需要将hdfs里的数据取出来进行计算，然后写回去，下次迭代再取，再存，即使现在hadoop也支持memory-cache，尽量避免数据频繁读取，加快计算速度，但是本质上的区别使spark和mapreduce还是有着很明显的效率差异。</p>
<p>默认情况下除了最终结果集之外的RDD都是临时的，被Transformation或Action使用过后即丢弃，可以对RDD进行持久化或cache操作，这也会触发Transformation进行真正的计算，而且Spark会将RDD的结果保持在集群的内存或磁盘中，甚至复制多份，这样再次访问时不需要重复计算，就可以获取更快的响应速度。</p>
<pre><code>举个栗子┑(￣Д ￣)┍
</code></pre><p><img src="http://p09u6sy9g.bkt.clouddn.com/1520999264260.png" alt=""></p>
<pre><code>上传测试文件到hdfs
</code></pre><p><img src="http://p09u6sy9g.bkt.clouddn.com/1520999696297.png" alt=""></p>
<pre><code>第一行从hdfs目录下的文件生成一个名为ff的RDD

第二行生成一个对ff执行flatMap操作后名为words的RDD，flatMap这里的作用是将文档里所有的单词用空格分隔，生成一个连续的字符串。**没有执行实际文件操作**

第三行生成一个对words执行map操作后名为wordPairs的RDD，map在这里的作用是遍历文档里所有单词，如从 &quot;a&quot; 变成 &quot;a,1&quot;。**没有执行实际文件操作**

第四行生成一个对wordPairs执行reduceByKey操作后名为wordCounts的RDD，reduceByKey在这里的作用是将文档里 &quot;key,value&quot;相同&quot;key&quot;后面的value都累加起来，这里就已经实现了对hdfs中README.md文档进行词频统计的目标，该RDD就是期待最后返回的结果RDD，但是这里**仍然没有执行实际文件操作**

前面都是transcation操作，涵盖读取数据源生成RDD，中间调用各种函数对RDD进行转换，每次转换都生成一个新的RDD，最终用collect触发Action操作，这时候才会触发实际的文件操作，读数据到内存进行计算，最后返回一个结果集。
</code></pre><p><img src="http://p09u6sy9g.bkt.clouddn.com/1520999917431.png" alt=""></p>
<pre><code>上面只是为了方便理解，实际没人那么写，一条命令直接搞定，完成词频统计就是这么简单干脆。
</code></pre><p>笔者同时还在学习一部小象学院的Spark视频教程，里面有详细讲解scala，具体形式是在spark环境中用scala做各种操作来演示scala的各种功能，因为Spark就是用scala开发的，编写程序也是用scala，那个scala的实际操作差不多就是spark日常操作的教程，到时候整理出来会在<strong>这里</strong>添加链接，敬请期待，不过那就是另一篇博客了O(∩_∩)O</p>
<p><strong>2018/3/13 19:23:07 </strong></p>
<h1 id="第三章-Spark工作机制"><a href="#第三章-Spark工作机制" class="headerlink" title=" 第三章 Spark工作机制 "></a><font color="#5CACEE"> 第三章 Spark工作机制 </font></h1><blockquote class="blockquote-center"><p>有生不生，有化不化。不生者能生生，不化者能化化。 </p>
</blockquote>
<p>本章主要内容是Spark底层的工作机制，包括调度管理、内存管理、容错机制、监控管理以及Spark程序配置管理，这对理解Spark程序的运行非常有帮助。</p>
<h2 id="调度管理"><a href="#调度管理" class="headerlink" title=" 调度管理 "></a><font color="green"> 调度管理 </font></h2><p>普通程序运行在普通电脑上，比如QQ跑在笔记本上，由win10操作系统来进行统一调度分配可以使用的CPU，内存，硬盘资源，Spark集群运行在服务器的linux系统上，这时的资源主要是指CPU core数量和物理内存，Spark支持自身资源调度，master节点自身可以进行资源调度，同时支持与其他调度管理系统一起工作，yarn和mesos，这俩资源调度有俩好处，一是外部调度系统更强大，支持更灵活的调度策略，二是当Spark集群和其他分布式系统一起部署时，可以被统一统筹，避免Spark集群整体对资源的吞噬。</p>
<p>主要分为两类调度场景：主要是Spark程序之间的调度，另一个时Spark程序内部的调度。</p>
<h3 id="集群名词解释"><a href="#集群名词解释" class="headerlink" title=" 集群名词解释 "></a><font color="purple"> 集群名词解释 </font></h3><p>1.Driver程序</p>
<p>集群模式下，用户编写的Spark程序称为Driver程序，每个Driver程序包含一个代表集群环境的SparkContext对象并与之连接，程序的执行从Driver程序开始，中间过程会调用RDD操作（transcation和action），这些操作通过集群资源管理器来调度执行，一般在worker节点执行，所有操作执行结束后返回到Driver程序中，在Driver程序中结束。</p>
<p>2.SparkContext对象</p>
<p>每个驱动程序中都有一个SparkContext对象，担负着与集群沟通的职责。</p>
<ul>
<li><p>sc对象联系集群管理器，分配CPU，内存等资源；</p>
</li>
<li><p>集群管理器在工作节点(worker node)上启动一个执行器；</p>
</li>
<li><p>程序代码被分发到相应的工作节点；</p>
</li>
<li><p>sc分发任务(task)至各执行器执行。</p>
</li>
</ul>
<p>3.集群管理器</p>
<p>集群管理器负责集群的资源调度，Spark支持三种资源管理器</p>
<ul>
<li>standalone模式，资源管理器是master节点。这是最简单的一种集群模式，不依赖于其他系统，调度策略相对单一，只支持先进先出模式(FIFO，First-In-First-Out)。</li>
<li>spark部署在hadoop上，资源管理器是yarn集群。yarn支持动态资源管理，更适合多用户场景下的集群管理，而且yarn可以同时调度Spark计算和MR计算，还可以调度其他实现了yarn调度接口的集群计算，是目前最主流的一种资源管理系统</li>
<li>Apache Mesos，资源管理器是Mesos，是一个专门用于分布式系统资源管理的开源系统，与yarn类似，用c++开发，可以对集群中的资源作弹性管理。</li>
</ul>
<p>还有一些需要了解的名词</p>
<p>Allocation。即“分配”。   </p>
<p>•App、Application、Spark程序。泛指用户编写的运行在Spark上的程序，不仅是Scala语言编写的，其他支持的语言编写的也是。   </p>
<p>•节点、Worker节点。集群上的计算节点，一般对应一台物理机器，仅在测试时会出现一台物理机器上启动多个节点。   </p>
<p>•Worker。每个节点上会启动一个进程，名为Worker，负责管理本节点，运行jps命令可以看到Worker进程在运行   </p>
<p>•core。Spark标识CPU资源的方式，对应一个或多个物理CPU核心，每个Task运行时至少需要一个core。   </p>
<p>•执行器。每个Spark程序在每个节点上启动的一个进程，专属于一个Spark程序，与Spark程序有相同的生命周期，负责Spark在节点上启动的Task，管理内存和磁盘。如果一个节点上有多个Spark程序在运行，那么相应地就会启动多个执行器。  </p>
<p>•Job。一次RDD Action对应一次Job，会提交至资源管理器调度执行。  </p>
<p>•Stage。Job在执行过程中被分为多个阶段。介于Job与Task之间，是按Shuffle分隔的Task集合。  </p>
<p>•Task。在执行器上执行的最小单元。比如RDD Transfor-mation操作时对RDD内每个分区的计算都会对应一个Task。</p>
<h3 id="Spark程序之间的调度"><a href="#Spark程序之间的调度" class="headerlink" title=" Spark程序之间的调度 "></a><font color="purple"> Spark程序之间的调度 </font></h3><p>两种分配策略，静态分配和动态分配。</p>
<p>静态分配指Spark程序启动时一次性分配所有资源，运行过程中固定不变直至推出。</p>
<blockquote>
<p>所有集群管理器都支持静态资源分配，每个Spark程序都分配一个最大可用的资源数量，在程序运行的整个过程都持有它。这种策略简单可靠，<strong>强烈建议</strong>，除非非常确定这种分配方式无法满足需求。</p>
</blockquote>
<p>动态分配指运行过程中不断调整分配的资源，可以按需增加或减少，比静态分配复杂很多，需要在实践中不断调试才能达到最优。</p>
<blockquote>
<p>Spark 1.2引入了基于负载情况的集群动态资源分配，分配给Spark程序的资源可以增加或减少。这意味着我们的程序可能会在不使用资源的时候将资源还给集群，需要的时候再从集群申请。这个特性在多个程序共享集群资源的时候特别有用。如果分配给Spark程序的资源中有一部分是空闲的，它就可以返还给集群放入资源池，这样可以被其他程序请求使用。在Spark中，动态资源分配的粒度是执行器，即增加或减少执行器。前面已经介绍过，Spark程序在机器的每个节点上只有一个执行器，所以增加或减少执行器意味着为Spark程序服务的节点数据量的增加或减少。通常，每台机器启动一个节点进程，但也不排除启动多个的情况发生。你可以通过设置spark.dynamicAllocation.en-abled来启动这个功能。<br>目前只有在yarn模式下可以使用，未来的版本也支持standalone和mesos模式。</p>
</blockquote>
<h3 id="Spark程序内部的调度"><a href="#Spark程序内部的调度" class="headerlink" title=" Spark程序内部的调度 "></a><font color="purple"> Spark程序内部的调度 </font></h3><p>Spark程序内部，不同线程提交的Job可以并行执行，调度器是线程安全的，因此可以支持这种需要同时处理多个请求的服务型应用。</p>
<p>默认情况下，调度器以FIFO的方式运行Job，每个Job分成多个stage(如map和reduce阶段)，如果最前面Job的Stage有task要运行，优先获取所有资源，然后才是第二个Job，依此类推。如果队列中第一个Job不需要太多资源，那么第二个Job可以马上运行，但如果第一个Job特别大，则后面的Job会明显延迟。</p>
<p>从Spark0.8开始，可以配置Job间公平共享资源，在公平共享方式下，采用”循环”(round robin)方式为不同Job之间的task分配资源，这样所有的Job可以获得差不多相同的资源，意味着有长时间的Job运行的情况下，短的Job也可以在提交后马上运行，不用等待长Job结束，特别适合多用户场景。</p>
<p>开启程序内公平调度，只需要在sc中设置spark.scheduler.mode值为FAIR。</p>
<pre><code>val conf = new SparkConf().setMaster(...).setAppName(...)

conf.set(&quot;spark.scheduler.mode&quot;, &quot;FAIR&quot;)

val sc = new SparkContext(conf)
</code></pre><p>公平调度还支持对多个Job进行分组，这个分组称为调度池，每个调度池可以设置不同的调度选项，比如想要为一些更重要的Job设置更高的优先级，比如为不同的用户设置不同的资源池，让各个资源池平等共享资源，而不是按Job来共享资源，不做设置的话，新提供的Job会自动进入默认调度池。我们可以指定让Job进入哪个调度池，具体方法是提交任务的线程在SparkContext中设置spark.scheduler.pool，这样该线程提交的所有Job都会使用这个调度池。设置按照线程来进行调度，可以很方便地让一个线程地所有Job在一个用户下。</p>
<pre><code>sc.setLocalProperty(&quot;spark.scheduler.pool&quot;, &quot;pool1&quot;)

sc.setLocalProperty(&quot;spark.scheduler.pool&quot;, null)
</code></pre><p>调度池的配置可以放在配置文件<code>conf/fairscheduler.xml.template</code>中，在sc中指定配置文件，每个调度池有如下三个属性。</p>
<p>schedulingMode，可以FIFO或FAIR，用于控制调度池内的Job是排队执行还是平均共享资源</p>
<p>weight，用于控制调度池相对于其他调度池的权重，是一个相对值。所有调度池的默认权重都是1，平均共享集群资源，如果某个调度池的是2，那么理论上它可以调用权重为1的资源池两倍的资源。设置一个非常高的权重，比如1k，相当于配置了各个调度池的优先级。实际上，权重为1k的调度池只要有一个Job就会优先启动。</p>
<p>minShare，设置最小资源值，默认值为0，公平调度器在按权限分配之前，会满足各个资源池的最小资源值，这样可以保证调度池总会获得一些资源，不会被其他高权重的调度池抢光。</p>
<p>。配置文件中没有出现的调度池都被设置为默认值（schedulingMode为FIFO，weight为1，minShare为0）。</p>
<pre><code>&lt;allocations&gt;
  &lt;pool name=&quot;production&quot;&gt;
    &lt;schedulingMode&gt;FAIR&lt;/schedulingMode&gt;
    &lt;weight&gt;1&lt;/weight&gt;
    &lt;minShare&gt;2&lt;/minShare&gt;
  &lt;/pool&gt;
  &lt;pool name=&quot;test&quot;&gt;
    &lt;schedulingMode&gt;FIFO&lt;/schedulingMode&gt;
    &lt;weight&gt;2&lt;/weight&gt;
    &lt;minShare&gt;3&lt;/minShare&gt;
  &lt;/pool&gt;
&lt;/allocations&gt;    
</code></pre><h2 id="内存管理"><a href="#内存管理" class="headerlink" title=" 内存管理 "></a><font color="green"> 内存管理 </font></h2><p>相比MR，spark具有巨大的性能优势，很大一部分原因是spark对内存的充分利用和缓存机制。</p>
<pre><code>RDD持久化
</code></pre><blockquote>
<p>如果一个RDD不止一次被用到，那就可以持久化它，将RDD缓存到内存中，内存不够时用磁盘顶上去，这样可以大幅提升程序性能，十倍甚至更多。</p>
<p>默认情况下，RDD只使用一次，用完即扔，再次使用需要重新计算得到，而持久化操作避免了重复计算，这就是Spark刚出现时被人称作内存计算的原因。</p>
<p>持久化的方法是调用persist()函数，RDD.unpersist()可以删除持久化。默认只持久化到内存中，可以在调用持久化函数时添加参数设置存储级别，有memory_and_disk，memory_only_ser，memore_and_disk_ser,disk_only，其中memory_only_ser类似默认的memory_only，格式是序列化后的数据，节省内存更消耗CPU</p>
</blockquote>
<h2 id="容错机制"><a href="#容错机制" class="headerlink" title=" 容错机制 "></a><font color="green"> 容错机制 </font></h2><p>分布式系统通常运行在一个机器集群，同时运行的几百台机器中有机器发生故障的概率大大增加，容错设计是分布式系统的重要能力。</p>
<p>Spark以前的集群容错机制，如MR，将计算转换为一个有向无环图(DAG)的任务集合，通过重复执行DAG里的一部分任务来完成容错。由于主要的数据存储在分布式文件系统中，没有提供其他存储的概念，容错过程需要在网络上进行数据复制，增加了大量的消耗，所以分布式编程经常需要做检查点，将某个时刻的中间数据写到存储中。</p>
<p>RDD处理过程也是一个DAG，每个RDD都会记住创建该数据集需要的操作，自己是由哪个RDD转化而来，这个继承关系叫lineage(血统)，由于创建RDD的操作是相对粗粒度的变换(如map，filter，join)，很多单一的操作应用于许多数据元素，不需要存储真正的数据，当RDD的某个分区丢失时，RDD有足够的信息记录其如何通过其他RDD进行计算，且只需要计算该分区。</p>
<p>lineage不是完美的，因为RDD之间的依赖有两种，父分区对应多个子分区的宽依赖和父分区对应一个子分区的窄依赖。</p>
<p>对于窄依赖，只需要重新计算丢失的那一块数据，如map，filter，union，分区相同的join，容错成本较小，但宽依赖容错重算分区时，就会有大量冗余计算。</p>
<p>所以不同的应用有时需要使用<code>doCheckPoint</code>适当设置数据检查点，RDD的只读特性使得它很容易做检查点。</p>
<p>某些场景中容错要求更高更复杂，比如计费服务要求零丢失，流计算应用场景系统上游不断产生数据，容错过程会造成数据丢失。为了解决这些问题，spark又提供了预写日志，先将数据写入支持容错的文件系统，然后对数据进行操作。</p>
<p>这样，容错机制包括从检查点重新计算恢复，从日志恢复，从数据源重发，实现了零丢失。</p>
<p>Master使用zookeeper容错，在<code>spark-env.sh</code>中添加选项，设置master的恢复模式为zookeeper，设置zookeeper的集群地址和用于恢复的zookeeper目录。或者采用一种更简单的方式，设置恢复模式为filesystem并设置一个恢复目录，该目录会存储必要的恢复信息，当master进程异常时，重启master进程即可。</p>
<p>slave节点运行着worker，执行器，Driver程序。</p>
<blockquote>
<p>Worker异常停止时，会先将自己启动的执行器停止，Driver需要有相应的程序来重启Worker进程。</p>
<p>执行器异常退出时，Driver没有在规定时间内收到执行器的StatusUpdate，于是Driver会将注册的执行器移除，Worker收到LaunchExecutor指令，再次启动执行器。</p>
<p>Driver异常退出时，一般要使用检查点重启Driver，重新构造上下文并重启接收器。第一步，恢复检查点记录的元数据块。第二步，未完成作业的重新形成。由于失败而没有处理完成的RDD，将使用恢复的元数据重新生成RDD，然后运行后续的Job重新计算后恢复。</p>
</blockquote>
<h2 id="监控管理"><a href="#监控管理" class="headerlink" title=" 监控管理 "></a><font color="green"> 监控管理 </font></h2><p>可以用多种方式来监控Spark程序的运行状况:web界面，metrics，外部系统。</p>
<p>web:每个Driver的SparkContext都会启动一个Web界面，默认端口是4040，用于显示程序的许多有用信息，包括：调度器Stage、Task列表，RDD大小和内存使用统计概况，环境信息，正在运行的执行器信息。浏览器输入<a href="http://xxx.xxx.xxx.xxx:4040即可访问，" target="_blank" rel="external">http://xxx.xxx.xxx.xxx:4040即可访问，</a></p>
<p>自定义:开发人员可以通过JSON接口自己开发可视化展示形式，历史服务器和正在运行的spark程序的web界面都支持rest api，所有入口地址都有严格的版本控制，spark保证:<br>    这些地址永远不会在某个版本中删除，任何地址中提供的每个字段都不会被删除；<br>    可能添加新的入口地址；<br>    现有入口地址可能添加新的字段；<br>    每个入口地址在未来都可能添加新版本的API（比如api/V2），新版本不要求向下兼容老版本；<br>    API版本可能被废弃，但前提是新的版本可以与之共存，而且新版本不只是发布主版本，至少有一个次版本发布。</p>
<p>metrics指标体系:spark支持基于<code>Coda Hale Metrics Library</code>的指标体系，可以主动将运行状态发送给其他系统，方便与其他监控系统进行集成，比如Ganglia。Spark支持的Metrics实例有Master、Applications、Worker、执行器和Driver，支持的接收者类型包含在org.apache.spark.metrics.sink包中，包括ConsoleSink、CSVSink、JmxSink、MetricsServlet、GraphiteSink和Slf4jSink。</p>
<p>其他监控工具:集群级别可以使用各种工具来监控各节点的CPU，网络，磁盘等负载情况，如zabbix，nagios，ganglia运维工具，操作系统级别可以使用dstat，iostat，iotop，top等linux工具对单点问题进行定位。</p>
<h2 id="Spark程序配置管理"><a href="#Spark程序配置管理" class="headerlink" title=" Spark程序配置管理 "></a><font color="green"> Spark程序配置管理 </font></h2><p>spark配置文件默认模式是<code>${SPARK_HOME}/conf</code>，可更改的配置项自行按需操作。</p>
<p>spark属性项的配置可以在三个地方进行设置，优先级依次是 sc对象，命令行参数，spark-defaults.conf。</p>
<p>可以在<code>spark-defults.conf文件</code>里面可以设置spark运行的诸多参数</p>
<p>可以在用命令行<code>./bin/spark-submit</code>提交程序时重新指定参数，</p>
<pre><code>./bin/spark-submit --name &quot;My app&quot; --master local[4] --conf spark.shuffle.spill=false --conf &quot;spark.executor.extraJavaOptions=-XX:+PrintGCDetails -XX:+PrintGCTimeStamps&quot; myApp.jar
</code></pre><p>在开发spark程序时可以把参数写在代码里，sc对象的优先级最高，不过灵活性差了点，改参数就得改代码。</p>
<pre><code>val conf = new SparkConf()    
    .setMaster(&quot;local[2]&quot;)    
    .setAppName(&quot;CountingSheep&quot;)    
    .set(&quot;spark.executor.memory&quot;, &quot;1g&quot;)

val sc = new SparkContext(conf)    
</code></pre><p>在web界面的environment选项里可以看到所有配置的spark属性</p>
<p>日志配置是另一项单独的配置，使用配置文件目录下的<code>log4j.properties</code>作为配置文件。</p>
<p><strong>2018/3/14 20:19:48 </strong></p>
<h1 id="第四章-Spark内核讲解"><a href="#第四章-Spark内核讲解" class="headerlink" title=" 第四章 Spark内核讲解 "></a><font color="#5CACEE"> 第四章 Spark内核讲解 </font></h1><p>深入Spark内核，并结合源码，介绍了核心结构RDD、RDD对象的Transformation和Action操作是如何实现的、SparkContext对象及初始化过程、DAG调度的工作流程。了解这些内容可以帮助读者编写出高质量的Spark程序代码。</p>
<h1 id="第五章-Spark-SQL和数据仓库"><a href="#第五章-Spark-SQL和数据仓库" class="headerlink" title=" 第五章 Spark SQL和数据仓库 "></a><font color="#5CACEE"> 第五章 Spark SQL和数据仓库 </font></h1><p>介绍Spark SQL，可以代替Hive，用于搭建一个企业级的数据仓库。案例基于淘宝的电商数据建立电商数据仓库，并以日常运营工作为例，通过电商数据库分析电商运营中的各类问题。</p>
<h1 id="第六章-Spark-流式计算"><a href="#第六章-Spark-流式计算" class="headerlink" title=" 第六章 Spark 流式计算 "></a><font color="#5CACEE"> 第六章 Spark 流式计算 </font></h1><p>章介绍Spark实时流式计算，类似于Storm，但吞吐量方面更有优势。案例是基于一个站点的Web日志建立一个类似百度统计的实时统计系统，是各种实时系统典型的参考例子。</p>
<h1 id="第七章-Spark-图计算"><a href="#第七章-Spark-图计算" class="headerlink" title=" 第七章 Spark 图计算 "></a><font color="#5CACEE"> 第七章 Spark 图计算 </font></h1><p>介绍Spark的图计算。案例基于新浪微博2000万的关系链数据，讲解了如果利用图计算来实现社交关系链的挖掘，比如闺蜜的发现、粉丝群体的发现等。</p>
<h1 id="第八章-Spark-MLlib"><a href="#第八章-Spark-MLlib" class="headerlink" title=" 第八章 Spark MLlib "></a><font color="#5CACEE"> 第八章 Spark MLlib </font></h1><p>介绍Spark的机器学习库。案例基于某个搜索引擎的点击日志，建立了一个搜索广告点击率预估系统。广告点击率预估是各家互联网系统的核心系统，公开的实战项目不多。</p>
<h2 id="二级目录"><a href="#二级目录" class="headerlink" title=" 二级目录 "></a><font color="green"> 二级目录 </font></h2><h3 id="三级目录"><a href="#三级目录" class="headerlink" title=" 三级目录 "></a><font color="purple"> 三级目录 </font></h3>]]></content>
      
        <categories>
            
            <category> 技术 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Book Summary-1 </tag>
            
            <tag> Spark </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Spark深入]]></title>
      <url>/2018/02/04/Spark%E6%B7%B1%E5%85%A5/</url>
      <content type="html"><![CDATA[<h1 id="简介"><a href="#简介" class="headerlink" title=" 简介 "></a><font color="#5CACEE"> 简介 </font></h1><p>Spark是一个解决大规模数据集运算分析的计算引擎，知其然更要知其所以然，深入了解编程模型，RDD原理，解析spark内核，能够让我们在一个更高的角度看程序运行，数据流动，而且现在对批处理有了其他的需求，比如sturcture stream实现的流处理，MLib实现的机器学习的分类聚类算法，对这种更高层次的spark应用的学习有助于更好的发挥spark的潜力，安装配置不值一提，具体应用才是画龙点睛。</p>
<p><a href="https://www.bilibili.com/video/av13765160/index_7.html?t=495" target="_blank" rel="external"><font color="#AAAAAA"> spark 教程 </font></a>讲的很透彻，已经刷了两遍，跟着做了一些实际操作，每一遍都有新的理解，学得越多越觉得自己懂得太少，奋斗吧少年！</p>
<a id="more"></a>
<p>深坑待填，关键是理论性东西写起来真的墨迹啊</p>
<h1 id="scala基础与实践"><a href="#scala基础与实践" class="headerlink" title=" scala基础与实践 "></a><font color="#5CACEE"> scala基础与实践 </font></h1><blockquote>
<p>scala是一门多范式的编程语言，设计初衷是要集成面向对象编程和函数式编程的各种特性。<br>scala用java开发，运行在java虚拟机上并兼容java程序，spark就是用scala开发的，用scala写spark的程序极为合适，其次是python和java。</p>
<p>不去学具体的使用技巧，不了解程序的内部原理，那么spark就只是一个软件，深入研究并自己写程序去测试spark的性能，它才会成为数据操控者手中的利剑，剑锋所指，一切真相无所遁形！额(⊙﹏⊙)，说人话就是:”用熟了之后做数据挖掘能手脚麻利一点”，<code>hiahiahia~~~</code></p>
</blockquote>
<h2 id="二级目录"><a href="#二级目录" class="headerlink" title=" 二级目录 "></a><font color="green"> 二级目录 </font></h2><h3 id="三级目录"><a href="#三级目录" class="headerlink" title=" 三级目录 "></a><font color="purple"> 三级目录 </font></h3><h1 id="spark编程模型"><a href="#spark编程模型" class="headerlink" title=" spark编程模型 "></a><font color="#5CACEE"> spark编程模型 </font></h1><h1 id="深入spark内核"><a href="#深入spark内核" class="headerlink" title=" 深入spark内核 "></a><font color="#5CACEE"> 深入spark内核 </font></h1><h1 id="spark-streaming"><a href="#spark-streaming" class="headerlink" title=" spark streaming "></a><font color="#5CACEE"> spark streaming </font></h1><h1 id="spark上运行机器学习，算法实现"><a href="#spark上运行机器学习，算法实现" class="headerlink" title=" spark上运行机器学习，算法实现 "></a><font color="#5CACEE"> spark上运行机器学习，算法实现 </font></h1><h1 id="shark多语言编程"><a href="#shark多语言编程" class="headerlink" title=" shark多语言编程 "></a><font color="#5CACEE"> shark多语言编程 </font></h1>]]></content>
      
        <categories>
            
            <category> Bigdata </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Spark </tag>
            
            <tag> Framework </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Spark实践]]></title>
      <url>/2018/02/02/Spark%E5%AE%9E%E8%B7%B5/</url>
      <content type="html"><![CDATA[<h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a><font color="#5CACEE">简介</font></h1><p>Apache Spark 是专为大规模数据处理而设计的快速通用的计算引擎,一个用来实现快速而通用的集群计算的平台,扩展了广泛使用的MapReduce计算模型，能高效支持更多的计算模式，包括交互式查询和流处理。在处理大规模数据集的时候有优势，Spark的一个重要特点就是能够在内存中计算，因而比MapReduce更快，即使在磁盘上进行的复杂计算，Spark依然更加高效。</p>
<a id="more"></a>
<table>
<thead>
<tr>
<th>软件</th>
<th>下载地址</th>
</tr>
</thead>
<tbody>
<tr>
<td>jdk-8u161-linux-x64.tar.gz</td>
<td><a href="https://pan.baidu.com/s/1eTSOOQQ" target="_blank" rel="external"><font color="#AAAAAA">点击下载</font></a></td>
</tr>
<tr>
<td>hadoop-2.7.2.tar.gz</td>
<td><a href="https://pan.baidu.com/s/1dGssSkLX" target="_blank" rel="external"><font color="#AAAAAA">点击下载</font></a></td>
</tr>
<tr>
<td>scala-2.11.7.tgz</td>
<td><a href="https://pan.baidu.com/s/1dGf8lS" target="_blank" rel="external"><font color="#AAAAAA">点击下载</font></a></td>
</tr>
<tr>
<td>spark-2.2.0-bin-hadoop2.7.tgz</td>
<td><a href="https://pan.baidu.com/s/1eTDpAqm" target="_blank" rel="external"><font color="#AAAAAA">点击下载</font></a></td>
</tr>
<tr>
<td>hadoop-native-64-2.7.0.tar</td>
<td><a href="https://pan.baidu.com/s/1daOeqM" target="_blank" rel="external"><font color="#AAAAAA">点击下载</font></a></td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th>服务器</th>
<th>节点</th>
</tr>
</thead>
<tbody>
<tr>
<td>192.168.4.50</td>
<td>SparkMaster</td>
</tr>
<tr>
<td>192.168.4.237</td>
<td>SparkWorker1</td>
</tr>
<tr>
<td>192.168.4.238</td>
<td>SparkWorker2</td>
</tr>
<tr>
<td>192.168.4.48</td>
<td>SparkWorker3</td>
</tr>
<tr>
<td>192.168.4.49</td>
<td>SparkWorker4</td>
</tr>
</tbody>
</table>
<h1 id="Hadoop"><a href="#Hadoop" class="headerlink" title=" Hadoop "></a><font color="#5CACEE"> Hadoop </font></h1><h2 id="运行环境配置"><a href="#运行环境配置" class="headerlink" title=" 运行环境配置 "></a><font color="green"> 运行环境配置 </font></h2><p>请务必关闭<strong>防火墙</strong>，包括<code>iptables</code>及<code>selinux</code></p>
<h3 id="java"><a href="#java" class="headerlink" title=" java "></a><font color="red"> java </font></h3><p>三台虚拟机均需配置java环境，下文所示环境是包括hadoop，scala，spark软件在内的环境变量最终形态，如果新玩家要入手，直接配置成这样纸，后面的软件注意存放在指定路径即可。</p>
<pre><code>$ tar xvf jdk-8u161-linux-x64.tar.gz  -C /usr/local/ 
$ vi /root/.bash_profile
$ source /root/.bash_profile
</code></pre><blockquote>
<pre><code>export PATH
export JAVA_HOME=/usr/local/jdk1.8.0_161
export JRE_HOME=$JAVA_HOME/jre
export CLASSPATH=$JAVA_HOME/lib:$JRE_HOME/lib
export SCALA_HOME=/usr/local/scala-2.11.7
export SPARK_HOME=/usr/local/spark-2.2.0-bin-hadoop2.7
export PATH=$JAVA_HOME/bin:/usr/local/hadoop-2.7.2/bin:/usr/local/hadoop-2.7.2/sbin:$SCALA_HOME/bin:$SPARK_HOME/bin:$PATH
export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native  
export HADOOP_OPTS=&quot;-Djava.library.path=$HADOOP_HOME/lib&quot;
</code></pre></blockquote>
<p><img src="http://p09u6sy9g.bkt.clouddn.com/1517884683021.png" alt=""></p>
<h3 id="主机名称"><a href="#主机名称" class="headerlink" title=" 主机名称 "></a><font color="red"> 主机名称 </font></h3><p>各节点均要更改，图为主节点示例，需要修改<code>/etc/hosts</code>，<code>/etc/sysconfig/network</code>两个文件，注意<code>hosts</code>文件各节点一致，<code>network</code>文件各节点需分别修改成自己的<code>hostname</code></p>
<p><img src="http://p09u6sy9g.bkt.clouddn.com/1517904904084.png" alt=""></p>
<h3 id="ssh"><a href="#ssh" class="headerlink" title=" ssh "></a><font color="red"> ssh </font></h3><p>在各虚拟机上执行命令生成密钥</p>
<pre><code># ssh-keygen 
</code></pre><p>ssh将各个机器的密钥拿来主节点（以下语句皆在SparkMaster上执行）合成authorized_keys，然后分发到各个节点</p>
<pre><code>$ ssh root@192.168.4.237 cat /root/.ssh/id_rsa.pub &gt;&gt; /root/.ssh/authorized_keys

237替换成238，50，48，49依次执行，完成后会在主节点生成/root/.ssh/authorized_keys,是一份包含了五台机器公钥信息的文件

$ scp authorized_keys root@192.168.4.237:/root/.ssh/

237替换成238，50，48，49依次执行，将包含了五台服务器的公钥文件分发到所有服务器，所有节点之间都可以无密码ssh操作
</code></pre><p><img src="http://p09u6sy9g.bkt.clouddn.com/1517905440009.png" alt=""></p>
<blockquote>
<p>配置互信的原因:namenode(即主节点)如果没有对datanode(即数据节点)的ssh免登陆权限，那么namenode起每个datanode的服务都需要输入密码，多节点的情况极为影响效率，所以namenode与datanode间必须配置互信，而datanode之间没有ssh通信的需求，所以上文是一种绝对可行但相对麻烦和不必要的操作，如果多节点的大规模hadoop配置互信建议使用下面这种ssh自带的方法，可以使用类似ansible这样的工具批量执行。</p>
</blockquote>
<pre><code>$ ssh-keygen 

生成密钥

$ ssh-copy-id -i /root/.ssh/id_rsa.pub root@192.168.4.50 

把datanode生成的公钥发送到namenode，namedode拥有所有datanode的公钥即可，datanode不必持有其他datanode或者namenode的公钥
</code></pre><h2 id="软件包解压"><a href="#软件包解压" class="headerlink" title=" 软件包解压 "></a><font color="green"> 软件包解压 </font></h2><p>在namenode解压并修改配置文件，用<code>rsync</code>把修改后的<code>/usr/local/hadoop-2.7.2</code>文件夹同步到其他节点</p>
<pre><code>$ tar xvf hadoop-2.7.2.tar.gz -C /usr/local/

$ mkdir -p {tmp,hdfs/{data,name}}
</code></pre><blockquote>
<p>tmp用来存储临时生成的文件</p>
<p>hdfs用来存储集群数据</p>
<p>hdfs/data用来存储真正的数据</p>
<p>hdfs/name用来存储文件系统元数据</p>
</blockquote>
<p><img src="http://p09u6sy9g.bkt.clouddn.com/1517907019997.png" alt=""></p>
<h2 id="修改配置文件"><a href="#修改配置文件" class="headerlink" title=" 修改配置文件 "></a><font color="green"> 修改配置文件 </font></h2><p>下列文件绝对路径:<code>/usr/local/hadoop-2.7.2/etc/hadoop/</code></p>
<h3 id="hadoop-env-sh"><a href="#hadoop-env-sh" class="headerlink" title=" hadoop-env.sh "></a><font color="red"> hadoop-env.sh </font></h3><pre><code>将 export JAVA_HOME=${JAVA_HOME}

改成 export JAVA_HOME=/usr/local/jdk1.8.0_161
</code></pre><h3 id="core-site-xml"><a href="#core-site-xml" class="headerlink" title=" core-site.xml "></a><font color="red"> core-site.xml </font></h3><pre><code>&lt;configuration&gt;
    &lt;property&gt;
        &lt;name&gt;fs.defaultFS&lt;/name&gt;
        &lt;value&gt;hdfs://SparkMaster:9000&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;
        &lt;value&gt;file:/usr/local/hadoop-2.7.2/tmp&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;io.file.buffer.size&lt;/name&gt;
        &lt;value&gt;131072&lt;/value&gt;
    &lt;/property&gt;
&lt;/configuration&gt;
</code></pre><h3 id="mapred-site-xml"><a href="#mapred-site-xml" class="headerlink" title=" mapred-site.xml "></a><font color="red"> mapred-site.xml </font></h3><pre><code>$ cp mapred-site.xml.template mapred-site.xml

&lt;configuration&gt;
    &lt;property&gt;
        &lt;name&gt;mapreduce.framework.name&lt;/name&gt;
        &lt;value&gt;yarn&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;
        &lt;value&gt;SparkMaster:10020&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;
        &lt;value&gt;SparkMaster:19888&lt;/value&gt;
    &lt;/property&gt;
&lt;/configuration&gt;
</code></pre><h3 id="hdfs-site-xml"><a href="#hdfs-site-xml" class="headerlink" title=" hdfs-site.xml "></a><font color="red"> hdfs-site.xml </font></h3><pre><code>&lt;configuration&gt;
    &lt;property&gt;
        &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;
        &lt;value&gt;file:/usr/local/hadoop-2.7.2/hdfs/name&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;
        &lt;value&gt;file:/usr/local/hadoop-2.7.2/hdfs/data&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;dfs.replication&lt;/name&gt;
        &lt;value&gt;2&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt;
        &lt;value&gt;SparkMaster:9001&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
    &lt;name&gt;dfs.webhdfs.enabled&lt;/name&gt;
    &lt;value&gt;true&lt;/value&gt;
    &lt;/property&gt;
&lt;/configuration&gt;
</code></pre><h3 id="yarn-site-xml"><a href="#yarn-site-xml" class="headerlink" title=" yarn-site.xml "></a><font color="red"> yarn-site.xml </font></h3><pre><code>&lt;configuration&gt;
        &lt;property&gt;
               &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;
               &lt;value&gt;mapreduce_shuffle&lt;/value&gt;
        &lt;/property&gt;
        &lt;property&gt;
               &lt;name&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;/name&gt;
               &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt;
        &lt;/property&gt;
        &lt;property&gt;
               &lt;name&gt;yarn.resourcemanager.address&lt;/name&gt;
               &lt;value&gt;SparkMaster:8032&lt;/value&gt;
       &lt;/property&gt;
       &lt;property&gt;
               &lt;name&gt;yarn.resourcemanager.scheduler.address&lt;/name&gt;
               &lt;value&gt;SparkMaster:8030&lt;/value&gt;
       &lt;/property&gt;
       &lt;property&gt;
               &lt;name&gt;yarn.resourcemanager.resource-tracker.address&lt;/name&gt;
               &lt;value&gt;SparkMaster:8031&lt;/value&gt;
       &lt;/property&gt;
       &lt;property&gt;
               &lt;name&gt;yarn.resourcemanager.admin.address&lt;/name&gt;
               &lt;value&gt;SparkMaster:8033&lt;/value&gt;
       &lt;/property&gt;
       &lt;property&gt;
               &lt;name&gt;yarn.resourcemanager.webapp.address&lt;/name&gt;
               &lt;value&gt;SparkMaster:8088&lt;/value&gt;
       &lt;/property&gt;
&lt;/configuration&gt;
</code></pre><h3 id="slaves"><a href="#slaves" class="headerlink" title=" slaves "></a><font color="red"> slaves </font></h3><pre><code>SparkWorker1
SparkWorker2
SparkWorker3
SparkWorker4
</code></pre><p><img src="http://p09u6sy9g.bkt.clouddn.com/1517907253222.png" alt=""></p>
<p>所有节点的文件都需要是经过修改后的状态，利用rsync将namenode修改过的文件夹直接同步到其他节点</p>
<pre><code>$ rsync -av /usr/local/hadoop-2.7.2 SparkWorker1:/usr/local/hadoop-2.7.2

SparkWorker1替换成slaves里其他节点依次执行，所有节点都有了修改过配置文件的hadoop文件夹后集群准备工作完成。
</code></pre><h2 id="启动并验证服务"><a href="#启动并验证服务" class="headerlink" title=" 启动并验证服务 "></a><font color="green"> 启动并验证服务 </font></h2><pre><code>hadoop namenode -format  格式化

start-all.sh 或者 start-dfs.sh &amp;&amp; start-yarn.sh   
每个节点都有完整的配置文件和命令，任何节点都可以执行集群的起停操作，都可以成功启动hdfs和yarn   

jps  查看各个节点进程状态
</code></pre><p><img src="http://p09u6sy9g.bkt.clouddn.com/1517907563706.png" alt=""></p>
<p><img src="http://p09u6sy9g.bkt.clouddn.com/1517907627610.png" alt=""></p>
<p><img src="http://p09u6sy9g.bkt.clouddn.com/1517907799956.png" alt=""></p>
<p>hadoop集群已经正常<strong>启动</strong>。</p>
<h2 id="简单测试"><a href="#简单测试" class="headerlink" title=" 简单测试 "></a><font color="green"> 简单测试 </font></h2><pre><code>$ hadoop fs -put /tmp/input.txt /

上传待处理文件到hdfs中

$ hadoop jar /usr/local/hadoop-2.7.2/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar wordcount /input.txt /output.txt

调用安装包自带jar包对hdfs中存储的待处理文件执行词频统计，这里就是诸位大数据开发工程师大显身手的地方，写好的程序打成jar包放在这里供hadoop调用，examples是hadoop自带练手用的测试包。

$ hadoop fs -ls /output.txt

可以看到已经生成了一个包含着若干文件的output.txt文件夹，存放着调用jar包函数后输出的处理结果，给文件夹起output.txt这种名字，皮一下就很开心

$ hadoop fs -text /output.txt/part-r-00000
</code></pre><p><img src="http://p09u6sy9g.bkt.clouddn.com/1517909164567.png" alt=""></p>
<p><img src="http://p09u6sy9g.bkt.clouddn.com/1517909184848.png" alt=""></p>
<p><img src="http://p09u6sy9g.bkt.clouddn.com/1517909479584.png" alt=""></p>
<h1 id="Scala"><a href="#Scala" class="headerlink" title=" Scala "></a><font color="#5CACEE"> Scala </font></h1><h2 id="安装包"><a href="#安装包" class="headerlink" title=" 安装包 "></a><font color="green"> 安装包 </font></h2><pre><code>$ tar xvf /tmp/scala-2.11.7.tgz -C /usr/local

上传解压安装包到指定目录
</code></pre><h2 id="环境变量"><a href="#环境变量" class="headerlink" title=" 环境变量 "></a><font color="green"> 环境变量 </font></h2><pre><code>$ vi /root/.bash_profile

添加 /usr/local/scala-2.11.7/bin 到PATH路径中
</code></pre><h2 id="同步并验证"><a href="#同步并验证" class="headerlink" title=" 同步并验证 "></a><font color="green"> 同步并验证 </font></h2><pre><code>scala -version
</code></pre><p>一个词，<code>rsync</code>，有点灵性啊同学们!</p>
<h1 id="Spark"><a href="#Spark" class="headerlink" title=" Spark "></a><font color="#5CACEE"> Spark </font></h1><h2 id="安装包-1"><a href="#安装包-1" class="headerlink" title=" 安装包 "></a><font color="green"> 安装包 </font></h2><pre><code>$ tar xvf /tmp/spark-2.2.0-bin-hadoop2.7.tgz -C /usr/local
</code></pre><h2 id="环境变量及配置文件"><a href="#环境变量及配置文件" class="headerlink" title=" 环境变量及配置文件 "></a><font color="green"> 环境变量及配置文件 </font></h2><pre><code>$ vi /root/.bash_profile

添加 /usr/local/spark-2.2.0-bin-hadoop2.7/bin到环境变量
</code></pre><h3 id="spark-env-sh"><a href="#spark-env-sh" class="headerlink" title=" spark-env.sh "></a><font color="red"> spark-env.sh </font></h3><pre><code>export JAVA_HOME=/usr/local/jdk1.8.0_161

export SCALA_HOME=/usr/scala-2.11.7

export HADOOP_HOME=/usr/local/hadoop-2.7.2

export HADOOP_CONF_DIR=/usr/local/hadoop-2.7.2/etc/hadoop

export SPARK_MASTER_IP=SparkMaster

export SPARK_WORKER_MEMORY=4g

export SPARK_WORKER_CORES=2

export SPARK_WORKER_INSTANCES=1
</code></pre><h3 id="slaves-1"><a href="#slaves-1" class="headerlink" title=" slaves "></a><font color="red"> slaves </font></h3><p><img src="http://p09u6sy9g.bkt.clouddn.com/1517974200450.png" alt=""></p>
<h2 id="同步并验证-1"><a href="#同步并验证-1" class="headerlink" title=" 同步并验证 "></a><font color="green"> 同步并验证 </font></h2><pre><code>将修改过配置文件的文件夹全部同步到数据节点
</code></pre><blockquote>
<p>$ start-dfs.sh</p>
<p>spark只使用hdfs文件系统，并不用启动所有功能</p>
<p>$ /usr/local/spark-2.2.0-bin-hadoop2.7/sbin/start-all.sh</p>
<p>启动spark集群，建议使用绝对路径</p>
<p>成功启动后使用jps可以在namenode看到</p>
</blockquote>
<pre><code>30049 Jps
29655 ResourceManager
29501 SecondaryNameNode
29311 NameNode
</code></pre><blockquote>
<p>可以在datanode看到</p>
</blockquote>
<pre><code>4208 Jps
3937 DataNode
4042 NodeManager
</code></pre><p>至此<strong>spark集群</strong>全部搭建完成<br><img src="http://p09u6sy9g.bkt.clouddn.com/1517912187159.png" alt=""></p>
<blockquote>
<p>可以通过spark的webui界面访问控制台</p>
<p>可以通过spark-shell执行各种操作，执行transcation  action任务，对存放在hdfs里的数据集进行分词，统计，排序</p>
<p>可以执行用python，scala，java等语言打包好的jar包程序</p>
</blockquote>
<pre><code>ENJOY IT!
</code></pre><h1 id="遇到的坑"><a href="#遇到的坑" class="headerlink" title=" 遇到的坑 "></a><font color="#5CACEE"> 遇到的坑 </font></h1><p>1.请务必关闭防火墙，包括iptables和selinux，否则会导致集群无法通信，页面无法访问等报错，如<code>java.net.NoRouteToHostException:没有找到主机的路由</code></p>
<p>2.启动后有警告<code>Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</code>，因为Apache提供的hadoop本地库是32位的，而在64位的服务器上就会有问题，因此需要自己编译64位的版本，解决方案是找个包，解压到目录并修改环境变量，操作后问题解决，原作者环境跟我的环境不一样，不确定是不是所有人都会碰到这个问题</p>
<pre><code>$ tar -xvf hadoop-native-64-2.7.0.tar -C /usr/local/hadoop-2.7.2/lib/native 

$ tar -xvf hadoop-native-64-2.7.0.tar -C /usr/local/hadoop-2.7.2/lib
</code></pre><p>3.第一次启动后进程都正常，页面无法访问，查看监听状态发现监听到了ipv6的端口，hadoop的配置文件明明标注的是监听到ipv4的端口，解决方案如下。</p>
<pre><code>vi /etc/sysctl.conf
添加以下内容并执行stop-all.sh和start-all.sh重启hadoop服务
#disable ipv6
net.ipv6.conf.all.disable_ipv6 = 1
net.ipv6.conf.default.disable_ipv6 = 1
net.ipv6.conf.lo.disable_ipv6 = 1
</code></pre><p>4.数据节点jps没有datanode进程的情况</p>
<p>多次format文件系统可能会出现datanode无法启动的情况，<strong>解决方案是删除数据节点的hdfs及tmp目录，重新启动即可。</strong></p>
<p><img src="http://p09u6sy9g.bkt.clouddn.com/1517971666429.png" alt=""></p>
<blockquote>
<p>原因是执行<code>hadoop namenode -format</code>后会在<code>namenode</code>的<code>/usr/lcoal/hadoop-2.7.2/hdfs/name/current/</code>文件夹下生成记录了<code>namespaceID</code>，<code>clusterID</code>及<code>blockpoolID</code>的<code>VERSION</code>文件，然后执行<code>start-dfs.sh</code>命令启动hdfs，这时datanode的<code>/usr/local/hadoop-2.7.2/hdfs/data/</code>文件夹下会生成记录了集群信息的文件。</p>
</blockquote>
<p><img src="http://p09u6sy9g.bkt.clouddn.com/1517970828891.png" alt=""></p>
<p><img src="http://p09u6sy9g.bkt.clouddn.com/1517971096804.png" alt=""></p>
<blockquote>
<p>hadoop启动关闭hdfs时需要读取namenode及datanode的集群信息，二次format的时候namenode的相关信息会自动更新，但datanode的原有信息不会自动删除，启动的时候namenode优先读取该节点原有信息，这样本来准备起datanode，一看有记录，要写入的信息不写了，按原有记录来，结果原有记录的信息是format之前集群的信息，hadoop以为那是其他集群的数据节点，自然不会带起来datanode进程。</p>
</blockquote>
]]></content>
      
        <categories>
            
            <category> Bigdata </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Spark </tag>
            
            <tag> Framework </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[NoSQL简介]]></title>
      <url>/2018/01/23/NoSQL/</url>
      <content type="html"><![CDATA[<h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a><font color="#5CACEE">简介</font></h1><blockquote>
<p>本文主要参考<a href="http://www.runoob.com/mongodb/nosql.html" target="_blank" rel="external"><font color="#AAAAAA">菜鸟教程</font></a>，原博文清晰明了，可以自行前往，写在这里是以手连心加深印象，同时把这些信息抓回来作为以博客为载体的个人技能图的重要组成部分。</p>
<p>NoSQL(NoSQL = Not Only SQL )，意即”不仅仅是SQL”。</p>
<p>现代计算系统上每天网络上都会产生庞大的数据量，这些数据很大一部分是由关系数据库RDBMS处理，实践证明关系模型非常适合于客户端服务器编程，是结构化数据存储在网络和商务应用的主导技术。</p>
<p>NoSQL是另一种数据组织形式，早期提出至09年趋势高涨，NoSQL相关的非关系型数据库在存储处理当前互联网快速产生，日益庞大，形式多样的数据方面有巨大优势，发展迅速。</p>
</blockquote>
<a id="more"></a>
<h1 id="NoSQL简史"><a href="#NoSQL简史" class="headerlink" title=" NoSQL简史 "></a><font color="#5CACEE"> NoSQL简史 </font></h1><p>NoSQL一词最早出现在1998年，是Carlo Strozzi开发的一个轻量，开源，不提供SQL功能的关系数据库；</p>
<p>2009年，有一次关于分布式开源数据库的讨论，有人再次提出了NoSQL的概念，主要指非关系型，分布式，不提供ACID的数据库设计模式；</p>
<p>2009年，亚特兰大举行的”no:sql(east)”讨论会是一个里程碑，其口号”select fun, profit from real_world where relational = false”，因此对NoSQL最普遍的解释是”非关系型的”，强调key-values stores和文档数据库的优点，而不是单纯的反对RDBMS。</p>
<p>时至今日，各种NoSQL数据库已经是大数据领域各种数据分布式存储的基石。</p>
<h1 id="NoSQL数据库分类"><a href="#NoSQL数据库分类" class="headerlink" title=" NoSQL数据库分类 "></a><font color="#5CACEE"> NoSQL数据库分类 </font></h1><table>
<thead>
<tr>
<th>类型</th>
<th>代表数据库</th>
<th>特点</th>
</tr>
</thead>
<tbody>
<tr>
<td>列存储</td>
<td>Hbase/Cassandra/Hypertable</td>
<td>按列存储，方便存储结构化和半结构化数据，方便做数据压缩，在某一列或者某几列的查询中有巨大的IO优势</td>
</tr>
<tr>
<td>文档存储</td>
<td>MongoDB/CouchDB</td>
<td>用类似json的格式存储文档，可以对某些字段建立索引，实现关系数据库的某些功能</td>
</tr>
<tr>
<td>key-value存储</td>
<td>Redis/MemcacheDB</td>
<td>存储的是key-value对，不管value的格式，可以通过key快速查询value</td>
</tr>
<tr>
<td>图存储</td>
<td>Neo4J/FlockDB</td>
<td>图形关系的最佳存储方案</td>
</tr>
<tr>
<td>对象存储</td>
<td>db4o/Versant</td>
<td>通过对象的方式存储数据，用类似面向对象语言的语法操作数据库</td>
</tr>
<tr>
<td>xml数据库</td>
<td>BaseX/Berkeley XML DB</td>
<td>高效存储XML数据，支持XML内部查询语法，比如XQuery，Xpath</td>
</tr>
</tbody>
</table>
<h1 id="RDBMS和NoSQL的规则"><a href="#RDBMS和NoSQL的规则" class="headerlink" title=" RDBMS和NoSQL的规则 "></a><font color="#5CACEE"> RDBMS和NoSQL的规则 </font></h1><h2 id="ACID"><a href="#ACID" class="headerlink" title=" ACID "></a><font color="green"> ACID </font></h2><p>关系型数据库有一个很重要的概念:事务(transaction)，类似现实中的交易，遵循ACID:</p>
<h3 id="A-Atomicity-原子性"><a href="#A-Atomicity-原子性" class="headerlink" title=" A-Atomicity 原子性 "></a><font color="red"> A-Atomicity 原子性 </font></h3><p>事务要么不做，要么做完，事务成功的条件是事务里所有操作均成功完成，只要有一个操作失败，整个事务便失败回滚。<br>比如银行转账，A账户转B账号，分为两个步骤:<br>1.从A账号取出100元<br>2.往B账号转入100元<br>银行的数据库处理该事务，要么两步一起完成，事务提交，任何一步出问题事务都会回滚，如果第一步未完成那么事务失败，第二步未完成，B没收到钱但A少了100元，事务会回滚到未转账状态。</p>
<h3 id="C-Consistency-一致性"><a href="#C-Consistency-一致性" class="headerlink" title=" C-Consistency 一致性 "></a><font color="red"> C-Consistency 一致性 </font></h3><p>数据库一直处于一致的状态，事务的运行不会改变原本的一致性约束。<br>例如某表上有完整性约束”a + b = 10”，如果一个事务改变了a，那么b必定会发生改变，使得事务结束后依然满足该约束，否则事务失败</p>
<h3 id="I-Isolation-独立性"><a href="#I-Isolation-独立性" class="headerlink" title=" I-Isolation 独立性 "></a><font color="red"> I-Isolation 独立性 </font></h3><p>并发的事务之间不会互相影响，如果A事务访问的数据正在被B事务修改，只要B事务未提交，A访问的数据就不会受到影响</p>
<h3 id="D-Durability-持久性"><a href="#D-Durability-持久性" class="headerlink" title=" D-Durability 持久性 "></a><font color="red"> D-Durability 持久性 </font></h3><p>一旦事务提交，所有的修改将会永久保存在数据库上，即使宕机也不会丢失，除非进行数据库恢复</p>
<h2 id="BASE"><a href="#BASE" class="headerlink" title=" BASE "></a><font color="green"> BASE </font></h2><p>Basically Available, Soft-state, Eventually Consistent</p>
<p>Basically Availble –基本可用</p>
<p>Soft-state –软状态/柔性事务。 “Soft state” 可以理解为”无连接”的, 而 “Hard state” 是”面向连接”的</p>
<p>Eventual Consistency –最终一致性 最终一致性， 也是是 ACID 的最终目的。</p>
<h2 id="CAP"><a href="#CAP" class="headerlink" title=" CAP "></a><font color="green"> CAP </font></h2><p>计算机科学中，<a href="http://blog.csdn.net/chen77716/article/details/30635543" target="_blank" rel="external"><font color="#AAAAAA"> CAP定理 </font></a>揭示了分布式系统的本质，并给出了设计准则，它指出任何分布式计算系统不能同时满足<br>一致性(Consistency) - 所有节点在同一时间具有相同的数据<br>可用性(Availability) - 保证每个请求都有响应，无论响应成功还是失败<br>分区容错性(Partition tolerance) - 系统中任意信息的丢失或失败不会影响系统的继续运作</p>
<p>CA without P,如果不要求P(分区)，那么CA(强一致性和可用性)是可以保证的，但分区是始终存在的问题，因此CA系统更多的是允许分区后各子系统依然保证CA。</p>
<p>CP without A，如果不要求A，相当于每个请求都需要server之间强一致，而P会导致时间无限延长，因此CP也是可以保证的，很多传统的数据库分布式事务都属于这种模式</p>
<p>AP without C，要高可用并允许分区，则需放弃一致性，一旦分区发生，节点间可能失去联系，为了高可用，每个节点只能用本地数据提供服务，而这会导致全局数据的不一致性，众多NoSQL数据库都属于此类</p>
<p>因此，根据CAP原理将数据库分成满足CA,CP,AP的三大类:</p>
<p><img src="http://p09u6sy9g.bkt.clouddn.com/1516593700888.png" alt=""></p>
<h1 id="为什么用NoSQL"><a href="#为什么用NoSQL" class="headerlink" title=" 为什么用NoSQL "></a><font color="#5CACEE"> 为什么用NoSQL </font></h1><h2 id="RDBMS"><a href="#RDBMS" class="headerlink" title=" RDBMS "></a><font color="green"> RDBMS </font></h2><ul>
<li>高度组织化结构化数据 </li>
<li>结构化查询语言（SQL） (SQL) </li>
<li>数据和关系都存储在单独的表中。 </li>
<li>数据操纵语言，数据定义语言 </li>
<li>严格的一致性</li>
<li>基础事务</li>
</ul>
<h2 id="NoSQL"><a href="#NoSQL" class="headerlink" title=" NoSQL "></a><font color="green"> NoSQL </font></h2><ul>
<li>代表着不仅仅是SQL</li>
<li>没有声明性查询语言</li>
<li>没有预定义的模式</li>
<li>键 - 值对存储，列存储，文档存储，图形数据库</li>
<li>最终一致性，而非ACID属性</li>
<li>非结构化和不可预知的数据</li>
<li>CAP定理 </li>
<li>高性能，高可用性和可伸缩性</li>
</ul>
<p>优点:</p>
<p>1.高可扩展性</p>
<p>2.分布式计算</p>
<p>3.低成本</p>
<p>4.架构灵活，可以处理半结构化/非结构化数据</p>
<p>5.不用处理复杂的关系模型</p>
<p>缺点:</p>
<p>1.没有关系型数据库发展得那么成熟</p>
<p>2.查询功能有限</p>
<p>3.是一种最终一致性的系统，它们为了高的可用性牺牲了一部分的一致性</p>
<p>可以通过第三方平台（如：Google,Facebook等）可以很容易的访问和抓取数据。用户的个人信息，社交网络，地理位置，用户生成的数据和用户操作日志已经成倍的增加。我们如果要对这些用户数据进行挖掘，那SQL数据库已经不适合这些应用了, NoSQL数据库的发展也却能很好的处理这些大的数据。</p>
<p><img src="http://p09u6sy9g.bkt.clouddn.com/1516691622785.png" alt=""></p>
]]></content>
      
        <categories>
            
            <category> NoSQL </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Theory </tag>
            
            <tag> Database </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Mongodb补充]]></title>
      <url>/2018/01/19/mongodb%E8%A1%A5%E5%85%85/</url>
      <content type="html"><![CDATA[<h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a><font color="#5CACEE">简介</font></h1><p>MongoDB 是一个由 C++ 语言编写的基于分布式文件存储的数据库，旨在为 WEB 应用提供可扩展的高性能数据存储解决方案，这里简要介绍一下基本操作和存储原理，勉强能够应付日常工作。如果主要工作环境和业务都是mongo，那么就需要深入研究了，本文是远远不够的。</p>
<a id="more"></a>
<h1 id="P1（基础阶段）"><a href="#P1（基础阶段）" class="headerlink" title=" P1（基础阶段） "></a><font color="#5CACEE"> P1（基础阶段） </font></h1><h2 id="增"><a href="#增" class="headerlink" title=" 增 "></a><font color="green"> 增 </font></h2><blockquote>
<p>db.COLLECTION_NAME.insert(document)</p>
<p>例:</p>
<pre><code>db.col.insert({title: &apos;MongoDB 教程&apos;, 
    description: &apos;MongoDB 是一个 Nosql 数据库&apos;,
    by: &apos;菜鸟教程&apos;,
    url: &apos;http://www.runoob.com&apos;,
    tags: [&apos;mongodb&apos;, &apos;database&apos;, &apos;NoSQL&apos;],
    likes: 100
})
</code></pre></blockquote>
<h2 id="删"><a href="#删" class="headerlink" title=" 删 "></a><font color="green"> 删 </font></h2><p>db.collection.remove(<br>query,<br>justOne<br>)</p>
<pre><code>db.col.remove({&apos;title&apos;:&apos;MongoDB 教程&apos;})

db.col.remove({})

db.col.find()
</code></pre><h2 id="改"><a href="#改" class="headerlink" title=" 改 "></a><font color="green"> 改 </font></h2><blockquote>
<p>db.collection.update(<br>   <query>,<br>   <update>,<br>   {<br>     upsert: <boolean>,<br>     multi: <boolean>,<br>     writeConcern: <document><br>   }<br>)</document></boolean></boolean></update></query></p>
<p>例:</p>
<p>db.col.update({‘title’:’MongoDB 教程’},{$set:{‘title’:’MongoDB’}})</p>
<p>db.col.update({‘title’:’MongoDB 教程’},{$set:{‘title’:’MongoDB’}},{multi:true})</p>
<p>db.col.save({<br>    “_id” : ObjectId(“56064f89ade2f21f36b03136”),<br>    “title” : “MongoDB”,<br>    “description” : “MongoDB 是一个 Nosql 数据库”,<br>    “by” : “Runoob”,<br>    “url” : “<a href="http://www.runoob.com" target="_blank" rel="external">http://www.runoob.com</a>“,<br>    “tags” : [<br>            “mongodb”,<br>            “NoSQL”<br>    ],<br>    “likes” : 110<br>})</p>
</blockquote>
<h2 id="查"><a href="#查" class="headerlink" title=" 查 "></a><font color="green"> 查 </font></h2><blockquote>
<p>MongoDB 查询文档使用 find() 方法,以非结构化的方式来显示所有文档。</p>
<p>db.collection.find(query, projection)</p>
<p>可以使用where，大于等于小于，与或非等条件进行查询，具体语句自行查找</p>
</blockquote>
<h2 id="排序"><a href="#排序" class="headerlink" title=" 排序 "></a><font color="green"> 排序 </font></h2><blockquote>
<p>db.COLLECTION_NAME.find().sort({KEY:1})</p>
</blockquote>
<pre><code>db.col.find({},{&quot;title&quot;:1,_id:0}).sort({&quot;likes&quot;:-1})
</code></pre><h2 id="索引"><a href="#索引" class="headerlink" title=" 索引 "></a><font color="green"> 索引 </font></h2><blockquote>
<p>db.COLLECTION_NAME.ensureIndex({KEY:1})</p>
</blockquote>
<pre><code>db.col.ensureIndex({&quot;title&quot;:1})
</code></pre><h2 id="聚合"><a href="#聚合" class="headerlink" title=" 聚合 "></a><font color="green"> 聚合 </font></h2><blockquote>
<p>MongoDB中聚合(aggregate)主要用于处理数据(诸如统计平均值,求和等)，并返回计算后的数据结果。有点类似sql语句中的 count(*)。</p>
<p>db.COLLECTION_NAME.aggregate(AGGREGATE_OPERATION)</p>
</blockquote>
<pre><code>db.mycol.aggregate([{$group : {_id : &quot;$by_user&quot;, num_tutorial : {$sum : 1}}}])
</code></pre><h2 id="mongo集群搭建"><a href="#mongo集群搭建" class="headerlink" title=" mongo集群搭建 "></a><font color="green"> mongo集群搭建 </font></h2><blockquote>
<p>大概就是副本集，主从，sharding，目的是将数据同步到多个服务器，复制提供数据的冗余备份，从而提高了数据的可用性， 并保证数据的安全性，主节点记录在其上的所有操作oplog，从节点定期轮询主节点获取这些操作，然后对自己的数据副本执行这些操作，从而保证从节点的数据与主节点一致。</p>
</blockquote>
<h2 id="mongo备份恢复"><a href="#mongo备份恢复" class="headerlink" title=" mongo备份恢复 "></a><font color="green"> mongo备份恢复 </font></h2><blockquote>
<p>在另一篇博客里有应用场景的命令格式，大概就是用mongodump和mongorestor两个命令实现数据导出到备份集和导入</p>
</blockquote>
<h2 id="mongo监控"><a href="#mongo监控" class="headerlink" title=" mongo监控 "></a><font color="green"> mongo监控 </font></h2><p>mongostat命令是mongo自带的状态检测工具，可以间隔固定时间获取并输出mongodb的当前运行状态，数据库变慢或者有异常的首选操作</p>
<h1 id="P2-稍高一点"><a href="#P2-稍高一点" class="headerlink" title=" P2(稍高一点) "></a><font color="#5CACEE"> P2(稍高一点) </font></h1><h2 id="mongodb-map-reduce"><a href="#mongodb-map-reduce" class="headerlink" title=" mongodb map reduce "></a><font color="green"> mongodb map reduce </font></h2><p>Map-Reduce是一种计算模型，简单的说就是将大批量的工作（数据）分解（MAP）执行，然后再将结果合并成最终结果（REDUCE），MongoDB提供的Map-Reduce非常灵活，对于大规模数据分析也相当实用</p>
<p>以下是MapReduce的基本语法：</p>
<pre><code>&gt;db.collection.mapReduce(
   function() {emit(key,value);},  //map 函数
   function(key,values) {return reduceFunction},   //reduce 函数
   {
  out: collection,
  query: document,
  sort: document,
  limit: number
   }
)
</code></pre><p>使用 MapReduce 要实现两个函数 Map 函数和 Reduce 函数,Map 函数调用 emit(key, value), 遍历 collection 中所有的记录, 将 key 与 value 传递给 Reduce 函数进行处理。</p>
<p>Map 函数必须调用 emit(key, value) 返回键值对。</p>
<p>参数说明:</p>
<ul>
<li>map ：映射函数 (生成键值对序列,作为 reduce 函数参数)。</li>
<li>reduce 统计函数，reduce函数的任务就是将key-values变成key-value，也就是把values数组变成一个单一的值value。。</li>
<li>out 统计结果存放集合 (不指定则使用临时集合,在客户端断开后自动删除)。</li>
<li>query 一个筛选条件，只有满足条件的文档才会调用map函数。（query。limit，sort可以随意组合）</li>
<li>sort 和limit结合的sort排序参数（也是在发往map函数前给文档排序），可以优化分组机制</li>
<li>limit 发往map函数的文档数量的上限（要是没有limit，单独使用sort的用处不大）</li>
</ul>
<p>以下实例在集合 orders 中查找 status:”A” 的数据，并根据 cust_id 来分组，并计算 amount 的总和。</p>
<p><img src="http://p09u6sy9g.bkt.clouddn.com/1516698348837.png" alt=""></p>
<p>操作实例</p>
<p>准备数据集</p>
<pre><code>db.posts.insert({
   &quot;post_text&quot;: &quot;菜鸟教程，最全的技术文档。&quot;,
   &quot;user_name&quot;: &quot;mark&quot;,
   &quot;status&quot;:&quot;active&quot;
})
WriteResult({ &quot;nInserted&quot; : 1 })
db.posts.insert({
   &quot;post_text&quot;: &quot;菜鸟教程，最全的技术文档。&quot;,
   &quot;user_name&quot;: &quot;mark&quot;,
   &quot;status&quot;:&quot;active&quot;
})
WriteResult({ &quot;nInserted&quot; : 1 })
db.posts.insert({
   &quot;post_text&quot;: &quot;菜鸟教程，最全的技术文档。&quot;,
   &quot;user_name&quot;: &quot;mark&quot;,
   &quot;status&quot;:&quot;active&quot;
})
WriteResult({ &quot;nInserted&quot; : 1 })
db.posts.insert({
   &quot;post_text&quot;: &quot;菜鸟教程，最全的技术文档。&quot;,
   &quot;user_name&quot;: &quot;mark&quot;,
   &quot;status&quot;:&quot;active&quot;
})
WriteResult({ &quot;nInserted&quot; : 1 })
db.posts.insert({
   &quot;post_text&quot;: &quot;菜鸟教程，最全的技术文档。&quot;,
   &quot;user_name&quot;: &quot;mark&quot;,
   &quot;status&quot;:&quot;disabled&quot;
})
WriteResult({ &quot;nInserted&quot; : 1 })
db.posts.insert({
   &quot;post_text&quot;: &quot;菜鸟教程，最全的技术文档。&quot;,
   &quot;user_name&quot;: &quot;runoob&quot;,
   &quot;status&quot;:&quot;disabled&quot;
})
WriteResult({ &quot;nInserted&quot; : 1 })
db.posts.insert({
   &quot;post_text&quot;: &quot;菜鸟教程，最全的技术文档。&quot;,
   &quot;user_name&quot;: &quot;runoob&quot;,
   &quot;status&quot;:&quot;disabled&quot;
})
WriteResult({ &quot;nInserted&quot; : 1 })
db.posts.insert({
   &quot;post_text&quot;: &quot;菜鸟教程，最全的技术文档。&quot;,
   &quot;user_name&quot;: &quot;runoob&quot;,
   &quot;status&quot;:&quot;active&quot;
})
WriteResult({ &quot;nInserted&quot; : 1 })
</code></pre><p>在 posts 集合中使用 mapReduce 函数来选取已发布的文章(status:”active”)，并通过user_name分组，计算每个用户的文章数</p>
<pre><code>db.posts.mapReduce( 
   function() { emit(this.user_name,1); }, 
   function(key, values) {return Array.sum(values)}, 
  {  
 query:{status:&quot;active&quot;},  
 out:&quot;post_total&quot; 
  }
)
</code></pre><p>结果如下</p>
<p><img src="http://p09u6sy9g.bkt.clouddn.com/1516700750023.png" alt=""></p>
<p>结果表明，共有 5 个符合查询条件（status:”active”）的文档， 在map函数中生成了 5 个键值对文档，最后使用reduce函数将相同的键值分为 2 组。</p>
<p>用类似的方式，MapReduce可以被用来构建大型复杂的聚合查询。</p>
<p>Map函数和Reduce函数可以使用 JavaScript 来实现，使得MapReduce的使用非常灵活和强大。</p>
<p>以小见大，mongo集群的存储性能，可扩展性，对mapreduce的支持，使得分布式集群搭建后运行各种数据分析挖掘算法来处理海量数据有了可靠的保证。</p>
<h2 id="mongodb-全文检索"><a href="#mongodb-全文检索" class="headerlink" title=" mongodb 全文检索 "></a><font color="green"> mongodb 全文检索 </font></h2><p>全文检索对每一个词建立一个索引，指明该词在文章中出现的次数和位置，当用户查询时，检索程序就根据事先建立的索引进行查找，并将查找的结果反馈给用户的检索方式。</p>
<p>这个过程类似于通过字典中的检索字表查字的过程。</p>
<p>MongoDB 从 2.4 版本开始支持全文检索，目前支持15种语言(暂时不支持中文)的全文索引。</p>
<p><img src="http://p09u6sy9g.bkt.clouddn.com/1516701675326.png" alt=""></p>
<h2 id="mongodb-正则表达式"><a href="#mongodb-正则表达式" class="headerlink" title=" mongodb 正则表达式 "></a><font color="green"> mongodb 正则表达式 </font></h2><p>正则表达式是使用单个字符串来描述，匹配一系列符合某个语法规则的字符串。</p>
<p>正则表达式在文本处理方面极为实用，很多语言都支持利用正则表达式进行字符串操作</p>
<p>mongodb使用$regex操作符来设置匹配字符串的正则表达式</p>
<p>可以设置 $options 为 $i 使正则匹配忽略大小写</p>
<p><img src="http://p09u6sy9g.bkt.clouddn.com/1516759167493.png" alt=""></p>
<p>如果你的文档中字段设置了索引，那么使用索引相比于正则表达式匹配查找所有的数据查询速度更快</p>
<h2 id="mongodb-管理工具-Rockmongo"><a href="#mongodb-管理工具-Rockmongo" class="headerlink" title=" mongodb 管理工具: Rockmongo "></a><font color="green"> mongodb 管理工具: Rockmongo </font></h2><p><a href="https://github.com/iwind/rockmongo.git" target="_blank" rel="external"><font color="#AAAAAA">rockmongo</font></a>是php写的一个mongodb管理工具，可以用来管理服务，数据库，集合，文档，索引等，具体使用方法等用到了学就是，界面管理方法想来也是跟Plsqldeveloper和navicat那些差不多的页面版</p>
<h1 id="P3-再高一点点"><a href="#P3-再高一点点" class="headerlink" title=" P3(再高一点点) "></a><font color="#5CACEE"> P3(再高一点点) </font></h1><p>mongo是常用的面向文档非关系数据库，主要应用场景在微博，博客等消息存储业务，数据与金融等传统行业比起来没有那么重要，对事务要求相对不高，这时候用mongo比关系型数据库更合适，因为关系型数据库每次操作都有ACK，mongo省去了这一步，大大提高存储性能，同时设计时就考虑了廉价设备的容错和业务增长时的可扩展性。</p>
<p>在其他的博客内容对mongo的集群搭建有了一定的认识，这里通过一种副本集与分片混合部署的方案和数据存取，数据存储，对mongo的应用做进一步的介绍。</p>
<p><img src="http://p09u6sy9g.bkt.clouddn.com/1516760767229.png" alt=""></p>
<p>副本集中每个节点存储的数据都是相同的，相当于主备方式的数据冗余，分片是为了数据拓展，按照片键进行节点划分，数据根据片键存储到对应服务器上。</p>
<p>mongo的集群中有三类角色:实际数据存储节点，配置文件存储节点和路由介入节点。</p>
<blockquote>
<p>ps:在另一个介绍集群搭建方案的博客里，副本集配置里除了存储节点还有一个arbiter节点，负责节点故障的时候仲裁主节点切换，实测有该节点时双节点中主节点停/起副节点会自动切主备，没有则主掉了副节点还是副节点，不会自动切换</p>
</blockquote>
<p>客户端与路由节点连接，从配置节点上查询数据，根据查询结果到实际的存储节点上查询和存储数据。</p>
<p><img src="http://p09u6sy9g.bkt.clouddn.com/1516761696689.png" alt=""></p>
<blockquote>
<p>客户端发起写操作请求，路由节点接到请求后查询配置服务器，得到有存储空间的服务器，路由节点把写入操作转发给由副本集组成的数据存储服务器，管理过程类似hdfs中master节点对chunk server的管理</p>
</blockquote>
<p><img src="http://p09u6sy9g.bkt.clouddn.com/1516761711577.png" alt=""></p>
<blockquote>
<p>客户端发起读操作请求，路由节点接到请求后查询配置服务器，根据存储服务器中的记录，返回目标数据的存储服务器路径大小等信息，路由节点转发读操作到存储服务器，获取信息后路由服务器将查询结果返回给客户端</p>
</blockquote>
<p>副本集内部的写操作</p>
<p><img src="http://p09u6sy9g.bkt.clouddn.com/1516762205709.png" alt=""></p>
<blockquote>
<p>写操作只写到主节点当中，由主节点以异步的方式同步到从节点</p>
</blockquote>
<p>副本集内部的读操作</p>
<p><img src="http://p09u6sy9g.bkt.clouddn.com/1516762230222.png" alt=""></p>
<blockquote>
<p>可以从任意节点读取数据，也可以指定具体到哪个节点</p>
</blockquote>
<p><img src="http://p09u6sy9g.bkt.clouddn.com/1516763021610.png" alt=""></p>
<p><img src="http://p09u6sy9g.bkt.clouddn.com/1516763044775.png" alt=""></p>
<h1 id="P4-可以说相当高了"><a href="#P4-可以说相当高了" class="headerlink" title=" P4(可以说相当高了) "></a><font color="#5CACEE"> P4(可以说相当高了) </font></h1><h2 id="存储引擎-wiredTiger"><a href="#存储引擎-wiredTiger" class="headerlink" title="存储引擎:wiredTiger"></a><font color="green">存储引擎:<a href="http://www.mongoing.com/archives/2540" target="_blank" rel="external">wiredTiger</a></font></h2><p>按照Mongodb默认的配置，WiredTiger的写操作会先写入Cache，并持久化到WAL(Write ahead log)，每60s或log文件达到2GB时会做一次Checkpoint，将当前的数据持久化，产生一个新的快照。Wiredtiger连接初始化时，首先将数据恢复至最新的快照状态，然后根据WAL恢复数据，以保证存储可靠性</p>
<p>所有write请求都基于”文档级别”的lock，因此多个客户端可以同时更新一个collection中的不同文档，这种更细颗粒度的lock，可以支撑更高的读写负载和并发量，因为对于production环境，更多的CPU可以有效提升wiredTiger性能，因为它是多线程IO。</p>
<h2 id="架构模式"><a href="#架构模式" class="headerlink" title="架构模式"></a><font color="green">架构模式</font></h2><p>Replica set：复制集，mongodb的架构方式之一 ，通常是三个对等的节点构成一个“复制集”集群</p>
<p>Sharding cluster：分片集群，数据水平扩展的手段之一,replica set这种架构的缺点就是“集群数据容量”受限于单个节点的磁盘大小，如果数据量不断增加，对它进行扩容将时非常苦难的事情，所以我们需要采用Sharding模式来解决这个问题。将整个collection的数据将根据sharding key被sharding到多个mongod节点上，即每个节点持有collection的一部分数据，这个集群持有全部数据，原则上sharding可以支撑数TB的数据。</p>
<p>系统配置：</p>
<pre><code>- 建议mongodb部署在linux系统上，较高版本，选择合适的底层文件系统（ext4），开启合适的swap空间  

- 无论是MMAPV1或者wiredTiger引擎，较大的内存总能带来直接收益。

- 对数据存储文件关闭“atime”（文件每次access都会更改这个时间值，表示文件最近被访问的时间），可以提升文件访问效率。 

- ulimit参数调整，这个在基于网络IO或者磁盘IO操作的应用中，通常都会调整，上调系统允许打开的文件个数（ulimit -n 65535）
</code></pre><h2 id="数据文件存储原理"><a href="#数据文件存储原理" class="headerlink" title="数据文件存储原理"></a><font color="green">数据文件存储原理</font></h2><p><img src="http://p09u6sy9g.bkt.clouddn.com/1516777869529.png" alt=""></p>
<h3 id="Data-Files"><a href="#Data-Files" class="headerlink" title="Data Files"></a><font color="red">Data Files</font></h3><p>mongodb的数据会保存在底层文件系统中，比如”dbpath=/usr/local/mongodb/master”，创建一个database为”runoob”，collection为”col”，然后插入若干documents，可以看到如下列表</p>
<p><img src="http://p09u6sy9g.bkt.clouddn.com/1516778681686.png" alt=""></p>
<p><img src="http://p09u6sy9g.bkt.clouddn.com/1516778713479.png" alt=""></p>
<p>可以看到数据文件所在目录”/usr/local/mongodb/master”下面生成了”runoob”文件夹，里面存放有若干collection文件，数据文件从16M开始，每次扩张一倍(16M，32M，64M，128M…)，默认情况下迟迟单个data file最大尺寸为2G，smallfile设置后限定512M，每个database最多支持16000个数据文件，约32T，smallfiles设置后单个database最大容量8T</p>
<p>一个database中所有的collections及索引信息会分散存储在多个数据文件中，没有像SQL数据库一样每个表的数据索引分别存储；</p>
<p>数据分块的单位是extent(范围，区域)，extent中可以保存collection数据或者index数据，每个extent包含多条document，大小不等，一个extent只能保存同一个collection的数据，不同collections数据分布在不同extents中，indexs也保存在各自的extent中，一个collection由一个或多个extent构成，最小size8K，最大2G，这些extent分散在多个datafile中，换句话说，一个datafile可能包含多个collection的数据，由不同collection的extent组成，但一个extent不会跨越两个datafile</p>
<p><img src="http://p09u6sy9g.bkt.clouddn.com/1516787466812.png" alt=""></p>
<p>举个栗子（吼吼），两个数据文件my-db.1和my-db.2里存放A和B两张表的数据，AB两张表上的document和index存放在一个个小的extent里面，这些extent并不是各自集中存放，而是散布在两个数据文件里面</p>
<p>在每个database的namespace文件中，比如test.ns文件中，每个collection只保存了第一个extent的位置信息，并不保存所有的extents列表，但每个extent都维护者一个链表关系，即每个extent都在其header信息中记录了此extent的上一个、下一个extent的位置信息，这样当对此collection进行scan操作时（比如全表扫描），可以提供很大的便利性。</p>
<p>由存储机制可以很容易想到，删除document会导致磁盘碎片，有些update也会导致磁盘碎片，比如update导致文档尺寸变大，进而超过原来分配的空间；当有新的insert操作时，mongodb会检测现有的extents中是否合适的碎片空间可以被重用，如果有，则重用这些fragment，否则分配新的存储空间。磁盘碎片对write操作有一定的性能影响，而且会导致磁盘空间浪费；</p>
<p> 如果database已经运行一段时间，数据已经有很大的磁盘碎片（storageSize与dataSize比较），可以通过mongodump将指定database的所有数据导出，然后将原有的db删除，再通过mongorestore指令将数据重新导入</p>
<h3 id="Namespace文件"><a href="#Namespace文件" class="headerlink" title="Namespace文件"></a><font color="red">Namespace文件</font></h3><p>对于namespace文件，比如“test.ns”文件，默认大小为16M，此文件中主要用于保存“collection”、index的命名信息，比如collection的“属性”信息、每个索引的属性类型等，如果你的database中需要存储大量的collection（比如每一小时生成一个collection，在数据分析应用中），那么我们可以通过配置文件“nsSize”选项来指定，参见<a href="http://blog.csdn.net/sun491922556/article/details/74973191" target="_blank" rel="external">某博客</a>。</p>
<h3 id="journal文件"><a href="#journal文件" class="headerlink" title="journal文件"></a><font color="red">journal文件</font></h3><p>journal日志保存在dbpath下“journal”子目录中，一般会有三个journal文件，journal文件中保存了write操作的记录，每条记录中包含write操作内容之外，还包含一个“lsn”（last sequence number），表示此记录的ID</p>
<p> journal是一个预写事务日志，来确保数据持久性，存储引擎每隔60s(默认/可调)或者待写入数据达到2G，mongo将对journal文件提交一个checkpoint检测点，将内存中的数据变更flush到磁盘的数据文件中，并做一个标记点，表示此前的数据已经持久存储在数据文件中，检测点创建后，journal日志清空，后续数据更改存在于内存和journal日志。</p>
<p> 例如write操作首先被写入journal日志，然后在内存中变更数据，数据量积累或者时间间隔条件满足后触发检测点，数据flush到数据文件。即检测点之前的数据只是在journal中持久存储，并没有写入数据文件，延迟持久化可以提高磁盘效率，如果在checkpoint之前mongo异常退出，再次启动可以根据journal恢复数据。</p>
<p>默认情况下，“journal”特性是开启的，特别在production环境中，我们没有理由来关闭它，当然也可以关闭journal，提高了性能，但单点mongo降低了数据安全性，如果有异常则丢失checkpoint之间的数据，副本集在所有节点同时退出的情况时有影响。</p>
<p>如果你希望数据尽可能的不丢失，可以考虑：</p>
<p>1）减小commitIntervalMs的值 </p>
<p>2）每个write指定“write concern”中指定“j”参数为true  </p>
<p>3）最佳手段就是采用“replica set”架构模式，通过数据备份方式解决，同时还需要在“write concern”中指定“w”选项，且保障级别不低于“majority”，最终我们需要在“写入性能”和“数据一致性”两个方面权衡，即CAP理论。</p>
<p><strong>至此mongodb的基础内容介绍得差不多，具体实践和高级技巧等工作项目中遇到了再深入研究，理论指导实践，在实<br>践中检验和调整升华理论，然后在下一次实践中表现得更加出色，理论和实践的螺旋式上升是不变的旋律</strong>。</p>
]]></content>
      
        <categories>
            
            <category> mongodb </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Theory </tag>
            
            <tag> Database </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Mongodb]]></title>
      <url>/2018/01/18/Mongodb/</url>
      <content type="html"><![CDATA[<h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a><font color="#5CACEE">简介</font></h1><blockquote>
<p>Mongodb是时下流行的Nosql数据库，存储方式是文档式存储，并不是key-value形式。</p>
<p>参考<a href="http://blog.csdn.net/luonanqin/article/details/8497860" target="_blank" rel="external"><font color="#AAAAAA">CSDN的博客</font></a>，在本地使用三种方式实际搭建mongo集群:</p>
<p>1.replica set</p>
<p>2.sharding</p>
<p>3.master-slaver</p>
<p>万丈高楼平地起，无论业务多复杂，架构多庞大，归根到底还是数据库本身各种功能的应用，理解了数据库的设计原理，数据存储形式，掌握了基本的功能应用，集群构建，数据流动抽取备份，到了实际工作场景中无非数据量更大，应用形式多样化，架构更庞大而已。</p>
</blockquote>
<a id="more"></a>
<h1 id="replica-set"><a href="#replica-set" class="headerlink" title="replica set"></a><font color="#5CACEE">replica set</font></h1><p>副本集，简单来说就是集群当中包含了多份数据，保证主节点挂掉，备节点能继续提供服务，备节点与主节点的数据一致，如图</p>
<p><img src="http://p09u6sy9g.bkt.clouddn.com/1516156461862.png" alt=""></p>
<p>Mongodb(M)表示主节点，Mongodb(S)表示备节点，Mongodb(A)表示仲裁节点，主备节点存储数据，仲裁节点不存储数据，客户端连接主备，不连接仲裁节点。</p>
<p>默认设置下，主节点提供所有增删改查服务，备节点不提供服务。可以设置read preference modes，让备节点提供查询服务来分担主节点的压力，客户端进行的数据查询请求会自动转到备节点。</p>
<p>仲裁节点本身不存储数据，主要作用是决定哪个备节点在主节点挂掉后提升为主节点。多个备节点仍然只需要一个仲裁节点，本文中即使一个备节点也需要仲裁节点，如果没有，主节点挂了备节点还是备节点。</p>
<h2 id="集群设计"><a href="#集群设计" class="headerlink" title="集群设计"></a><font color="green">集群设计</font></h2><table>
<thead>
<tr>
<th>服务器</th>
<th>节点</th>
</tr>
</thead>
<tbody>
<tr>
<td>192.168.3.7</td>
<td>主节点</td>
</tr>
<tr>
<td>192.168.3.8</td>
<td>备节点</td>
</tr>
<tr>
<td>192.168.3.9</td>
<td>仲裁节点</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th>软件版本</th>
<th>下载地址</th>
</tr>
</thead>
<tbody>
<tr>
<td>mongodb-linux-x86_64-3.4.10</td>
<td><a href="https://pan.baidu.com/s/1qZ30PNQ" target="_blank" rel="external">下载地址</a></td>
</tr>
</tbody>
</table>
<h2 id="实验过程"><a href="#实验过程" class="headerlink" title="实验过程"></a><font color="green">实验过程</font></h2><h3 id="解压到指定目录，创建数据文件夹"><a href="#解压到指定目录，创建数据文件夹" class="headerlink" title="解压到指定目录，创建数据文件夹"></a>解压到指定目录，创建数据文件夹</h3><pre><code>tar xvf mongodb-linux-x86_64-3.4.10.tgz -C /usr/local

cd /usr/local

mv mongodb-linux-x86_64-3.4.10/ mongodb

mkdir -p /usr/local/mongodb/data/master   
mkdir -p /usr/local/mongodb/data/slaver   
mkdir -p /usr/local/mongodb/data/arbiter    
三个目录分别对应主，备，仲裁节点
</code></pre><h3 id="建立配置文件-起mongo"><a href="#建立配置文件-起mongo" class="headerlink" title="建立配置文件,起mongo"></a>建立配置文件,起mongo</h3><blockquote>
<p>以下三个文件是三个节点的mongo启动参数文件，编辑好放在任何位置都可以，启动时按绝对路径指定即可</p>
</blockquote>
<pre><code>#master.conf  
dbpath=/usr/local/mongodb/data/master  
logpath=/usr/local/mongodb/log/master.log  
pidfilepath=/usr/local/mongodb/master.pid  
directoryperdb=true  
logappend=true  
replSet=testrs  
bind_ip=192.168.3.7  
port=27017
oplogSize=10000  
fork=true  
noprealloc=true

#slaver.conf
dbpath=/usr/local/mongodb/data/slaver  
logpath=/usr/local/mongodb/log/slaver.log  
pidfilepath=/usr/local/mongodb/slaver.pid  
directoryperdb=true  
logappend=true  
replSet=testrs  
bind_ip=192.168.3.8  
port=27017 
oplogSize=10000  
fork=true  
noprealloc=true 


#arbiter.conf  
dbpath=/usr/local/mongodb/data/arbiter  
logpath=/usr/local/mongodb/log/arbiter.log  
pidfilepath=/usr/local/mongodb/arbiter.pid  
directoryperdb=true  
logappend=true  
replSet=testrs  
bind_ip=1192.168.3.9  
port=27017
oplogSize=10000  
fork=true  
noprealloc=true  
</code></pre><blockquote>
<p><strong>参数解释</strong>：</p>
<p>dbpath：数据存放目录</p>
<p>logpath：日志存放路径</p>
<p>pidfilepath：进程文件，方便停止mongodb</p>
<p>directoryperdb：为每一个数据库按照数据库名建立文件夹存放</p>
<p>logappend：以追加的方式记录日志</p>
<p>replSet：replica set的名字</p>
<p>bind_ip：mongodb所绑定的ip地址</p>
<p>port：mongodb进程所使用的端口号，默认为27017</p>
<p>oplogSize：mongodb操作日志文件的最大大小。单位为Mb，默认为硬盘剩余空间的5%</p>
<p>fork：以后台方式运行进程</p>
<p>noprealloc：不预先分配存储</p>
</blockquote>
<pre><code>./mongod -f ../master.conf 
</code></pre><p><img src="http://p09u6sy9g.bkt.clouddn.com/1516175089971.png" alt=""></p>
<p><code>./mongod -f ../slaver.conf</code><br><img src="http://p09u6sy9g.bkt.clouddn.com/1516175119466.png" alt=""></p>
<p><code>./mongod -f ../arbiter.conf</code><br><img src="http://p09u6sy9g.bkt.clouddn.com/1516175135042.png" alt=""></p>
<blockquote>
<p>原博客这里并没有提到需要手动创建log文件夹，经过若干次各个节点若干次启动失败各种原因排查，最终确定在我搭建的环境里需要手动创建log文件夹，但是原博文和推送我博文的朋友他们搭建过程貌似都不需要，不得解，待议。</p>
</blockquote>
<p><img src="http://p09u6sy9g.bkt.clouddn.com/1516174467395.png" alt=""></p>
<h3 id="配置主，备，仲裁节点"><a href="#配置主，备，仲裁节点" class="headerlink" title="配置主，备，仲裁节点"></a>配置主，备，仲裁节点</h3><pre><code>./mongo 192.168.3.7:27018   #ip和port是某个节点的地址  
&gt;use admin  
&gt;cfg={_id:&quot;testrs&quot;,members:[ {_id:0,host:&apos;192.168.3.7:27017&apos;,priority:2}, {_id:1,host:&apos;192.168.3.8:27017&apos;,priority:1},   
{_id:2,host:&apos;192.168.3.9:27017&apos;,arbiterOnly:true}] };  
&gt;rs.initiate(cfg) #使配置生效  
</code></pre><p><img src="http://p09u6sy9g.bkt.clouddn.com/1516176794251.png" alt=""></p>
<blockquote>
<p>现在基本上已经完成了集群的所有搭建工作。</p>
</blockquote>
<pre><code>测试方案:
一个是往主节点插入数据，从备节点查询到之前插入的数据.

二是停掉主节点，让备节点变成主节点提供服务。

三是恢复主节点，备节点也能恢复其备的角色，而不是继续充当主的角色。二和三都可以通过rs.status()命令实时查看集群的变化
</code></pre><h1 id="sharding"><a href="#sharding" class="headerlink" title="sharding"></a><font color="#5CACEE">sharding</font></h1><p>和Replica Set类似，都需要一个仲裁节点，但是Sharding还需要配置节点和路由节点。就三种集群搭建方式来说，这种是最复杂的。部署图如下：<br><img src="http://p09u6sy9g.bkt.clouddn.com/1516182269464.png" alt=""></p>
<p>非常简单，懒得配了，看文档都能想到所有的具体操作和屏幕输出还有最后的结果，如果工作用到现搭就行了，有兴趣的自己去简介的原博客地址去撸一遍。</p>
<h1 id="master-slaver"><a href="#master-slaver" class="headerlink" title="master-slaver"></a><font color="#5CACEE">master-slaver</font></h1><blockquote>
<p>这个是最简答的集群搭建，严格来说不能算是集群，只能说是主备。并且官方已经不推荐这种方式，简单的介绍下，搭建方式相对简单。</p>
</blockquote>
<pre><code>./mongod --master --dbpath /data/masterdb/  #主节点  

./mongod --slave --source &lt;masterip:masterport&gt; --dbpath /data/slavedb/ 备节点  
</code></pre><p><img src="http://p09u6sy9g.bkt.clouddn.com/1516180240698.png" alt=""></p>
<p><img src="http://p09u6sy9g.bkt.clouddn.com/1516180303967.png" alt=""></p>
<blockquote>
<p>只要在主节点和备节点上分别执行这两条命令，Master-Slaver就算搭建完成了。我没有试过主节点挂掉后备节点是否能变成主节点，不过既然官方已经不推荐了，了解即可。</p>
<p>三种集群搭建方式首选<code>Replica Set</code>，只有真的是大数据，Sharding才能显现威力，毕竟备节点同步数据是需要时间的。Sharding可以将多片数据集中到路由节点上进行一些对比，然后将数据返回给客户端，Replica Set的ips在数据达到1400w条时基本能达到1000左右，而Sharding在300w时已经下降到500ips，mongodb吃内存的问题，解决办法只能通过ulimit来控制内存使用量，但是如果控制不好的话，mongodb会挂掉</p>
</blockquote>
<h1 id="mongo数据库工作原理"><a href="#mongo数据库工作原理" class="headerlink" title="mongo数据库工作原理"></a><font color="#5CACEE">mongo数据库工作原理</font></h1><blockquote>
<p>MongoDB使用的是内存映射存储引擎，它会把磁盘IO操作转换成内存操作，如果是读操作，内存中的数据起到缓存的作用，如果是写操作，内存还可以把随机的写操作转换成顺序的写操作，总之可以大幅度提升性能。MongoDB并不干涉内存管理工作，而是把这些工作留给操作系统的虚拟缓存管理器去处理，这样的好处是简化了MongoDB的工作，但坏处是你没有方法很方便的控制MongoDB占多大内存，事实上MongoDB会占用所有能用的内存，所以最好不要把别的服务和MongoDB放一起。</p>
</blockquote>
<p>出于某些原因，你可能想释放掉MongoDB占用的内存，不过前面说了，内存管理工作是由虚拟内存管理器控制的，所以</p>
<pre><code>1.重启服务
2.调整内核参数drop_caches  systemctl vm.drop_caches=3
3.使用MongoDB内置的closeAllDatabases命令

可以通过mongo命令行来监控MongoDB的内存使用情况
db.serverStatus().mem
</code></pre><p>这篇博客简单介绍下怎么使用，具体技术细节服务库架构应用场景等我深入研究回来补上。</p>
]]></content>
      
        <categories>
            
            <category> Mongodb </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Database </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Leanote]]></title>
      <url>/2018/01/16/Leanote/</url>
      <content type="html"><![CDATA[<h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a><font color="#5CACEE">简介</font></h1><blockquote>
<p>Leanote 蚂蚁笔记是一款国产的在线网页版云笔记软件， 集知识管理、笔记、分享、博客功能于一身，界面简约但功能不简单！它支持多笔记本、标签分类、笔记共享、添加保存附件等，而且还提供了免打扰写作模式、支持图片尺寸调整、并且支持 Markdown 语法写作，最重要的是，它还能完美支持代码高亮显示！</p>
<p>该项目采用 Golang+MongoDB 开发，现已完全开源并能免费使用。普通用户可以直接使用 Leanote 提供的公共服务，也可以自行搭建属于自己或公司局域网内的私有云笔记平台。 而且，Leanote 可以让用户创建一个用户组，并将笔记共享到这个组里，所有组员都可以浏览、编辑笔记，可以非常方便地进行协作或知识共享。（题外话：团队内还可以通过 SeaFile 搭建私有的云存储配合使用）</p>
</blockquote>
<a id="more"></a>
<h1 id="搭建思路"><a href="#搭建思路" class="headerlink" title=" 搭建思路 "></a><font color="#5CACEE"> 搭建思路 </font></h1><pre><code>1.上传mongodb软件包到服务器，解压，起服务

2.上传leanote软件包到服务器，解压，修改配置文件，向mongodb导入lenote文件夹下的原始库表

4.起leanote服务
</code></pre><h1 id="软件包"><a href="#软件包" class="headerlink" title=" 软件包 "></a><font color="#5CACEE"> 软件包 </font></h1><table>
<thead>
<tr>
<th>软件包</th>
<th>下载地址</th>
</tr>
</thead>
<tbody>
<tr>
<td>mongodb-linux-x86_64-3.0.1</td>
<td><a href="https://pan.baidu.com/s/1dGGE0hV" target="_blank" rel="external"><font color="#AAAAAA">点击下载</font></a></td>
</tr>
<tr>
<td>leanote-linux-amd64-v2.6</td>
<td><a href="https://pan.baidu.com/s/1jJynuOQ" target="_blank" rel="external"><font color="#AAAAAA">点击下载</font></a></td>
</tr>
<tr>
<td>leanote-desktop</td>
<td><a href="https://pan.baidu.com/s/1sncACfb" target="_blank" rel="external"><font color="#AAAAAA">点击下载</font></a></td>
</tr>
</tbody>
</table>
<h1 id="实验步骤"><a href="#实验步骤" class="headerlink" title=" 实验步骤 "></a><font color="#5CACEE"> 实验步骤 </font></h1><h2 id="1-上传，解压，创建数据文件夹，起服务"><a href="#1-上传，解压，创建数据文件夹，起服务" class="headerlink" title="1.上传，解压，创建数据文件夹，起服务"></a><font color="green">1.上传，解压，创建数据文件夹，起服务</font></h2><pre><code>$ tar xf mongodb-linux-x86_64-3.0.1.tgz -C /usr/local

$ mv mongodb-linux-x86_64-3.0.1/ mongodb

$ mkdir -p /usr/local/mongodb/data

$ /usr/local/mongodb/bin/mongod --dbpath /usr/local/mongodb/data/ --bind_ip 127.0.0.1 &amp;&gt;&gt; /tmp/mongo.log &amp;  

$ kill -4 xxx        # xxx为mongo的进程号，安全关闭
</code></pre><h2 id="2-上传，解压，修改配置文件，导原始库表"><a href="#2-上传，解压，修改配置文件，导原始库表" class="headerlink" title="2.上传，解压，修改配置文件，导原始库表"></a><font color="green">2.上传，解压，修改配置文件，导原始库表</font></h2><pre><code>$ tar xf leanote-linux-amd64-v2.6.bin.tar.gz -C /usr/local/

$ vi /usr/local/leanote/conf/app.conf 

修改服务端口，加密字符串

$ /usr/local/mongodb/bin/mongorestore -h localhost -d leanote --dir /usr/local/leanote/mongodb_backup/leanote_install_data/
</code></pre><h2 id="3-起服务并验证可用"><a href="#3-起服务并验证可用" class="headerlink" title="3.起服务并验证可用"></a><font color="green">3.起服务并验证可用</font></h2><pre><code>$ cd /usr/local/leanote/bin

$ nohup ./run.sh &amp;   
</code></pre><p><img src="http://p09u6sy9g.bkt.clouddn.com/1516067074310.png" alt=""></p>
<blockquote>
<p>进程正常</p>
</blockquote>
<p><img src="http://p09u6sy9g.bkt.clouddn.com/1516067113075.png" alt=""></p>
<blockquote>
<p>服务正常</p>
</blockquote>
<p><em>至此以私人服务器为server，<strong>仅供个人及好友使用</strong>的leanote搭建完成。</em></p>
<p><em>能在线编辑保存普通文件</em></p>
<p><em>能在线编辑保存markdown文件并发布为个人博客等</em></p>
<h1 id="效果演示"><a href="#效果演示" class="headerlink" title=" 效果演示 "></a><font color="#5CACEE"> 效果演示 </font></h1><p><img src="http://p09u6sy9g.bkt.clouddn.com/1516067579674.png" alt=""></p>
<blockquote>
<p>这是leanote登陆页面，访问服务器的9000端口，如该服务器ip为192.168.x.x，访问”<a href="http://192.168.x.x:9000&quot;即可进入该页面。" target="_blank" rel="external">http://192.168.x.x:9000&quot;即可进入该页面。</a></p>
<p>因私人服务器性能有限，且非盈利自用，管理员已经关闭注册功能，使用该私人服务的唯一途径就是向管理者申请，通过后台单独添加用户</p>
</blockquote>
<p><img src="http://p09u6sy9g.bkt.clouddn.com/1516070707862.png" alt=""></p>
<blockquote>
<p>这是在线编辑页面，可以设置头像，访问别名，自定义很多参数，可以创建普通文本文件并保存，进行上传附件，插入图片，网页链接，版本管理等操作，类似有道云，为知等在线笔记</p>
</blockquote>
<p><img src="http://p09u6sy9g.bkt.clouddn.com/toblog.gif" alt=""></p>
<blockquote>
<p>可以将写好的markdown文件公布为博客，任何人都可以通过链接访问该博客，添加评论。</p>
</blockquote>
<p><img src="http://p09u6sy9g.bkt.clouddn.com/1516071739785.png" alt=""></p>
<blockquote>
<p>访问该网站就是对server服务器的9000端口发起访问请求，leanote将数据库的数据展示出来，用户编辑的普通文件和博客文件在保存后都会经过leanote的加密字符串运算加密后存放在数据库，本人对自己的文章具有最高权限，admin管理员具有后台添加删除用户并设置一些参数等权限，对普通用户的博客文章没有修改删除等权限。</p>
</blockquote>
<p><img src="http://p09u6sy9g.bkt.clouddn.com/desk.gif" alt=""></p>
<blockquote>
<p>有没有发现软件包有三个文件至此只用了两个，没错，第三个是leanote桌面版，下载解压就能用，不用安装，登陆的时候选择登陆自建服务，进去后可以像在线一样编辑文件，完成后保存，公布为博客，软件内即可访问自己的博客，而这所有的都是开源免费，如此干净高效，良心得我有点感动。</p>
</blockquote>
<h1 id="解锁更多姿势"><a href="#解锁更多姿势" class="headerlink" title=" 解锁更多姿势 "></a><font color="#5CACEE"> 解锁更多姿势 </font></h1><h2 id="1-数据库备份策略"><a href="#1-数据库备份策略" class="headerlink" title="1.数据库备份策略"></a><font color="green">1.数据库备份策略</font></h2><pre><code>备份
$ /usr/local/mongodb/bin/mongodump -h 127.0.0.1 -d leanote -o /~（备份集存放路径）


恢复
$ /usr/local/mongodb/bin/mongorestore -h 127.0.0.1 -d leanote --drop --dir ~/leanote （备份集所在路径）
</code></pre><blockquote>
<p>用linux系统自带的crontab定时执行备份脚本，实现每天三次备份，数据备份集可以保证即使服务挂了，甚至服务器挂了，被回收了，随时找个腾讯云或者阿里云的主机，分分钟重新搭建一套，导入数据后恢复服务，对普通用户来说除了期间无法使用之外，没有区别，所有数据都在。</p>
</blockquote>
<pre><code>$ crontab -l

0 6,14,22 * * * /usr/local/mongodb/backup/back.sh

$ more /usr/local/mongodb/backup/back.sh

#!/bin/sh

#该脚本通过crontab定时执行，实现每天6，14，22整点共三次备份mongo数据，保留两周内的备份集

#备份目录为BASEDIR，将带时间戳文件夹的备份集压缩，最后检测是否有14天前的文件夹，有则删除

BASEDIR=&quot;/usr/local/mongodb/backup&quot;

CURDIR=$BASEDIR/`date &quot;+%Y-%m-%d_%H.%M.%S&quot;`

/usr/local/mongodb/bin/mongodump -h 127.0.0.1 -d leanote -o $CURDIR

cd $CURDIR

tar -zcvf leanote.tar.gz leanote

rm -rf leanote/

find $BASEDIR -mtime +14 -exec rm -rf {} \;
</code></pre><h2 id="2-docker部署"><a href="#2-docker部署" class="headerlink" title="2.docker部署"></a><font color="green">2.docker部署</font></h2><p>1.制作docker镜像</p>
<p>docker是个很好用的工具，将系统，应用，程序制作成docker镜像，分发部署使用都会特别方便，这里是leanote的docker镜像，服务器有docker可以直接导入然后使用，没有需要先安装docker。导入制作好的<a href="https://pan.baidu.com/s/1slZC8TR" target="_blank" rel="external"><font color="#AAAAAA">docker镜像</font></a>并启动，这时所有前面配置过程的安装包，备份策略，配置文件更改都已经解决，使用效果一致。</p>
<p>2.一键部署使用<br>$ docker load -i leanote_2.6-image.tar.gz</p>
<p>$ docker history 69e799db2e66</p>
<p>$ docker run -tid –name leanote -p 80:80 –restart always -v /data/leanote:/data/  leanote:2.6</p>
<p>$ docker exec -ti cbc598f37d50 sh</p>
<p>$ tar xf leanote.tar.xz </p>
<p>$ mongorestore -h 127.0.0.1 -d leanote –drop –dir ./leanote</p>
<h2 id="3-使用技巧详细介绍"><a href="#3-使用技巧详细介绍" class="headerlink" title="3.使用技巧详细介绍"></a><font color="green">3.使用技巧详细介绍</font></h2><h3 id="1-普通用户傻瓜攻略"><a href="#1-普通用户傻瓜攻略" class="headerlink" title="1.普通用户傻瓜攻略"></a><font color="red">1.普通用户傻瓜攻略</font></h3><blockquote>
<p>普通用户不用管上述搭建过程，有兴趣的可以研究下，没兴趣知道怎么使用就足够了。</p>
<p><img src="http://p09u6sy9g.bkt.clouddn.com/1516067579674.png" alt=""><br>首先，访问。要在线写文章并保存以待后来回顾，写博客发布出去给别人看，得知道在哪里写，先访问我搭建的<a href="http://lovepanda.tk:9000/" target="_blank" rel="external"><font color="#AAAAAA">私人leanote</font></a>的网页；</p>
<p><img src="http://p09u6sy9g.bkt.clouddn.com/1516092135404.png" alt=""></p>
<p>然后，登陆。你需要一个账号密码来登陆站点使用各种功能，我已经关闭了注册权限，新用户只能沟通后由我后台手动建立账号。因为服务器存储计算网络资源有限，这个服务也不是盈利性质，就是个使用方便小范围共享的利器，所以使用者那必须是有限制的。</p>
<p><img src="http://p09u6sy9g.bkt.clouddn.com/toblog.gif" alt=""></p>
<p>最后，编辑。登陆成功就可以为所欲为了，具体功能看本文中间段落<strong>效果演示</strong>。所有编写的文章和markdown（后缀为.md的博客文件）保存在我的服务器上，每天备份，保留两周的备份集，所以就可劲作吧。</p>
<p>最最后，可以写什么？</p>
<p>什么都可以写，在线写的文章和博客如果不发布，没有任何人可以看到，包括管理员，所有文章加密后保存在数据库；<br>可以发布出去，比如<a href="http://lovepanda.tk:9000/blog/post/alxinfff/Leanote" target="_blank" rel="external"><font color="#AAAAAA">这样</font></a>，发布的内容所有人都可以通过网络访问。</p>
<p>你可以写一些技术文档分类保存不发布，当作一个在线笔记存储，可以写一些游戏攻略经验发布出去装逼，可以写一些抒情的文章然后把链接给别人看（说真的，这个有点骚包，我绝逼干不出来这种事），基本功能就是这些，具体的骚操作自己开发，enjoy it！！！</p>
</blockquote>
<h3 id="2-深度用户高级攻略"><a href="#2-深度用户高级攻略" class="headerlink" title="2.深度用户高级攻略"></a><font color="red">2.深度用户高级攻略</font></h3>]]></content>
      
        <categories>
            
            <category> Software </category>
            
        </categories>
        
        
        <tags>
            
            <tag> service </tag>
            
            <tag> blog </tag>
            
            <tag> community </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[HBase入门]]></title>
      <url>/2018/01/10/HBase%E5%85%A5%E9%97%A8/</url>
      <content type="html"><![CDATA[<h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a><font color="#5CACEE">简介</font></h1><blockquote>
<p>HBase – Hadoop Database，是一个高可靠性、高性能、面向列、可伸缩的分布式存储系统，利用HBase技术可在廉价 PC Server 上搭建起大规模结构化存储集群。</p>
<p>面向可扩展性的<strong>分布式数据库</strong>，如<code>hbase</code>，与面向高性能并发读写的<strong>key-value</strong>数据库，如<code>Redis</code>，面向海量数据访问的<strong>文档数据库</strong>，如<code>MongoDB</code>共同构成的应用场景，是用于解决在当前互联网发展阶段产生的巨量数据分布式处理，大量非结构化数据处理，各种社交平台日常生活中产生的的文本信息处理的重要解决方案。</p>
</blockquote>
<a id="more"></a>
<h1 id="Hbase是什么"><a href="#Hbase是什么" class="headerlink" title=" Hbase是什么 "></a><font color="#5CACEE"> Hbase是什么 </font></h1><p>HBase 是 Google Bigtable 的开源实现，类似 Google Bigtable 利用 GFS  作为其文件存储系统，HBase 利用 Hadoop HDFS 作为其文件存储系统；Google运行 MapReduce 来处理 Bigtable 中的海量数据，HBase 利用 Hadoop MapReduce 来处理 HBase 中的海量数据；Google Bigtable 利用 Chubby 作为协同服务，HBase 利用 ookeeper 作为对应。</p>
<p>HBase 是一个分布式的、面向列的开源数据库,是 Apache 的 Hadoop 项目的子项目。HBase 不同于一般的关系数据库，它是一个适合于非结构化数据存储的数据库,另一个不同是 HBase 基于列而不是基于行的模式。</p>
<p><img src="http://p09u6sy9g.bkt.clouddn.com/1515029250857.png" alt=""></p>
<p>如图,HBase 位于结构化存储层，Hadoop HDFS 为 HBase  提供了高可靠性的底层存储支持，Hadoop MapReduc 为 HBase 提供了高性能的计算能力，Zookeeper 为 HBase 提供了稳定服务和 failover 机制。</p>
<p>Pig 和 Hive 还为 HBase 提供了高层语言支持，使得在 HBase 上进行数据统计处理变的非常简单。 Sqoop 则为 HBase 提供了方便的 RDBMS 数据导入功能，使得传统数据库数据向 HBase 中迁移变的非常方便。</p>
<p><strong>spark的构成</strong></p>
<p><img src="http://p09u6sy9g.bkt.clouddn.com/1516938659693.png" alt=""></p>
<h1 id="如何访问Hbase"><a href="#如何访问Hbase" class="headerlink" title=" 如何访问Hbase "></a><font color="#5CACEE"> 如何访问Hbase </font></h1><pre><code>1. Native Java API，最常规和高效的访问方式，适合Hadoop MapReduce Job并行批处理HBase表数据
2. HBase Shell，HBase的命令行工具，最简单的接口，适合HBase管理使用
3. Thrift Gateway，利用Thrift序列化技术，支持C++，PHP，Python等多种语言，适合其他异构系统在线访问HBase表数据
4. REST Gateway，支持REST 风格的Http API访问HBase, 解除了语言限制
5. Pig，可以使用Pig Latin流式编程语言来操作HBase中的数据，和Hive类似，本质最终也是编译成MapReduce Job来处理HBase表数据，适合做数据统计
6. Hive，当前Hive的Release版本尚没有加入对HBase的支持，但在下一个版本Hive 0.7.0中将会支持HBase，可以使用类似SQL语言来访问HBase
</code></pre><h1 id="Hbase数据模型"><a href="#Hbase数据模型" class="headerlink" title=" Hbase数据模型 "></a><font color="#5CACEE"> Hbase数据模型 </font></h1><h2 id="HBase的特点"><a href="#HBase的特点" class="headerlink" title=" HBase的特点 "></a><font color="green"> HBase的特点 </font></h2><blockquote>
<ol>
<li>大：一个表可以有上亿行，上百万列。</li>
<li>面向列：面向列表（簇）的存储和权限控制，列（簇）独立检索。</li>
<li>稀疏：对于为空（NULL）的列，并不占用存储空间，因此，表可以设计的非常稀疏。</li>
<li>无模式：每一行都有一个可以排序的主键和任意多的列，列可以根据需要动态增加，同一张表中不同的行可以有截然不同的列。</li>
<li>数据多版本：每个单元中的数据可以有多个版本，默认情况下，版本号自动分配，版本号就是单元格插入时的时间戳。</li>
<li>数据类型单一：HBase中的数据都是字符串，没有类型。</li>
</ol>
</blockquote>
<h2 id="Table-amp-Column-Family"><a href="#Table-amp-Column-Family" class="headerlink" title=" Table &amp; Column Family "></a><font color="green"> Table &amp; Column Family </font></h2><p><img src="http://p09u6sy9g.bkt.clouddn.com/1515137523563.png" alt=""></p>
<p><img src="http://p09u6sy9g.bkt.clouddn.com/1515031598634.png" alt=""></p>
<blockquote>
<p>Row Key: 行键，Table的主键，Table中的记录按照Row Key排序<br>访问 HBase table 中的行，只有三种方式：</p>
</blockquote>
<p>1)通过单个 Row Key 访问。</p>
<p>2)通过 Row Key 的 range 全表扫描。</p>
<p>3)Row Key 可以使任意字符串（最大长度是64KB，实际应用中长度一般为 10 ~ 100bytes），在HBase 内部，Row Key 保存为字节数组。</p>
<blockquote>
<p>Timestamp: 时间戳，每次数据操作对应的时间戳，可以看作是数据的version number</p>
<p>Column Family：列簇，Table在水平方向有一个或者多个Column Family组成，一个Column Family中可以由任意多个Column组成，即Column Family支持动态扩展，无需预先定义Column的数量以及类型，所有Column均以二进制格式存储，用户需要自行进行类型转换。</p>
<p>逻辑数据模型中空白cell在物理上是不存储的，因为根本没有必要存储，因此若一个请求为要获取t8时间的contents:html，他的结果就是空。相似的，若请求为获取t9时间的anchor:my.look.ca，结果也是空。但是，如果不指明时间，将会返回最新时间的行，每个最新的都会返回</p>
</blockquote>
<h2 id="Table-amp-Region"><a href="#Table-amp-Region" class="headerlink" title=" Table &amp; Region "></a><font color="green"> Table &amp; Region </font></h2><p><img src="http://p09u6sy9g.bkt.clouddn.com/1515031712437.png" alt=""></p>
<blockquote>
<p>当Table随着记录数不断增加而变大后，会逐渐分裂成多份splits，成为regions，一个region由[startkey,endkey)表示，不同的region会被Master分配给相应的RegionServer进行管理</p>
</blockquote>
<p><img src="http://p09u6sy9g.bkt.clouddn.com/1515031843096.png" alt=""></p>
<blockquote>
<p>HBase中有两张特殊的Table，-ROOT-和.META.</p>
<p>.META.：记录了用户表的Region信息，.META.可以有多个regoin</p>
<p>-ROOT-：记录了.META.表的Region信息，-ROOT-只有一个region</p>
<p>Zookeeper中记录了-ROOT-表的location</p>
</blockquote>
<h2 id="MapReduce-on-HBase"><a href="#MapReduce-on-HBase" class="headerlink" title=" MapReduce on HBase "></a><font color="green"> MapReduce on HBase </font></h2><p><img src="http://p09u6sy9g.bkt.clouddn.com/1515032000264.png" alt=""></p>
<blockquote>
<p>在HBase系统上运行批处理运算，最方便和实用的模型依然是MapReduce，如图所示，HBase Table和Region的关系，比较类似HDFS File和Block的关系，HBase提供了配套的TableInputFormat和TableOutputFormat API，可以方便的将HBase Table作为Hadoop MapReduce的Source和Sink，对于MapReduce Job应用开发人员来说，基本不需要关注HBase系统自身的细节。</p>
</blockquote>
<h1 id="HBase系统架构"><a href="#HBase系统架构" class="headerlink" title=" HBase系统架构 "></a><font color="#5CACEE"> HBase系统架构 </font></h1><p><img src="http://p09u6sy9g.bkt.clouddn.com/1515032454254.png" alt=""></p>
<h2 id="Client"><a href="#Client" class="headerlink" title=" Client "></a><font color="green"> Client </font></h2><blockquote>
<p>HBase Client使用HBase的RPC（Remote Procedure Call – 远程过程调用）机制与HMaster和HRegionServer进行通信，对于管理类操作，Client与HMaster进行RPC；对于数据读写类操作，Client与HRegionServer进行RPC</p>
</blockquote>
<h2 id="Zookeeper"><a href="#Zookeeper" class="headerlink" title=" Zookeeper "></a><font color="green"> Zookeeper </font></h2><blockquote>
<p>Zookeeper Quorum中除了存储了-ROOT-表的地址和HMaster的地址，HRegionServer也会把自己以Ephemeral方式注册到 Zookeeper中，使得HMaster可以随时感知到各个HRegionServer的健康状态。此外，Zookeeper也避免了HMaster的 单点问题</p>
</blockquote>
<h2 id="HMaster"><a href="#HMaster" class="headerlink" title=" HMaster "></a><font color="green"> HMaster </font></h2><blockquote>
<p>HMaster没有单点问题，HBase中可以启动多个HMaster，通过Zookeeper的Master Election机制保证总有一个Master运行，HMaster在功能上主要负责Table和Region的管理工作：</p>
</blockquote>
<pre><code>1.管理用户对Table的增、删、改、查操作

2.管理HRegionServer的负载均衡，调整Region分布

3.在Region Split后，负责新Region的分配

4.在HRegionServer停机后，负责失效HRegionServer 上的Regions迁移
</code></pre><h2 id="HRegionServer"><a href="#HRegionServer" class="headerlink" title=" HRegionServer "></a><font color="green"> HRegionServer </font></h2><p><img src="http://p09u6sy9g.bkt.clouddn.com/1515033447893.png" alt=""></p>
<blockquote>
<p>HRegionServer主要负责响应用户I/O请求，向HDFS文件系统中读写数据，是HBase中最核心的模块。</p>
<p>如图，HRegionServer内部管理了一系列HRegion对象，每个HRegion对应了Table中的一个Region，HRegion中由多个HStore组成。每个HStore对应了Table中的一个Column Family的存储，可以看出每个Column Family其实就是一个集中的存储单元</p>
</blockquote>
<p><img src="http://p09u6sy9g.bkt.clouddn.com/1515048287534.png" alt=""></p>
<blockquote>
<p>HStore存储是HBase存储的核心,由两部分组成，一部分是MemStore，一部分是StoreFiles，MemStore是 Sorted Memory Buffer，用户写入的数据首先会放入MemStore，当MemStore满了以后会Flush成一个StoreFile（底层实现是HFile）， 当StoreFile文件数量增长到一定阈值，会触发Compact合并操作，将多个StoreFiles合并成一个StoreFile，合并过程中会进 行版本合并和数据删除，因此可以看出HBase其实只有增加数据，所有的更新和删除操作都是在后续的compact过程中进行的，这使得用户的写操作只要 进入内存中就可以立即返回，保证了HBase I/O的高性能。当StoreFiles Compact后，会逐步形成越来越大的StoreFile，当单个StoreFile大小超过一定阈值后，会触发Split操作，同时把当前 Region Split成2个Region，父Region会下线，新Split出的2个孩子Region会被HMaster分配到相应的HRegionServer 上，使得原先1个Region的压力得以分流到2个Region上。</p>
<p>在理解了上述HStore的基本原理后，还必须了解一下HLog的功能，因为上述的HStore在系统正常工作的前提下是没有问题的，但是在分布式 系统环境中，无法避免系统出错或者宕机，因此一旦HRegionServer意外退出，MemStore中的内存数据将会丢失，这就需要引入HLog了。 每个HRegionServer中都有一个HLog对象，HLog是一个实现Write Ahead Log的类，在每次用户操作写入MemStore的同时，也会写一份数据到HLog文件中（HLog文件格式见后续），HLog文件定期会滚动出新的，并 删除旧的文件（已持久化到StoreFile中的数据）。当HRegionServer意外终止后，HMaster会通过Zookeeper感知 到，HMaster首先会处理遗留的 HLog文件，将其中不同Region的Log数据进行拆分，分别放到相应region的目录下，然后再将失效的region重新分配，领取 到这些region的HRegionServer在Load Region的过程中，会发现有历史HLog需要处理，因此会Replay HLog中的数据到MemStore中，然后flush到StoreFiles，完成数据恢复。</p>
</blockquote>
<h2 id="HBase存储格式"><a href="#HBase存储格式" class="headerlink" title=" HBase存储格式 "></a><font color="green"> HBase存储格式 </font></h2><blockquote>
<p>HBase中的所有数据文件都存储在Hadoop HDFS文件系统上，主要包括上述提出的两种文件类型：</p>
</blockquote>
<pre><code>1.HFile， HBase中KeyValue数据的存储格式，HFile是Hadoop的二进制格式文件，实际上StoreFile就是对HFile做了轻量级包装，即StoreFile底层就是HFile

2.HLog File，HBase中WAL（Write Ahead Log） 的存储格式，物理上是Hadoop的Sequence File
</code></pre><h3 id="HFile"><a href="#HFile" class="headerlink" title=" HFile "></a><font color="red"> HFile </font></h3><p><img src="http://p09u6sy9g.bkt.clouddn.com/1515048543469.png" alt=""></p>
<blockquote>
<p>如图HFile的存储格式，首先HFile是不定长的，长度固定的只有其中两块:Trailer和Fileinfo。</p>
</blockquote>
<pre><code>Trailer中有指针指向其他数据块的起始点；
Fileinfo则记录了文件的部分meta信息，如AVG_KEY_LEN, AVG_VALUE_LEN, LAST_KEY, COMPARATOR, MAX_SEQ_ID_KEY等;
Data Index和Meta Index块记录了每个Data块和Meta块的起始点。
</code></pre><blockquote>
<p>Data Block是HBase I/O的基本单元，为了提高效率，HRegionServer中有基于LRU的Block Cache机制。每个Data块的大小可以在创建一个Table的时候通过参数指定，大号的Block有利于顺序Scan，小号Block利于随机查询。 每个Data块除了开头的Magic以外就是一个个KeyValue对拼接而成, Magic内容就是一些随机数字，目的是防止数据损坏。后面会详细介绍每个KeyValue对的内部构造。</p>
</blockquote>
<pre><code>LRU:Least Recently Used ,内存管理的一种页面置换算法，对于在内存中但又不用的数据块（内存块）叫做LRU，操作系统会根据哪些数据属于LRU而将其移出内存而腾出空间来加载另外的数据。
</code></pre><p><img src="http://p09u6sy9g.bkt.clouddn.com/1515049020714.png" alt=""></p>
<blockquote>
<p>HFile里面的每个KeyValue对就是一个简单的byte数组。但是这个byte数组里面包含了很多项，并且有固定的结构。具体结构如图:开始时两个固定长度的数值，分别表示key的长度和value的长度，紧接着是key，开始是固定长度的数值，表示rowkey的长度，紧接着是rowkey，然后是固定长度的数值，表示Family的长度，然后是Family，接着是Qualifier，然后是两个固定长度的数值，表示Time Stamp和Key Type（Put/Delete）。Value部分没有这么复杂的结构，就是纯粹的二进制数据。</p>
</blockquote>
<h3 id="HLogFile"><a href="#HLogFile" class="headerlink" title=" HLogFile "></a><font color="red"> HLogFile </font></h3><p><img src="http://p09u6sy9g.bkt.clouddn.com/1515063082942.png" alt=""></p>
<blockquote>
<p>如图是HLog文件的结构，其实HLog文件就是一个普通的Hadoop Sequence File，Sequence File 的Key是HLogKey对象，HLogKey中记录了写入数据的归属信息，除了table和region名字外，同时还包括 sequence number和timestamp，timestamp是“写入时间”，sequence number的起始值为0，或者是最近一次存入文件系统中sequence number。</p>
<p>HLog Sequece File的Value是HBase的KeyValue对象，即对应HFile中的KeyValue</p>
</blockquote>
<h1 id="HBase的高并发和实时处理数据"><a href="#HBase的高并发和实时处理数据" class="headerlink" title="HBase的高并发和实时处理数据"></a><font color="#5CACEE">HBase的高并发和实时处理数据</font></h1><blockquote>
<p>Hadoop是一个高容错、高延时的分布式文件系统和高并发的批处理系统，不适用于提供实时计算；HBase是可以提供实时计算的分布式数据库，数据被保存在HDFS分布式文件系统上，由HDFS保证其高容错性。</p>
<p>在生产环境中，HBase是如何基于hadoop提供实时性呢？</p>
<p> HBase上的数据是以StoreFile(HFile)二进制流的形式存储在HDFS上block块儿中；但是HDFS并不知道的HBase存的是什么，它只把存储文件视为二进制文件，也就是说，HBase的存储数据对于HDFS文件系统是透明的。</p>
<p> 从根本上说，HBase能提供实时计算服务主要原因是由其架构和底层的数据结构决定的，即由LSM-Tree + HTable(region分区) + Cache决定——客户端可以直接定位到要查数据所在的HRegion server服务器，然后直接在服务器的一个region上查找要匹配的数据，并且这些数据部分是经过cache缓存的。</p>
<p> 具体数据访问流程如下：</p>
<pre><code>1. Client会通过内部缓存的相关的-ROOT-中的信息和.META.中的信息直接连接与请求数据匹配的HRegion server；
2. 然后直接定位到该服务器上与客户请求对应的Region，客户请求首先会查询该Region在内存中的缓存——Memstore(Memstore是一个按key排序的树形结构的缓冲区)；
3. 如果在Memstore中查到结果则直接将结果返回给Client；
4. 在Memstore中没有查到匹配的数据，接下来会读已持久化的StoreFile文件中的数据，StoreFile也是按 key排序的树形结构的文件——并且是特别为范围查询或block查询优化过的，HBase读取磁盘文件是按其基本I/O单元(即 HBase Block)读数据的。
</code></pre></blockquote>
<p>hbase的具体配置使用跟hadoop，spark有密切关系，后面会有专门的博客来介绍openstack和hadoop，包括原理和配置，那里面会包含hbase的安装配置使用方法，这个坑交给后面的文章来填，就这么愉快得决定了！</p>
]]></content>
      
        <categories>
            
            <category> HBase </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Database </tag>
            
            <tag> Original </tag>
            
            <tag> Part-transported </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Google三大论文]]></title>
      <url>/2018/01/03/Google%E4%B8%89%E5%A4%A7%E8%AE%BA%E6%96%87/</url>
      <content type="html"><![CDATA[<h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a><font color="#5CACEE">简介</font></h1><blockquote>
<p>Google File System、MapReuce以及Bigtable三驾马车可以说是大数据算法的起源，虽然Google没有公布这三个产品的源码，但是他发布了这三个产品的详细设计论文，奠定了风靡全球的大数据算法的基础！</p>
</blockquote>
<table>
<thead>
<tr>
<th>软件</th>
<th>下载地址</th>
</tr>
</thead>
<tbody>
<tr>
<td>Google云计算三大论文英文版</td>
<td><a href="https://pan.baidu.com/s/1dFOyXOX" target="_blank" rel="external"><font color="#AAAAAA">点击下载</font></a></td>
</tr>
<tr>
<td>Google-File-System中文版</td>
<td><a href="https://pan.baidu.com/s/1eShdCKa" target="_blank" rel="external"><font color="#AAAAAA">点击下载</font></a></td>
</tr>
<tr>
<td>Google-MapReduce中文版</td>
<td><a href="https://pan.baidu.com/s/1jIOoRaA" target="_blank" rel="external"><font color="#AAAAAA">点击下载</font></a></td>
</tr>
<tr>
<td>Google-Bigtable中文版</td>
<td><a href="https://pan.baidu.com/s/1geX8jLL" target="_blank" rel="external"><font color="#AAAAAA">点击下载</font></a></td>
</tr>
</tbody>
</table>
<a id="more"></a>
<h2 id="那些年google发过的论文"><a href="#那些年google发过的论文" class="headerlink" title=" 那些年google发过的论文 "></a><font color="green"> 那些年google发过的论文 </font></h2><blockquote>
<p>1.按时间算第一篇的论文应该2003年公布的<strong> Google File System</strong>，这是一个可扩展的分布式文件系统，用于大型的、分布式的、对大量数据进行访问的应用。文件被分割成很多块，使用冗余的方式储存于商用机器集群上(基本上Google每篇论文都是关于“商用机型”)。</p>
<p>2.2004年发布的的<strong> MapReduce</strong>现在基本上可以代表大数据。主要思想是将任务分解然后在多台处理能力较弱的计算节点中同时处理，将结果合并从而完成大数据处理，传说中Google使用它计算他们的搜索索引。Mikio L. Braun(柏林工业大学机器学习学博士后，TWIMPACT联合创始人兼首席数据科学家)认为其工作模式应该是：Google把所有抓取的页面都放置于他们的集群上，并且每天都使用MapReduce来重算。</p>
<p>3.<strong>Bigtable</strong>发布于2006年，启发了无数的NoSQL数据库，比如：Cassandra、HBase等等。Cassandra架构中有一半是模仿Bigtable，包括了数据模型、SSTables以及提前写日志（另一半是模仿Amazon的Dynamo数据库，使用点对点集群模式）。</p>
<p>Google并没有止步于MapReduce，事实上，随着Internet的指数增长，从零开始重算所有搜索索引变得不切实际,他们在MapReduce不适用的地方开发新方法,对于大数据领域来说这是个福音。MapReduce不是万能的。当然，你可以更深入一步，比如说将磁盘数据移入内存，然而同样还存在一些任务的内部结构并不是MapReduce可以扩展的。</p>
<p>2010年发表的 <strong>Percolator</strong>的论文中，Google展示了其网络搜索是如何保持着与时俱进。Percolator建立于已存类似Bigtable的技术，但是加入了事务以及行和表上的锁和表变化的通知。这些通知之后会被用于触发不同阶段的计算。通过这样的方式，个体的更新就可以“渗透”整个数据库。</p>
<p>在2010年，Google还公布了 <strong>Dremel</strong>论文。一个为结构化数据设计，并拥有类SQL语言的交互式数据库。然而取代SQL数据库使用字段填补的表格，Dremel中使用的是类JSON格式数据（更准确的说，使用Google Protocol buffer格式，这将加强对允许字段的限制）。内部，数据被使用特殊格式储存，可以让数据扫描工作来的更高效。查询被送往服务器，而优秀的格式可以最大性能的输出结果</p>
<p>Google还需要挖掘图数据，比如在线社交网络的社交图谱；所以他们开发了 <strong>Pregel</strong>，并在2010年公布其论文。论文陈述了许多算法的实现，比如Google的PageRank、最短路径、二分图匹配等。Mikio L. Braun认为，对比MapReduce或SPF，Pregel需要更多实现的再思考。</p>
<p>Google在2009年提出了Spanner远景计划，并在2012年对外公布<strong>Spanner–全球分布式数据库</strong>论文。Spanner的公布可以说是Google向大数据技术中添的又一把火，Spanner具有高扩展性、多版本、全球级分布以及同步复制等特性，跨数据中心的高扩展性及全球分布会对一致性保障提出苛刻的需求,读写的外部一致性和基于时间戳的全局读一致性。为了保障这一点，Google引入了TrueTime API。TureTime API可以同步全球的时间，拥有一个TT.now（）的方法，将获得一个绝对时间，同时还能得到时间误差。为了保证万无一失，TrueTime API具有GPS和原子钟双保险。也只有这样的机制才能让全球范围内的并发处理得到保障。</p>
<p>在Google思路以及论文的启发下，同样涌现出一些开源项目，比如：Apache Drill、Apache Giraph、斯坦福GPS等等。</p>
<p>Google近年来每篇论文都有着深远的影响，同时大数据领域内有很多人必然在翘首以盼Google的下一篇论文。</p>
</blockquote>
<h2 id="Google-File-System-2003年"><a href="#Google-File-System-2003年" class="headerlink" title=" Google-File-System(2003年) "></a><font color="green"> Google-File-System(2003年) </font></h2><pre><code>文件被分割成很多块，使用冗余的方式储存于商用机器集群上
</code></pre><p><img src="http://p09u6sy9g.bkt.clouddn.com/1514950151670.png" alt=""></p>
<h2 id="Google-MapReduce-2004年"><a href="#Google-MapReduce-2004年" class="headerlink" title=" Google-MapReduce (2004年)"></a><font color="green"> Google-MapReduce (2004年)</font></h2><pre><code>Mapreduce是针对分布式并行计算的一套编程模型
</code></pre><p><img src="http://p09u6sy9g.bkt.clouddn.com/1514950324076.png" alt=""><br><img src="http://p09u6sy9g.bkt.clouddn.com/1514950356709.png" alt=""></p>
<h2 id="Google-Bigtable-2006年"><a href="#Google-Bigtable-2006年" class="headerlink" title=" Google-Bigtable(2006年)"></a><font color="green"> Google-Bigtable(2006年)</font></h2><pre><code>Bigtable发布于2006年，启发了无数的NoSQL数据库，比如：Cassandra、HBase等等
</code></pre><p><img src="http://p09u6sy9g.bkt.clouddn.com/1514950769173.png" alt=""></p>
<pre><code>为了管理巨大的Table，把Table根据行分割，这些分割后的数据统称为：Tablets。每个Tablets大概有 100-200 MB，每个机器存储100个左右的 Tablets。底层的架构是：GFS。

由于GFS是一种分布式的文件系统，采用Tablets的机制后，可以获得很好的负载均衡。比如：可以把经常响应的表移动到其他空闲机器上，然后快速重建。
</code></pre><h2 id="总结"><a href="#总结" class="headerlink" title=" 总结 "></a><font color="green"> 总结 </font></h2><p><img src="http://p09u6sy9g.bkt.clouddn.com/1514950224224.png" alt=""></p>
<p><strong><em>MapReduce 和 BigTable都是以GFS为基础，三大基础核心技术构建出了完整的分布式运算架构。</em></strong></p>
<p>大数据的出现是互联网技术发展的<strong>大势所趋</strong>，随着越来越多的智能化数字化应用，社交媒体信息爆炸，超级大公司的业务数据不断膨胀，需要从海量数据中挖掘发现有价值的信息来进行商务决策，企业管理，产品调整，支撑各种互联网＋的公司，传统的高性能服务器加oracle等结构化数据库方案已经不足以满足需求。</p>
<p>谷歌本身就是一个体量巨大的全球性科技公司，旗下youtube，twitch，twitter，搜索业务，承载着互联网数据总量相当比重的<strong>处理压力</strong>，主动或者被动都要面对这个问题，从无到有地建立了一套技术体系之后，没有依靠技术垄断获取更大的利益，而是<strong>拿出了一整套解决方案的理论基础</strong>，可以说当前的这些分布式系统，框架，非结构化数据库，大部分都基于此，谷歌为互联网进入下个时代做出了极大的贡献，再次膜拜。</p>
<p>这是基于当前可观测事实的合理推测，事实上谷歌作为资本控制下的科技公司，做出这种大公无私的事可能性有但很小，大概率有自己的<strong>目的</strong>在里面，比如已经有了其他的解决方案，或许是另一个技术方向，用这些论文误导业界的发展方向，结果发现业界依然如火如荼┑(￣Д ￣)┍；比如是技术上又有了突破，把淘汰下来的技术拿出去给你们这些战五渣用(╯‵□′)╯︵┻━┻;又或者是有什么别的考量，遇上了瓶颈，把这些拿出来准备接受业界的反哺啦，跟某些团体有什么不可描述的py交易啦。这些都无所谓，论迹不论心，论心世上无完人，不管当时抱着什么目的，客观上确实极大地促进了大数据这个领域的发展，我宁愿相信谷歌出于公心出于科技界一员的责任感出于促进人类科技发展，主动自愿地发起了推动大数据领域前进的一系列行为，<em>科技宅天下第一！！！</em></p>
<p>最后说一句，估计有巨多的人都听说过谷歌的三篇传奇论文推进一个领域的故事，但是看过论文并仔细研究过的没多少，毕竟术业有专攻，该领域的从业人员限定就能划掉大部分人，领域内分工不同，也不是人人都需要看这些，又划掉一部分，比如我就没看过，写这篇博客是因为最近又开始玩hbase和mongo，查资料越查越深，决定单独把这三篇奠基性的论文拿出来过一遍，顺便mark一下。</p>
<p>最最后说一句，论文是最为精确信息量最大的，但这种专业度极高的文献看起来超级麻烦，所以我推荐b站的一个教学视频<a href="https://www.bilibili.com/video/av9787020/" target="_blank" rel="external"><font color="#AAAAAA">【大数据系统基础】.MOOC.清华大学</font></a>，可以说把GFS和MapReduce讲的相当透彻。</p>
<blockquote>
<p>PS:b站超良心的，up主把视频搬过来排版调好放在那没有广告免费观看，还可以选择2倍速，资源贼多，真真的“我在b站看纪录片”。</p>
</blockquote>
<p><img src="http://p09u6sy9g.bkt.clouddn.com/1514959732808.png" alt=""></p>
<p><img src="http://p09u6sy9g.bkt.clouddn.com/1514959759871.png" alt=""></p>
]]></content>
      
        <categories>
            
            <category> Bigdata </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Theory </tag>
            
            <tag> Paper </tag>
            
            <tag> Google </tag>
            
            <tag> Transported </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[安装配置]]></title>
      <url>/2018/01/02/Oracle%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE/</url>
      <content type="html"><![CDATA[<h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a><font color="#5CACEE">简介</font></h1><blockquote>
<p>Oralce体系结构主要可以分为三个部分，对其分别进行深入理解可以很好得帮助理解oracle软件的运行和结构</p>
<ul>
<li>内存结构</li>
<li>进程结构</li>
<li>物理存储结构</li>
</ul>
</blockquote>
<a id="more"></a>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      
<h2 id="一级目录"><a href="#一级目录" class="headerlink" title="一级目录"></a><font color="green">一级目录</font></h2><p>###<font color="red">二级目录</font>###</p>
<h4 id="三级目录"><a href="#三级目录" class="headerlink" title="三级目录"></a>三级目录</h4>]]></content>
      
        <categories>
            
            <category> Oracle </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Database </tag>
            
            <tag> Original </tag>
            
            <tag> Part-transported </tag>
            
            <tag> Operate </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[oracle大纲]]></title>
      <url>/2017/12/29/Oracle%E5%A4%A7%E7%BA%B2/</url>
      <content type="html"><![CDATA[<h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a><font color="#5CACEE">简介</font></h1><blockquote>
<p>Oracle Database，又名Oracle RDBMS，或简称Oracle。是甲骨文公司的一款关系数据库管理系统。它是在数据库领域一直处于领先地位的产品。可以说Oracle数据库系统是目前世界上流行的关系数据库管理系统，系统可移植性好、使用方便、功能强，适用于各类大、中、小、微机环境。它是一种高效率、可靠性好的 适应高吞吐量的数据库解决方案。</p>
</blockquote>
<a id="more"></a>
<h1 id="oracle的起源"><a href="#oracle的起源" class="headerlink" title="oracle的起源"></a><font color="#5CACEE">oracle的起源</font></h1><p>oracle是<a href="https://baike.baidu.com/item/%E7%94%B2%E9%AA%A8%E6%96%87%E5%85%AC%E5%8F%B8/430115?fr=aladdin" target="_blank" rel="external"><font color="#AAAAAA">甲骨文公司</font></a>的拳头产品，是一个功能完善，商业应用程度极高的关系型数据库软件，是计算机行业发展的阶段性产物。</p>
<h1 id="数据库是什么"><a href="#数据库是什么" class="headerlink" title="数据库是什么"></a><font color="#5CACEE">数据库是什么</font></h1><p>数据库是按照数据结构组织，存储和管理数据的仓库，可视为电子化的文件柜，用户可以对文件中的数据进行新增、截取、更新、删除等操作。</p>
<p>它将数据以一定方式储存在一起、能为多个用户共享、具有尽可能小的冗余度，是与应用程序彼此独立的数据集合。</p>
<h2 id="数据库中数据的存储形式"><a href="#数据库中数据的存储形式" class="headerlink" title="数据库中数据的存储形式"></a><font color="#DDA0DD">数据库中数据的存储形式</font></h2><p>数据模型是数据库中数据的存储方式，是数据库系统的基础</p>
<p><strong>数据模型</strong>经历了：</p>
<blockquote>
<p>1.层次模型：层次模型发展最早，它以树结构为基本结构，典型代表是IMS模型。由于多数实际问题中数据间关系不简单地是树型结构，层次型数据模型渐被淘汰。</p>
<p>2.网状模型：网状数据模型通过网状结构表示数据间联系，开发较早且有一定优点，目前使用仍较多，典型代表是 DBTG模型。</p>
<p>3.关系型：关系模型开发较晚，它是通过满足一定条件的二维表格来表示实体集合以及数据间联系的一种模型，具有坚实的数学基础与理论基础，使用灵活方便，适应面广，发展十分迅速</p>
<p>4.非关系型 ： NoSQL( Not Only SQL )，用于指代那些非关系型的，分布式的，且一般不保证遵循ACID原则的数据存储系统。</p>
</blockquote>
<p><strong>关系型数据库</strong>是指采用了关系模型来组织数据的数据库，如<code>SQLite,Oracle,Mysql,SQLserver</code>，最大特点就是事务的一致性（ACID），简单来说，关系模型指的就是二维表格模型，关系型数据库就是由二维表及其之间的联系所组成的一个数据组织。</p>
<blockquote>
<p>优点:</p>
<p>1、容易理解：二维表结构是非常贴近逻辑世界一个概念，关系模型相对网状、层次等其他模型来说更容易理解；</p>
<p>2、使用方便：通用的SQL语言使得操作关系型数据库非常方便；</p>
<p>3、易于维护：丰富的完整性(实体完整性、参照完整性和用户定义的完整性)大大减低了数据冗余和数据不一致的概率；</p>
<p>4、支持SQL，可用于复杂的查询。</p>
<p>缺点:</p>
<p>1、为了维护一致性所付出的巨大代价就是其读写性能比较差；</p>
<p>2、固定的表结构；</p>
<p>3、高并发读写需求；</p>
<p>4、海量数据的高效率读写；</p>
</blockquote>
<p><strong>非关系型数据库</strong>提出了另一种理念，随之产生的面向高性能并发读写的<strong>key-value</strong>数据库，如<code>Redis,Tokyo Cabinet,Flare</code> ，面向海量数据访问的<strong>面向文档数据库</strong>，如<code>MongoDB以及CouchDB</code>，面向可扩展性的<strong>分布式数据库</strong>,如<code>hadoop的hbase</code>，用于解决在当前互联网发展阶段产生的巨量数据分布式处理，大量非结构化数据处理，各种社交平台日常生活中产生的的文本信息处理。</p>
<blockquote>
<p>优点：</p>
<p>1）成本：nosql数据库简单易部署，基本都是开源软件，不需要像使用oracle那样花费大量成本购买使用，相比关系型数据库价格便宜。</p>
<p>2）查询速度：nosql数据库将数据存储于缓存之中，关系型数据库将数据存储在硬盘中，自然查询速度远不及nosql数据库。</p>
<p>3）存储数据的格式：nosql的存储格式是key,value形式、文档形式、图片形式等等，所以可以存储基础类型以及对象或者是集合等各种格式，而数据库则只支持基础类型。</p>
<p>4）扩展性：关系型数据库有类似join这样的多表查询机制的限制导致扩展很艰难。</p>
<p>缺点：</p>
<p>1）维护的工具和资料有限，因为nosql是属于新的技术，不能和关系型数据库10几年的技术同日而语。</p>
<p>2）不提供对sql的支持，如果不支持sql这样的工业标准，将产生一定用户的学习和使用成本。</p>
<p>3）不提供关系型数据库对事物的处理。</p>
</blockquote>
<h2 id="数据库技术的发展"><a href="#数据库技术的发展" class="headerlink" title="数据库技术的发展"></a><font color="#DDA0DD">数据库技术的发展</font></h2><p>未来不确定，但是可预测，基于事物发展的规律进行合理推测，更大的信息量带来的是更精确的预测结果。</p>
<p>数据库就是用来存储数据的一个容器，信息时代万物皆数据，当前科技的热门发展方向，智能汽车，无人机，机器学习，人工智能，物联网等等，都是基于对描述物质世界的巨量数据进行处理后得到期望的结果</p>
<blockquote>
<p>人工智能和深度学习不管是分类聚类算法，决策树还是神经网络，监督学习还是非监督学习，都要有经过预处理干净的数据集；物联网存在的基础就是存在于所有要监控和可监控的硬件中的嵌入式集成芯片，对这些芯片进行数据采集和操作来实现物联网控制一切的目的；智能汽车无人机这种基于数字技术和传感器的技术，最基础的技术很大部分都是对车辆行为数据集的学习和对行驶状态中数据的快速处理算法。</p>
</blockquote>
<p>数据是根本，对数据安全一致的存放管理和以合理形式进行组织应用的需求将会是永久的课题，不管将来信息科技如何发展，数据组织形式如何革新，数据库这一类对数据进行管理的软件永远有一席之地，而且必将是信息时代基石般的存在。</p>
<p>oracle作为关系型数据库领域登峰造极的产品，安全性稳定性可扩展性都已经高度成熟，从1977年IBM提出“关系数据库”的论文，埃里森以此做出oracle产品，并在之后的不断发展，到2013年甲骨文已经超越IBM,成为<strong>继MICORSOFT后全球第二大软件公司</strong>。oracle的各代产品在全世界各个行业，电信，电力，金融，政府及大量制造业扮演着很重要的角色，互联网总数据量的不断增长，现在的发展趋势是大数据处理，对非关系型，文本数据的处理需求使nosql类数据库和分布式处理框架越来越火，发展势头迅猛，然而在传统电信电力金融政府等领域，要处理的还是高度关系化，稳定性安全性需求极高的数据，oracle作为业界大佬，即使有了强力的挑战者，基本盘依然巨大，从业人员的需求短时间内也是不会衰退太多，而且作为如此成熟的一个产品，它的设计思路，工作方式都是值得深入学习，SQL和PL/SQL如此朴实通用的数据操作语言也是必须掌握。</p>
<p>从软件本身，从业，学习其他数据技术等各个角度来说，学习oracle都是一件拓宽眼界有益身心的事情。</p>
<h2 id="oracle知识谱系"><a href="#oracle知识谱系" class="headerlink" title="oracle知识谱系"></a><font color="#DDA0DD">oracle知识谱系</font></h2><p><img src="http://p09u6sy9g.bkt.clouddn.com/1513844572016.png" alt=""></p>
<p>上图知识谱系的内容基本能够覆盖诠释oracle的产品特性和使用，我会按照图中的思路来介绍oracle的相关技术，本文章是大纲，是oracle技能树的框架，后面我会每个分枝都用一篇博客来介绍。</p>
<p>(～￣(OO)￣)ブ，本来处于兴趣搭了个博客，又觉得空荡荡的写点东西好了，然后发现想到哪写到哪的写作方式效率太低，就大概规划了一下。</p>
<p>初步目标是由系列文章构成的oracle技能树，mysql技能树，mongodb技能树，python的一些相关内容，然后是研究生课程学的大数据相关内容，商务智能一篇，算法一片，数据仓库一篇，数据挖掘一篇，再加上自学的hadoop，spark这些，貌似不知不觉给自己挖了个巨坑啊（微笑挥手），加油填坑，跟妹子吹的牛跪着也要圆回来ORZ~~~</p>
<h3 id="体系结构（√）"><a href="#体系结构（√）" class="headerlink" title="体系结构（√）"></a><a href="https://uxtuo.github.io/2017/12/28/%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/#more" target="_blank" rel="external"><font>体系结构（√）</font></a></h3><blockquote>
<p>内存结构</p>
<p>进程结构</p>
<p>物理存储结构</p>
</blockquote>
<h3 id="安装配置"><a href="#安装配置" class="headerlink" title="安装配置"></a><font>安装配置</font></h3><h3 id="SQL和PL-SQL"><a href="#SQL和PL-SQL" class="headerlink" title="SQL和PL/SQL"></a><font>SQL和PL/SQL</font></h3><h3 id="数据加载"><a href="#数据加载" class="headerlink" title="数据加载"></a><font>数据加载</font></h3><h3 id="性能调优"><a href="#性能调优" class="headerlink" title="性能调优"></a><font>性能调优</font></h3><h3 id="数据迁移和备份恢复"><a href="#数据迁移和备份恢复" class="headerlink" title="数据迁移和备份恢复"></a><font>数据迁移和备份恢复</font></h3><h3 id="安全加固和故障处理"><a href="#安全加固和故障处理" class="headerlink" title="安全加固和故障处理"></a><font>安全加固和故障处理</font></h3>]]></content>
      
        <categories>
            
            <category> Oracle </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Database </tag>
            
            <tag> Original </tag>
            
            <tag> Part-transported </tag>
            
            <tag> Framework </tag>
            
            <tag> theory </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[体系结构]]></title>
      <url>/2017/12/28/Oracle%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/</url>
      <content type="html"><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a><font color="#5CACEE">简介</font></h2><blockquote>
<p>Oralce体系结构主要可以分为三个部分，对其分别进行深入理解可以很好得帮助理解oracle软件的运行和结构</p>
<ul>
<li>内存结构</li>
<li>进程结构</li>
<li>物理存储结构</li>
</ul>
<p>Oracle服务器由数据库实例和数据库文件构成</p>
<p>数据库 = 数据文件 + 控制文件 + 日志文件</p>
<p>实例 = 内存池 + 后台进程</p>
<p>Oracle实例就是由一些内存区和后台进程构成的一个逻辑概念。<br>要访问数据库，先启动实例,分配内存区，然后启动后台进程</p>
</blockquote>
<a id="more"></a>
<h2 id="内存结构-SGA-PGA"><a href="#内存结构-SGA-PGA" class="headerlink" title="内存结构 SGA+PGA"></a><font color="green">内存结构 SGA+PGA</font></h2><p><img src="http://p09u6sy9g.bkt.clouddn.com/1513927427014.png" alt=""></p>
<h3 id="SGA-–-system-global-area"><a href="#SGA-–-system-global-area" class="headerlink" title="SGA – system global area"></a><font color="red">SGA – system global area</font></h3><pre><code>由所有服务进程和后台进程共享
</code></pre><h4 id="shared-pool"><a href="#shared-pool" class="headerlink" title="shared pool"></a>shared pool</h4><pre><code>缓存了各用户间可共享的各种结构
</code></pre><blockquote>
<p><br>数据字典，执行计划<br><br>数据块相关性高，版本转换产生bug的几率更大<br><br>SQL执行计划，硬解析和软解析<br><br>语法语句检查，软解析去shared pool调用缓存的执行计划，硬解析，分析统计信息，生成执行计划执行，得到数据，缓存到buffer cache，执行计划缓存到shared pool，发送给客户，网络问题产生的等待事件<br><br>当客户端进程，将SQL语句通过监听器发送到Oracle时, 会触发一个Server process生成，来对该客户进程服务。Server process得到SQL语句之后，对SQL语句进行Hash运算，然后根据Hash值到library cache中查找，如果存在，则直接将library cache中的缓存的执行计划拿来执行，最后将执行结果返回该客户端，这种SQL解析叫做软解析；如果不存在，则会对该SQL进行解析parse，然后执行，返回结果，这种SQL解析叫做硬解析。</p>
</blockquote>
<p>硬解析的步骤：</p>
<blockquote>
<p><br>1）对SQL语句进行语法检查，看是否有语法错误。如果存在语法错误，则退出解析过程；<br><br> 2）通过数据字典(row cache)，检查SQL语句中涉及的对象和列是否存在，检查SQL语句的用户是否对涉及到的对象是否有权限。<br><br>3）通过优化器创建一个最优的执行计划。这个过程会根据数据字典中的对象的统计信息，来计算多个执行计划的cost，从而得到一个最优的执行计划。这一步涉及到大量的数据运算，从而会消耗大量的CPU资源；(library cache最主要的目的就是通过软解析来减少这个步骤)；<br><br>4）将该游标所产生的执行计划，SQL文本等装载进library cache中的heap中。</p>
</blockquote>
<p>软解析：就是因为相同文本的SQL语句存在于library cache中，所以本次SQL语句的解析就可以去掉硬解析中的一个或多个步骤，从而节省大量的资源的耗费。<br><br>软软解析：就是不解析。</p>
<h4 id="buffer-cache"><a href="#buffer-cache" class="headerlink" title="buffer cache"></a>buffer cache</h4><pre><code>缓存了从磁盘上检索的数据块
</code></pre><p>灌数据     内存或者进程读数据，server process<br>排数据     dbwr<br>某个表经常使用，放在keep池中，防止冲出去</p>
<p>user I/O 等待事件，都是往buffer cache里灌的时候引起的，<br>system I/O 等待事件，都是往出排的时候</p>
<h4 id="redo-log-buffer"><a href="#redo-log-buffer" class="headerlink" title="redo log buffer"></a>redo log buffer</h4><pre><code>缓存了写到磁盘之前的重做信息
</code></pre><p>重做信息（用于实例恢复）在写入磁盘中存储的物理重做日志文件 之前，将缓存在此处<br>数据，undo的改变，会产生redo日志，写进logbuffer<br>触发事件后 lgwr 进程写出去，生成归档日志</p>
<h3 id="PGA-–-process-global-area"><a href="#PGA-–-process-global-area" class="headerlink" title="PGA – process global area"></a><font color="red">PGA – process global area</font></h3><pre><code>由每个服务进程、后台进程专有；每个进程都有一个PGA
</code></pre><blockquote>
<p>1.Private SQL area：包含绑定信息、运行时的内存结构。每个发出sql语句的会话，都有一个private SQL area（私有SQL区）</p>
<p>2.Session memory：为保存会话中的变量以及其他与会话相关的信息，而分配的内存区。</p>
</blockquote>
<p>pga不足的时候使用临时表空间</p>
<h3 id="SGA和PGA分配"><a href="#SGA和PGA分配" class="headerlink" title="SGA和PGA分配 "></a><font color="red">SGA和PGA分配 </font></h3><p>Oracle官方文档推荐:</p>
<pre><code>MEMORY_TARGET=物理内存 x 80%

MEMORY_MAX_SIZE=物理内存 x 80%

对于OLTP系统： 

SGA_TARGET=(物理内存 x 80%) x 80%

SGA_MAX_SIZE=(物理内存 x 80%) x 80%

PGA_AGGREGATE_TARGET=(物理内存 x 80%) x 20%

对于DSS系统：

SGA_TARGET=(物理内存 x 80%) x 50%

SGA_MAX_SIZE=(物理内存 x 80%) x 50%

PGA_AGGREGATE_TARGET=(物理内存 x 80%) x 50%
</code></pre><h2 id="进程结构"><a href="#进程结构" class="headerlink" title="进程结构"></a><font color="green">进程结构</font></h2><p>五个主要后台进程</p>
<pre><code>SQL&gt; select name,description from v$bgprocess where paddr&lt;&gt;&apos;00&apos;;
</code></pre><p><img src="http://p09u6sy9g.bkt.clouddn.com/1514537708795.png" alt=""></p>
<h3 id="SMON-–-System-monitor-系统监控进程"><a href="#SMON-–-System-monitor-系统监控进程" class="headerlink" title="SMON – System monitor 系统监控进程 "></a><font color="red">SMON – System monitor 系统监控进程 </font></h3><p><img src="http://p09u6sy9g.bkt.clouddn.com/1514538245662.png" alt=""></p>
<blockquote>
<p>SMON启动后会自动的用于在实例崩溃时进行数据库实例自动恢复。 </p>
<p>清除作废的排序临时段，回收整理碎片，合并空闲空间，释放临时段，维护闪回的时间点。 </p>
<p>在老数据库版本中，当我们大量删除表的时候，会观测到SMON进程很忙，直到把所有的碎片空间都整理完毕。</p>
</blockquote>
<h3 id="PMON-–-Process-monitor-进程监控"><a href="#PMON-–-Process-monitor-进程监控" class="headerlink" title="PMON – Process monitor 进程监控 "></a><font color="red">PMON – Process monitor 进程监控 </font></h3><p><img src="http://p09u6sy9g.bkt.clouddn.com/1514538085023.png" alt=""></p>
<blockquote>
<p>PMON在后台进程执行失败后负责清理数据库缓存和闲置资源，是Oracle的自动维护机制。</p>
<ul>
<li>清除死进程</li>
<li>重新启动部分进程（如调度进程）</li>
<li>监听的自动注册</li>
<li>回滚事务</li>
<li>释放锁</li>
<li>释放其他资源</li>
</ul>
</blockquote>
<h3 id="DBWR-数据写进程"><a href="#DBWR-数据写进程" class="headerlink" title="DBWR 数据写进程"></a><font color="red">DBWR 数据写进程</font></h3><p><img src="http://p09u6sy9g.bkt.clouddn.com/1514537996791.png" alt=""></p>
<blockquote>
<p>Server process连接Oracle后，通过数据库写进程(DBWn)将数据缓冲区中的“脏缓冲区”的数据块写入到存储结构(数据文件、磁盘文件)</p>
<p>只做一件事，将数据写到磁盘。就是将数据库的变化写入到数据文件。<br>该进程最多20 个，即使你有36 个CPU 也只能最多有20 个数据库写进程。<br>进程名称DBW0-DBW9 DBWa-DBWj </p>
</blockquote>
<h3 id="LGWR-日志写进程"><a href="#LGWR-日志写进程" class="headerlink" title="LGWR 日志写进程 "></a><font color="red">LGWR 日志写进程 </font></h3><p><img src="http://p09u6sy9g.bkt.clouddn.com/1514538315144.png" alt=""></p>
<blockquote>
<p>主要用于记录数据库的改变和记录数据库被改变之前的原始状态，所以应当对其作多重备份，用于恢复和排错。</p>
<p>激活LGWR的情况：</p>
<ul>
<li>提交指令</li>
<li>日志缓冲区超过1/3</li>
<li>每三秒</li>
<li>每次DBWn执行之前</li>
</ul>
</blockquote>
<h3 id="CKPT-校验点进程"><a href="#CKPT-校验点进程" class="headerlink" title="CKPT  校验点进程"></a><font color="red">CKPT  校验点进程</font></h3><p><img src="http://p09u6sy9g.bkt.clouddn.com/1514538043664.png" alt=""></p>
<blockquote>
<p>主要用户更新数据文件头，更新控制文件和触发DBWn数据库写进程。<br>Ckpt 进程会降低数据库性能，但是提高数据库崩溃时，自我恢复的性能。我们可以理解为阶段性的保存数据，一定的条件满足就触发，执行DBWn存盘操作。</p>
</blockquote>
<h2 id="物理结构"><a href="#物理结构" class="headerlink" title="物理结构"></a><font color="green">物理结构</font></h2><p>oracle数据库是个运行在操作系统上最终目的是存储和管理相关数据的软件</p>
<p>每一个Oracle数据库都是由三种类型的文件组成：数据文件（Data File）、日志文件（Log File）和控制文件（Control File）。数据库的文件为数据库信息提供真正的物理存储。</p>
<h3 id="一-控制文件"><a href="#一-控制文件" class="headerlink" title="一. 控制文件"></a><font color="red">一. 控制文件</font></h3><p>为二进制文件，初始化大小由CREATE DATABASE指定,可以使用RMAN备份</p>
<p>记录了当前数据库的结构信息,同时也包含数据文件及日志文件的信息以及相关的状态,归档信息等等</p>
<p>在参数文件中描述其位置，个数等等。通常采用分散放开，多路复用的原则。在mount阶段被读取，open阶段一直被使用</p>
<p>维护数据库一致性(数据库启动时会比较控制文件与联机日志文件中的ckpt,即起始scn号，如相等则正常启动，否则需要介质恢复)</p>
<p>一个控制文件只能属于一个数据库</p>
<p>控制文件的任意修改将写入到初始化参数中指定的所有控制文件中，读取时则仅读取第一个控制文件</p>
<p>控制文件只能连接一个数据库，控制文件的大小一般不要超过MB,最多为个，最少一个，互为镜像</p>
<p>控制文件中包含的内容</p>
<blockquote>
<p>数据库的名字、ID、创建的时间戳</p>
<p>表空间的名字</p>
<p>联机日志文件、数据文件的位置、个数、名字</p>
<p>联机日志的Sequence号码</p>
<p>检查点的信息</p>
<p>撤销段的开始或结束</p>
<p>归档信息</p>
<p>备份信息</p>
</blockquote>
<h4 id="一、何时创建新的控制文件"><a href="#一、何时创建新的控制文件" class="headerlink" title="一、何时创建新的控制文件"></a>一、何时创建新的控制文件</h4><blockquote>
<p>a、在控制文件发生永久性的损坏且之前未对控制文件进行备份</p>
<p>b、需要修改控制文件中的某些内容</p>
</blockquote>
<h4 id="二、多路复用控制文件"><a href="#二、多路复用控制文件" class="headerlink" title="二、多路复用控制文件"></a>二、多路复用控制文件</h4><blockquote>
<p>Oracle推荐最好将控制文件分布在不同的物理磁盘上。如果由于磁盘故障导致控制文件发生损坏，与之相关联的实例应被关闭。一旦磁盘被修复，可以通过其他磁盘上的控制文件恢复损坏的控制文件。待恢复完成后实例就可以重新启动，不需要进行介质恢复。<br>多路复用控制文件是如何进行工作的呢？<br>1、数据库启动时，仅读取control_file中第一个参数文件的信息<br>2、数据库打开时，将会向control_file中所有的控制文件更新信息<br>3、在数据库操作期间，如果任意一个控制文件不可用，实例将无法操作。</p>
</blockquote>
<h4 id="三、备份与恢复控制文件"><a href="#三、备份与恢复控制文件" class="headerlink" title="三、备份与恢复控制文件"></a>三、备份与恢复控制文件</h4><p>1、何时需要备份控制文件：</p>
<blockquote>
<p>a、添加、删除或重命名数据文件</p>
<p>b、添加、删除一个表空间或修改表空间的读写状态 //在添加，删除或重命名的前后都需要进行备份吗？文档没有说明，我认为应该这样。</p>
<p>c、添加、删除重做日志文件或组</p>
</blockquote>
<p>2、备份控制文件的方法如下：</p>
<blockquote>
<p>a、以二进制文件的形式备份控制文件：alter database backup controlfile to ‘/u02/backup’;</p>
<p>b、以SQL语句的形式备份控制文件便于以后可以重建：alter database backup controlfile to trace;该备份的存放位置可以通过查看alert告警日志获知。</p>
</blockquote>
<p>3、恢复控制文件的方法如下：</p>
<blockquote>
<p>a、假设control_file参数中的一个参数文件发生损坏，但是控制文件的存放目录仍然可以访问，这时可以：关闭数据库–&gt;cp控制文件另一副本覆盖损坏控制文件–startup</p>
<p>b、假设control_file参数中的一个参数文件发生发生介质损坏，这时可以：关闭数据库 –在新介质上恢复控制文件 –修改control_file参数 –startup<br>4、删除控制文件<br>    关闭数据库 –&gt; 修改control_file参数(删除对于信息) –&gt; startup，该系列操作不会删除操作系统上的物理文件，需要手动删除。</p>
</blockquote>
<h4 id="实验-rac-asm控制文件的多路复用"><a href="#实验-rac-asm控制文件的多路复用" class="headerlink" title="实验:rac+asm控制文件的多路复用"></a><strong>实验:rac+asm控制文件的多路复用</strong></h4><blockquote>
<p><code>SQLshow parameter control_files;</code></p>
<p><img src="http://p09u6sy9g.bkt.clouddn.com/1514449341804.png" alt=""></p>
<p><code>$ srvctl stop database -d CHAOS</code>  #关闭集群数据库</p>
<p><code>SQL&gt; startup nomount;</code> # 单节点启动到nomount状态</p>
<p>使用rman连接nomount状态的数据库并且备份控制文件到指定路径</p>
<p><code>$ rman target /</code></p>
<p><code>RMAN&gt; restore controlfile to &#39;+ORADATA/chaos/controlfile/current.260&#39; from &#39;+ORADATA/chaos/controlfile/current.260.958910433&#39;;</code></p>
<p>进入asmcmd，可以看到已经在目标路径生成备份</p>
<p><code>su - grid</code></p>
<p><code>$ export ORACLE_SID=+ASM</code></p>
<p><code>$ asmcmd</code></p>
<p><code>ASMCMD&gt; cd oradata/chaos/controlfile</code></p>
<p><code>ASMCMD&gt; ls</code></p>
<p><img src="http://p09u6sy9g.bkt.clouddn.com/1514452284051.png" alt=""></p>
<p><code>SQL&gt; alter system set control_files=&#39;+oradata/chaos/controlfile/current.270.963936137&#39;,&#39;+oradata/chaos/controlfile/current.260.958910433&#39; scope=spfile sid=&#39;*&#39;;   #自动生成了两个备份文件curren.260和current.270.963936137，用哪个都一样</code></p>
<p><code>$ srvctl start database -d CHAOS  #这个时候已经完成了对rac集群所有节点控制文件的多路复用</code></p>
<p><img src="http://p09u6sy9g.bkt.clouddn.com/1514452594076.png" alt=""></p>
</blockquote>
<h3 id="二-数据文件"><a href="#二-数据文件" class="headerlink" title="二.数据文件 "></a><font color="red">二.数据文件 </font></h3><p>每个数据库有一个或多个物理的数据文件。逻辑数据库结构（如表、索引等）的数据物理地存储在数据库的数据文件中，数据文件通常为*.dbf格式。<br>数据文件包含数据库中的实际数据，是数据库操作中数据的最终存储位置<br>数据文件有下列特征：</p>
<blockquote>
<p> 1、一个数据文件仅与一个数据库联系；</p>
<p> 2、一旦建立，数据文件只增不减；</p>
<p> 3、一个表空间（数据库存储的逻辑单位）由一个或多个数据文件组成。</p>
</blockquote>
<p> Oracle数据库在逻辑上是由多个表空间组成的，表空间在物理上包含一个或多个数据文件。而数据文件大小是块大小的整数倍；表空间中存储的对象叫段，比如数据段，索引段和回退段。段由区组成，区是磁盘分配的最小单位。段的增大是通过增加区的个数来实现的。每个区的大小是数据块大小的整数倍，区的大小可以不相同；数据块是数据库中的最小的I/O单位，同时也是内存数据缓冲区的单位，及数据文件存储空间单位。块的大小由参数DB_BLOCK_SIZE设置，其值应设置为操作系统块大小的整数倍。</p>
<p> 最后再来说一下Oracle的用户、表空间和数据文件之间的关系：<br>  一个用户可以使用一个或多个表空间，一个表空间也可以供多个用户使用。用户和表空间没有隶属关系，表空间是一个用来管理数据存储的逻辑概念，表空间只和数据文件存在关系，数据文件是物理的，一个表空间可以包含多个数据文件，而一个数据文件只能隶属一个表空间。</p>
<p>解释数据库、表空间、数据文件、表、数据的最好办法，就是<strong>想象一个装满东西的柜子，数据库其实就是柜子，柜中的抽屉是表空间，抽屉中的文件夹是数据文件，文件夹中的纸是表，写在纸上的信息就是数据</strong>。</p>
<h4 id="表空间：是一个或多个数据文件的逻辑集合"><a href="#表空间：是一个或多个数据文件的逻辑集合" class="headerlink" title="表空间：是一个或多个数据文件的逻辑集合"></a>表空间：是一个或多个数据文件的逻辑集合</h4><h4 id="表空间逻辑存储对象："><a href="#表空间逻辑存储对象：" class="headerlink" title="表空间逻辑存储对象："></a>表空间逻辑存储对象：</h4><p>   –永久段:如表与索引<br>   –临时段:如临时表数据与排序段<br>   –回滚段:用于事物回滚或闪回内存的撤销数据</p>
<h4 id="表空间分类："><a href="#表空间分类：" class="headerlink" title="表空间分类："></a>表空间分类：</h4><p>系统表空间(system、sysaux)，非系统表空间,一个表空间至少包含一个数据文件，一个数据文件只能属于一个表空间。</p>
<blockquote>
<p>   –SYSTEM :字典表空间，不能被损坏</p>
<p>   –UNDO   :dml,ddl把数据快照到此，数据提交即消失（用于恢复)</p>
<p>   –SYSAUX :10g 高并发系统繁忙时，会造成system争用，将工具放到SYSAUX,减轻system的压力，SYSAUX不影响系统（影响性能）</p>
<p>   –TEMP   :临时数据相关的内容</p>
<p>   –USERS  :10g  用户数据从system拨离出来</p>
</blockquote>
<p>逻辑结构是Oracle内部管理数据库中对象的方式<br>database数据库—&gt;tablespace表空间—&gt; segment段—&gt;extent区间—-&gt; block块</p>
<p>Schema: 用户—&gt;创建相关对象、表、视图、序列、函数、存储过程、包等</p>
<p>举例描述scott用户创建对象的组织方式,scott用户创建一张emp表，数据定义于system，数据逻辑存储于user表空间: 表段: 区间: 内存块/索引段: 区间: 内存块,user表空间<br>物理存储于user01.dbf数据文件，采用本地管理，包含头部信息，可用，已用等位图信息，buffer cache满或者commit之后dbwr进程将数据从内存写到物理文件中</p>
<pre><code>`SQL&gt; select username,default_tablespace,temporary_tablespace from dba_users where username=&apos;SCOTT&apos;;`
</code></pre><p><img src="http://p09u6sy9g.bkt.clouddn.com/1514427149119.png" alt=""></p>
<pre><code>SQL&gt; select t1.name tbname,t2.name from v$tablespace t1,v$datafile t2 where t1.ts# = t2.ts#;#查看当前数据库所有表空间及其数据文件路径
</code></pre><blockquote>
<p><code>SQL&gt; select file_name,tablespace_name from dba_data_files;</code>  #  某人竟然嘲讽我，哼哼<br><img src="http://p09u6sy9g.bkt.clouddn.com/1514427174791.png" alt=""></p>
</blockquote>
<h4 id="表空间管理"><a href="#表空间管理" class="headerlink" title="表空间管理"></a>表空间管理</h4><p>创建表空间的条件</p>
<blockquote>
<p>1.具有create tablespace权限，dba，sysdba，sysoper拥有改权限，可授予</p>
<p>2.创建的是bigfile/smallfile，超过T级应考虑bigfile</p>
<p>3.新建的表空间的I/O，是否会导致磁盘I/O不够用</p>
<p>4.oracle需要具有创建表空间时指定datafile路径的写权限</p>
</blockquote>
<p> <code>SQL&gt; select PROPERTY_NAME,PROPERTY_VALUE from database_properties where PROPERTY_NAME like &#39;%TBS%&#39;;  #查看表空间创建缺省状态时bigfile还是smallfile</code></p>
<p> <code>SQL&gt; alter database set default bigfile/smallfile tablespace;  #修改创建表空间缺省为大/小文件状态</code></p>
<p>大表文件（bigfile)最大可以存放个T的容量。头文件的大小达到了G－－＞block,普通的头文件大小为M—-&gt;block。<br>好处：减少了数据文件的个数，管理方便，大的对象的存放得到了优化。减少了control文件的信息，控制文件定义了datafile的个数。<br>bigfile只能存在一个数据文件，所以要保证分配的的磁盘具有足够的空间。</p>
<pre><code>SQL&gt; create tablespace TBS1 datafile &apos;+ORADATA/chaos/datafile/tbs1.dbf&apos; size 100m;   #创建数据文件路径为 &apos;~tbs1.dbf&apos;的名为TBS1的表空间

SQL&gt; create temporary tablespace TMP1 tempfile &apos;+ORADATA/chaos/datafile/tmp1.dbf&apos; size 10m; 创建数据文件路径为 &apos;~tmp1.dbf&apos;的名为TBS2的临时表空间，临时表空间文件不能设置为只读，不能重命名数据文件，日志方式总是nologing，主要用途是在数据库进行排序运算，管理索引，访问视图等操作时提供临时的运算空间，当运算完成之后系统会自动清理，因为用途不同所以才有了默认表空间和临时表空间区分，实际上数据库都是有默认临时空间的，但实际应用中很难满足需求，所以才需要自己创建临时空间。

SQL&gt; alter database tempfile &apos;+ORADATA/chaos/tempfile/tmp1.dbf&apos; resize 15m; #重置大小

SQL&gt; alter database tempfile &apos;+ORADATA/chaos/tempfile/tmp1.dbf&apos; autoextend on next 10m maxsize 100m;#打开自动扩展

SQL&gt; alter tablespace tmp1 add tempfile &apos;+ORADATA/chaos/datafile/tmp2.dbf&apos; size 10m;   #给临时表空间增加临时文件，增加到表空间中的数据文件不能直接从表空间中删除，除非删掉整个表空间，增加数据文件将有助于均衡I/O

SQL&gt; alter database default temporary tablespace tmp1;   #设置tmp1为默认临时表空间，如果没有指定默认临时表空间，那么将使用system表空间作为排序区

SQL&gt; SELECT dbms_metadata.get_ddl(&apos;TABLESPACE&apos;,&apos;SYSTEM&apos;) FROM dual;  #查看创建表空间语句
</code></pre><p><img src="http://p09u6sy9g.bkt.clouddn.com/1514427094493.png" alt=""></p>
<pre><code>SQL&gt; CREATE UNDO TABLESPACE tablespace_name DATAFILE &apos;+ORADATA/chaos/datafile/undotbs2.dbf&apos; size 10m;  #创建undo表空间

SQL&gt; ALTER SYSTEM SET UNDO_TABLESPACE=tablespace_name;   #修改默认undo表空间
</code></pre><p><img src="http://p09u6sy9g.bkt.clouddn.com/1514427510106.png" alt=""></p>
<pre><code>SQL&gt; alter tablespace TABLESPACE_NAME rename to TBS2; # 表空间改名

undo表空间扩容，重置表空间大小，添加数据文件，自动扩展，跟临时表空间操作没有区别

SQL&gt; drop tablespace tbs2；  #删除表空间，注意，system，sysaux，user表空间强烈不建议删除，会崩，默认表空间和当前undo表空间无法删除
</code></pre><p>表空间的管理方式</p>
<pre><code>SQL&gt; select TABLESPACE_NAME,EXTENT_MANAGEMENT,BLOCK_SIZE,STATUS,CONTENTS,FORCE_LOGGING,BIGFILE from dba_tablespaces;
</code></pre><p><img src="http://p09u6sy9g.bkt.clouddn.com/1514428149844.png" alt=""></p>
<p>字典管理:字典管理表空间-DMT，指oracle的表空间分配和回收是通过数据库中的数据字典表来记录和管理，用于管理的两个数据字典分别是 UET$（used extents)和FET$(freeextents),其工作方式是当建立一个新的段或者表空间时，oracle通过一系列SQL语句来完成这个工作，更新<br>上述两个字点的信息，在繁忙系统中会造成竞争和等待（另一个DMT会带来的问题是空间碎片）</p>
<p>本地管理:LMT的表空间数据文件头部加入了一个位图区域，在其中记录每个extent的使用状况，当extent被使用或者被释放，oracle会更新头部的记录来反映这个变化，不产生回滚信息，因为仅仅操作数据文件头部的几个数据块，不用操作数据字典，LMT比DMT要快，尤其是数据库繁忙的时候更明显</p>
<p>通过使用PL/SQL 完成表空间转换 </p>
<pre><code>exec dbms_space_admin.tablespace_migrate_to_local(&apos;USERS&apos;);
exec dbms_space_admin.tablespace_migrate_from_local(&apos;USERS&apos;);
</code></pre><h4 id="表空间的四种状态："><a href="#表空间的四种状态：" class="headerlink" title="表空间的四种状态："></a>表空间的四种状态：</h4><p>online</p>
<p>offline</p>
<p>read only</p>
<p>read write</p>
<p>一个表空间的正常状态是联机（online），有时需要将某表空间进行脱机，以进行数据库维护</p>
<p>如：在数据打开的状态下移动数据文件，恢复一个表空间或数据文件,执行表空间的脱机备份，在数据库正常工作情况下使数据库的某部分不可访问</p>
<pre><code>SQL&gt; alter tablespace tbs1 offline;

SQL&gt; alter tablespace tbs1 online;
</code></pre><p>read only状态不能执行DML语句，可以使用DDL，DQL语句</p>
<pre><code>SQL&gt; alter tablespace tbs1 read only;

SQL&gt; alter tablespace tbs1 read write;

system 必须online 必须read write

sysaux 可以offline 不能read only

undo 不能offline 不能read only

SQL&gt; select tablespace_name,file#,v.status,v.enabled from dba_data_files d,v$datafile v where d.file_id = v.file#;  #查看表空间的状态
</code></pre><p><img src="http://p09u6sy9g.bkt.clouddn.com/1514447367034.png" alt=""></p>
<p>表空间重定位</p>
<p>open且archive状态，先offline，移动数据文件位置，修改控制文件该数据文件路径，online</p>
<h4 id="表空间相关视图"><a href="#表空间相关视图" class="headerlink" title="表空间相关视图"></a><em>表空间相关视图</em></h4><pre><code>表空间数据字典视图:DBA_TABLESPACES
表空间动态性能视图:V$TABLESPACE
数据字典视图:DBA_DATA_FILES
动态性能视图:V$DATAFILE
数据字典视图:DBA_TEMP_FILES
动态性能视图:V$TEMPFILE
被段分配的区:DBA_EXTENTS
没有被段分配的区: DBA_FREE_SPACE
系统表空间默认设置:database_properties
</code></pre><h3 id="三-归档文件"><a href="#三-归档文件" class="headerlink" title=" 三.归档文件 "></a><font color="red"> 三.归档文件 </font></h3><p>在重做日志分成2部分，一个是在线重做日志文件，另外一个就是归档日志文件。在线重做日志大小毕竟是有限的，当都写满了的时候，就面临着2个选择，第一个就是把以前在线重做日志从头擦除开始继续写，第二种就是把以前的在线重做日志先进行备份，然后对被备份的日志擦除开始写新的在线Redo File。这种备份的在线重做日志就是归档日志。而数据库如果采用这种生成归档日志的模式的话，就是归档日志模式(ARCHIVELOG模式)，反之如果不生成归档日志，就是非归档日志模式(NOARCHIVELOG模式)。</p>
<p>归档日志(Archive Log)是非活动的重做日志备份.通过使用归档日志,可以保留所有重做历史记录,当数据库处于ARCHIVELOG模式并进行日志切换式,后台进程ARCH会将重做日志的内容保存到归档日志中.当数据库出现介质失败时,使用数据文件备份,归档日志和重做日志可以完全恢复数据库.</p>
<h4 id="实验-rac环境下修改归档模式"><a href="#实验-rac环境下修改归档模式" class="headerlink" title="实验: rac环境下修改归档模式"></a>实验: rac环境下修改归档模式</h4><p>rac环境的归档模式切换和单实例稍有不同，主要是共享存储所产生的差异，将rac数据库切换到非集群状态下，在一个实例上实施归档模式切换即可完成rac数据库归档模式转换问题，非切归与归切非为镜像操作，这里仅描述切成归档模式的具体操作</p>
<p>1.主要步骤</p>
<ul>
<li>备份spfile，防止参数修改失败导致数据库无法启动</li>
<li>修改集群参数cluster_database为false</li>
<li>关闭集群数据库</li>
<li>启动单实例到mount状态</li>
<li>修改该实例为归档模式（alter database archivelog/noarchivelog）</li>
<li>修改集群参数cluster_database为true</li>
<li>关闭单实例，启动集群数据库</li>
</ul>
<p>2.操作</p>
<pre><code>查看归档模式

SQL&gt; archive log list;

SQL&gt; select instance_name,host_name,status from gv$instance;

SQL&gt; show parameter cluster；

SQL&gt; create pfile=&apos;/u01/app/oracle/spfileback.ora&apos; from spfile;  #备份spfile

SQL&gt; alter system set cluster_database=false scope=spfile sid=&apos;*&apos;;  #修改为非集群数据库，该参数为静态参数，需要使用scope=spfile

退出sql命令行在linux命令行下面执行集群和单节点的起停操作

$ srvctl stop database -d CHAOS#停集群数据库

$ srvctl start instance -d CHAOS -i CHAOS1 -o mount   #起单个实例到mount状态

SQL&gt; select instance_name,status from v$instance;   #查看启动的单实例的数据库的状态，这个时候应该是mounted状态

SQL&gt; alter database archivelog;   #在mount状态下设置该单实例为归档模式

SQL&gt; alter system set cluster_database=true scope=spfile sid=&apos;*&apos;;   #修改为集群数据库

$ srvctl stop instance -d CHAOS -i CHAOS1#关闭当前操作的单实例数据库

$ srvctl start database -d CHAOS#起集群数据库
</code></pre><p>以上操作按顺序执行，已经在本机实验环境中实际操作验证，执行完最后的起集群操作之后双节点上数据库都处于open状态，并成功修改为归档模式。<br>双节点实验环境是这样，实际生产环境多节点操作也是同理，多节点rac修改归档模式笨办法就是挨个节点启动到mount状态并修改，好处是不影响其他节点的工作，缺点是工作量略大，并且得执行节点数*2的起停数据库操作，风险稍大。<br>这样修改集群状态，更改单实例归档模式，让其他节点自动同步的方式不管有多少节点都一次到位，并且只要操作一次集群的起停操作和一次单实例的起停操作，安全高效</p>
<p>一些归档文件的操作</p>
<pre><code>SQL&gt; alter system switch logfile; # 手工切归档

SQL&gt; select inst_id,name,thread#,sequence#,status from gv$archived_log;   #查看归档日志

SQL&gt; select * from v$log;#查看v$log视图里的归档文件信息

SQL&gt; alter system set log_archive_dest = &apos;/u01/app/oracle/arch1/&apos; scope = spfile;  #修改归档文件路径

SQL&gt; alter system set log_archive_duplex_dest = &apos;/u01/app/oracle/arch2&apos; scope = spfile;   #归档到本机且大于等于两个归档位置

SQL&gt; alter system set log_archive_format = &apos;arch_%t_%s_%r.arc&apos;;  #修改归档文件命名格式
</code></pre><h4 id="归档日志相关视图"><a href="#归档日志相关视图" class="headerlink" title="归档日志相关视图"></a><em>归档日志相关视图</em></h4><pre><code>v$archived_log  #从控制文件中获得归档的相关信息

v$archive_dest  #归档路径及状态

v$log_history   #控制文件中日志的历史信息

v$archive_processes #归档相关的后台进程信息
</code></pre><h2 id="rac起停操作"><a href="#rac起停操作" class="headerlink" title="rac起停操作"></a>rac起停操作</h2><h3 id="进程正常"><a href="#进程正常" class="headerlink" title="进程正常"></a>进程正常</h3><p>如图，双节点RAC进程正常，端口监听正常，可以对外提供服务<br><img src="http://p09u6sy9g.bkt.clouddn.com/1516161797321.png" alt=""></p>
<p><img src="http://p09u6sy9g.bkt.clouddn.com/1516161937018.png" alt=""></p>
<p><img src="http://p09u6sy9g.bkt.clouddn.com/1516161963701.png" alt=""></p>
<h3 id="停止HAS-High-Availability-Services"><a href="#停止HAS-High-Availability-Services" class="headerlink" title="停止HAS(High Availability Services)"></a>停止HAS(High Availability Services)</h3><p>必须以root用户操作，每个rac节点都得执行，在执行过程中vip会不断漂移到正常节点，直到所有节点服务都被关闭，执行完成后所有节点1521端口不再监听，所有相关进程全部清除，集群无法被访问</p>
<pre><code># cd /u01/app/11.2/grid/bin

# ./crsctl stop has -f
</code></pre><p><img src="http://p09u6sy9g.bkt.clouddn.com/1516162462783.png" alt=""></p>
<p><img src="http://p09u6sy9g.bkt.clouddn.com/1516164898940.png" alt=""></p>
<h3 id="启动HAS"><a href="#启动HAS" class="headerlink" title="启动HAS"></a>启动HAS</h3><p>root在每个rac节点执行起has的命令，可以在单一节点启动集群数据库</p>
<pre><code># crsctl start has 

$ srvctl status database -d racdb   oracle用户
</code></pre><p><img src="http://p09u6sy9g.bkt.clouddn.com/1516165599493.png" alt=""></p>
<p><img src="http://p09u6sy9g.bkt.clouddn.com/1516165626017.png" alt=""></p>
<p><img src="http://p09u6sy9g.bkt.clouddn.com/1516165664331.png" alt=""></p>
<p>可以看到所有的rac相关进程都已经启动，可以通过sqlplus本地登陆，用开发工具访问vip也正常，这是一次完整的rac起停过程</p>
]]></content>
      
        <categories>
            
            <category> Oracle </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Theory </tag>
            
            <tag> Database </tag>
            
            <tag> Original </tag>
            
            <tag> Part-transported </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[搭建个人博客]]></title>
      <url>/2017/11/14/%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/</url>
      <content type="html"><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a><font color="#5CACEE">简介</font></h2><blockquote>
<p>  Hexo 是一个快速、简洁且高效的博客框架。Hexo 使用 Markdown（或其他渲染引擎）解析文章，在几秒内，即可利用靓丽的主题生成静态网页。看官档是个好习惯，<a href="https://hexo.io/zh-cn/docs/" target="_blank" rel="external"><font color="#AAAAAA">戳这里</font></a>跟着官方的文档走清晰明确又全面。</p>
<p><br>很久前搭建过一个基于wordpress的博客，然而那个时候并没有写东西的兴趣，完成了环境搭建之后确定博客可用，实验过程可复现就没再管理。</p>
<p><br>最近一直在看世界历史，越发感觉到生产关系和生产资料在人类文明史发展过程中的核心地位，可以清楚得看到历史的脉络跟生产力的发展之间的关系.</p>
<p><br>做个对社会有贡献的人真的不是一句套话。生产资料的占有者，生产关系中的生产者，社会财富的创造者,这些人才真的有机会去在社会变革的浪潮中大幅度改变既定的人生轨迹。</p>
<p><br>在信息时代，在下一次科技革命的前夕，在即将到来注定是数字化信息化的世界里，一切皆数据。数据就是新的生产资料，用数据去形成各种特定功能的产品，解决问题，就是新世界的生产关系，就如同石器时代会做石斧，铁器时代掌握了冶金技术，化石能源时代掌握了开采石油并加工成产品，核能时代掌握了可控核聚变技术一样，在数据时代中掌握了跟数据相关的姿势，从数据的收集和传递，存储，到在线分析，机器学习，人工智能、从数据支持的企业商务决策到各种炫酷到不行的网页设计，前端展示。就算站不到浪潮之巅，苟全性命于盛世绰绰有余。</p>
<p><br>这个博客第一是把我的学习状态和进度拿出来晒一晒，即使没人看，也是对我自己的一种鼓励（ps：万一有人看到了呢），现在处于大数据职业技能树生成阶段，既然走在正确的路上，用点手段督促自己走快点。再一个就是分享一些技术文档，刚从菜鸟阶段提升了一点，踩过的坑，做过的事都摆出来，看看能不能帮到一起前进的同道中人，也算是一种展现自己价值的方式。</p>
</blockquote>
<a id="more"></a>
<h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a><font color="#5CACEE">思路</font></h2><blockquote>
<p>要搭建一个自己的博客，可以选择类似CSDN之类的网站，网站提供现成服务和模板，作者编辑内容后提交，网站审核发布。也可以在自己的服务器上用博客模板搭建服务，编辑内容后发布，游客通过访问服务器来访问作者的博客。我在某服务商那里常年租借VPS，所以选择自己搭建。<br> <br>搭建的时候可以选择<a href="https://baike.baidu.com/item/WordPress/450615?fr=aladdin" target="_blank" rel="external"><font color="#AAAAAA">wrodpress</font></a>，<a href="https://gohugo.io/" target="_blank" rel="external"><font color="#AAAAAA">hugo</font></a>，<a href="https://jekyllrb.com/" target="_blank" rel="external"><font color="#AAAAAA">jekyll</font></a>,<a href="https://hexo.io/zh-cn/" target="_blank" rel="external"><font color="#AAAAAA">hexo</font></a>等博客框架和生成静态网页的工具，这里选择使用的是hexo。<br><br>确定了要做什么和怎么做，之后的事情就简单了。用xshell或CRT终端连接VPS，部署服务，编辑内容，测试，发布，博客就算搭建完成。<br><br>搭建完成后得提供访问地址，总不能给别人一个IP和端口说“这是我的博客，跑在这个服务器这个端口上，欢迎访问”。一不专业，二不安全，建议申请域名进行<a href="https://baike.baidu.com/item/%E5%9F%9F%E5%90%8D%E8%A7%A3%E6%9E%90/574285?fr=aladdin" target="_blank" rel="external"><font color="#AAAAAA">域名解析</font></a>，可以选择国内的域名，申请备案然后使用。笔者这里选择在<a href="http://www.freenom.com/zh/index.html?lang=zh" target="_blank" rel="external"><font color="#AAAAAA">freenom</font></a>申请一个免费域名并设置解析VPS，达到通过域名直接访问博客的目的，同时因为是境外的域名，所以不涉及到备案，可以直接访问。还可以通过更改配置文件，添加公钥等步骤将博客设置托管到github上，通过github提供的服务来访问博客，好处是所有的内容都会推送一份到github上，哪怕服务器挂了也算有个备份。</p>
<h2 id="环境"><a href="#环境" class="headerlink" title="环境"></a><font color="#5CACEE">环境</font></h2></blockquote>
<table>
<thead>
<tr>
<th>软件</th>
<th>下载地址</th>
</tr>
</thead>
<tbody>
<tr>
<td> node.js</td>
<td><a href="https://nodejs.org/zh-cn/download/" target="_blank" rel="external"><font color="#AAAAAA">点击下载</font></a></td>
</tr>
<tr>
<td> git</td>
<td><font color="#AAAAAA">yum安装即可</font></td>
</tr>
<tr>
<td> hexo</td>
<td><font color="#AAAAAA">npm安装即可</font></td>
</tr>
<tr>
<td> nginx</td>
<td><a href="http://nginx.org/download/nginx-1.13.6.tar.gz" target="_blank" rel="external"><font color="#AAAAAA">点击下载</font></a></td>
</tr>
</tbody>
</table>
<h2 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a><font color="#5CACEE">步骤</font></h2><h3 id="一-安装"><a href="#一-安装" class="headerlink" title="一 安装 "></a><font color="#CDAA7D">一 安装 </font></h3><h4 id="1-安装node-js"><a href="#1-安装node-js" class="headerlink" title="1. 安装node.js"></a><font color="#CDAA7D">1. 安装node.js</font></h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rpm -ivh node-8.4.0-el6.x86_64.rpm</span><br></pre></td></tr></table></figure>
<p><img src="http://p09u6sy9g.bkt.clouddn.com/1512702233333.png" alt="avatar"></p>
<h4 id="2-安装git"><a href="#2-安装git" class="headerlink" title="2. 安装git"></a><font color="#CDAA7D">2. 安装git</font></h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install git</span><br></pre></td></tr></table></figure>
<p><img src="http://p09u6sy9g.bkt.clouddn.com/1512702341648.png" alt=""><br><img src="http://p09u6sy9g.bkt.clouddn.com/1512702385850.png" alt=""></p>
<h4 id="3-安装hexo"><a href="#3-安装hexo" class="headerlink" title="3. 安装hexo"></a><font color="#CDAA7D">3. 安装hexo</font></h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install -g hexo-cli</span><br></pre></td></tr></table></figure>
<p><img src="http://p09u6sy9g.bkt.clouddn.com/1512702417480.png" alt=""></p>
<h4 id="4-创建工作文件夹"><a href="#4-创建工作文件夹" class="headerlink" title="4. 创建工作文件夹"></a><font color="#CDAA7D">4. 创建工作文件夹</font></h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hexo init myblog</span><br><span class="line">cd /myblog; npm install</span><br></pre></td></tr></table></figure>
<p><img src="http://p09u6sy9g.bkt.clouddn.com/1512702453585.png" alt=""><br><img src="http://p09u6sy9g.bkt.clouddn.com/1512702547734.png" alt=""></p>
<h4 id="5-启动服务"><a href="#5-启动服务" class="headerlink" title="5. 启动服务"></a><font color="#CDAA7D">5. 启动服务</font></h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo s (完整命令hexo server，可简写)</span><br></pre></td></tr></table></figure>
<p>访问4000端口即可看到已经提供服务</p>
<h3 id="二-使用"><a href="#二-使用" class="headerlink" title="二 使用 "></a><font color="#CDAA7D">二 使用 </font></h3><h4 id="1-推一篇文章"><a href="#1-推一篇文章" class="headerlink" title="1.推一篇文章"></a><font color="#DDA0DD">1.推一篇文章</font></h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo new &quot;My first post with hexo&quot;</span><br></pre></td></tr></table></figure>
<p><img src="http://p09u6sy9g.bkt.clouddn.com/1512702662761.png" alt=""></p>
<p>该命令会在存放博客内容的文件夹（/myblog/source/_posts）内生成一个My first post with hexo.md文件,写入下文并使用hexo g(hexo generate)命令使hexo根据.md文件生成网页静态文件，hexo s 开启测试服务，访问4000端口即可看到新增的名为”My first post with hexo”的博客文章</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">title: My Fist Post with hexo</span><br><span class="line">date: 2016-09-25 20:03:25</span><br><span class="line">tags:</span><br><span class="line">---</span><br><span class="line">This my first post using [Hexo](https://hexo.io/)! </span><br><span class="line"></span><br><span class="line">## First title</span><br><span class="line"></span><br><span class="line">### a first subtitile </span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">    $ hexo new &quot;My New Post&quot;</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">More info: [Writing](https://hexo.io/docs/writing.html)</span><br></pre></td></tr></table></figure>
<p><img src="http://p09u6sy9g.bkt.clouddn.com/1512702911276.png" alt=""></p>
<h4 id="2-将博客托管在github上"><a href="#2-将博客托管在github上" class="headerlink" title="2.将博客托管在github上"></a><font color="#DDA0DD">2.将博客托管在github上</font></h4><p>github创建的仓库可以提供一个链接，将静态页面文件以网页的形式展示出来。在git里面创建该特殊的仓库，然后通过修改hexo的配置文件将托管形式改成git，使用hexo d(hexo deployment)将本地生成博客文件推送到仓库，便可以通过github提供的服务访问博客，大概可以分为以下三步</p>
<h5 id="添加密钥"><a href="#添加密钥" class="headerlink" title="* 添加密钥"></a><font color="#DDA0DD">* 添加密钥</font></h5><p>在服务器生成密钥，添加到github里，使服务器和github可以进行文件的读写,具体操作步骤可以参考我的另一篇博客<a href="https://hexo.io/zh-cn/docs/" target="_blank" rel="external"><font color="#AAAAAA">Build your own blog with hexo</font></a>内容中相关部分</p>
<h5 id="安装可以实现将hexo文件部署到git的组建"><a href="#安装可以实现将hexo文件部署到git的组建" class="headerlink" title="* 安装可以实现将hexo文件部署到git的组建"></a><font color="#DDA0DD">* 安装可以实现将hexo文件部署到git的组建</font></h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-deployer-git --save</span><br></pre></td></tr></table></figure>
<p><img src="http://p09u6sy9g.bkt.clouddn.com/1512713232758.png" alt=""></p>
<h5 id="修改配置文件并将内容推送到git"><a href="#修改配置文件并将内容推送到git" class="headerlink" title="* 修改配置文件并将内容推送到git"></a><font color="#DDA0DD">* 修改配置文件并将内容推送到git</font></h5><blockquote>
<p>修改配置文件</p>
</blockquote>
<p><img src="http://p09u6sy9g.bkt.clouddn.com/1512716088946.png" alt=""></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">将博文推送到github上</span><br><span class="line">hexo d</span><br></pre></td></tr></table></figure>
<p><img src="http://p09u6sy9g.bkt.clouddn.com/1512716177202.png" alt=""></p>
<p>至此，在本地部署一个博客网站，托管到gitbhub上，可以通过链接 <a href="https://uxtuo.github.io" target="_blank" rel="external">https://uxtuo.github.io</a> 访问，get！</p>
<h4 id="3-设置域名并用nginx代理"><a href="#3-设置域名并用nginx代理" class="headerlink" title="3.设置域名并用nginx代理 "></a><font color="#DDA0DD">3.设置域名并用nginx代理 </font></h4><p>将博客托管在github上和通过域名直接访问服务器的博客不冲突。<br><br>hexo作为生成静态文件的工具，编辑内容并生成博客文件,推送并访问位于git上的文件，亦或者通过ip或者域名访问服务器上的文件没有区别，这里申请一个域名对vps的ip进行解析，使游客可以通过域名直接访问服务器上的博客，并用nginx做代理处理对服务器的请求。</p>
<h5 id="申请域名并设置解析地址"><a href="#申请域名并设置解析地址" class="headerlink" title="* 申请域名并设置解析地址"></a><font color="#DDA0DD">* 申请域名并设置解析地址</font></h5><blockquote>
<p>在<a href="http://www.freenom.com/zh/index.html?lang=zh" target="_blank" rel="external"><font color="#AAAAAA">freenom</font></a>申请免费域名，设置域名解析为vps地址</p>
</blockquote>
<h5 id="设置nginx做代理以应付可能的高并发访问"><a href="#设置nginx做代理以应付可能的高并发访问" class="headerlink" title="* 设置nginx做代理以应付可能的高并发访问"></a><font color="#DDA0DD">* 设置nginx做代理以应付可能的高并发访问</font></h5><p>安装nginx（下载压缩包后解压到/usr/local/目录即可使用），修改配置文件，使nginx代理对该服务器80端口的所有访问，设置nginx解析位于/myblog/public目录hexo生成的静态网页文件</p>
<p>这样就可以实现通过域名访问部署在VPS上的博客，get！</p>
<blockquote>
<p>本博客就是通过上文方法搭建，所有操作均通过实验，有兴趣的筒子们可以跟着做，绝对轻松愉快可操作，有问题可以发邮件给我（yung241088@126.com）,我一定会看但不一定会回/滑稽</p>
</blockquote>
]]></content>
      
        <categories>
            
            <category> env </category>
            
        </categories>
        
        
        <tags>
            
            <tag> blog </tag>
            
            <tag> env </tag>
            
            <tag> hexo </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Build your own blog with hexo]]></title>
      <url>/2017/11/10/what-is-hexo/</url>
      <content type="html"><![CDATA[<h2 id="What-is-Hexo"><a href="#What-is-Hexo" class="headerlink" title="What is Hexo?"></a><font color="#5CACEE">What is Hexo?</font></h2><blockquote>
<p>Hexo is a fast, simple and powerful blog framework. You write posts in Markdown (or other languages) and Hexo generates static files with a beautiful theme in seconds.<br>Installing Hexo is quite easy. However, you do need to have Nodejs &amp; Git installed first.<br>In order to install Nodejs you can see Install &amp; run your first application Nodejs.<br>In order to install Git you can see <a href="https://git-scm.com/" target="_blank" rel="external">https://git-scm.com/</a> .</p>
</blockquote>
<a id="more"></a>
<h2 id="Install-Hexo"><a href="#Install-Hexo" class="headerlink" title="Install Hexo"></a><font color="#5CACEE">Install Hexo</font></h2><p>  Once all the requirements are installed,you can install Hexo.<br> <br> $ npm install -g hexo-cli</p>
<blockquote>
<p><img src="https://t1.picb.cc/uploads/2017/11/10/vOiqe.png" alt="vOiqe.png"></p>
</blockquote>
<h2 id="Create-a-blog"><a href="#Create-a-blog" class="headerlink" title="Create a blog"></a><font color="#5CACEE">Create a blog</font></h2><p>  Now that hexo is installed run the following commands to initialise Hexo project<br> <br>$ hexo init myblog <br> $ cd myblog  <br>$ npm install</p>
<blockquote>
<p><img src="https://t1.picb.cc/uploads/2017/11/10/vOIiN.png" alt="vOIiN.png"></p>
</blockquote>
<p>You can modify site settings in <font color="brow" size="2">_config.yml</font>. for the sake of simplicity we�re only modify the Title and author name .</p>
<h2 id="Run-the-Blog"><a href="#Run-the-Blog" class="headerlink" title="Run the Blog"></a><font color="#5CACEE">Run the Blog</font></h2><p>  Run the server:<br><br>    $ hexo server </p>
<blockquote>
<p><img src="https://t1.picb.cc/uploads/2017/11/10/vOfzK.png" alt="vOfzK.png"><br>  launch your browser and navigate to <a href="http://localhost:4000" target="_blank" rel="external">http://localhost:4000</a>.<br><img src="https://t1.picb.cc/uploads/2017/11/10/vOZcj.png" alt="vOZcj.png"><br>Voila your first blog is working!</p>
</blockquote>
<h2 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a><font color="#5CACEE">Create a new post</font></h2><p>  Create a new post is very simlpe all what you have to do is :<br>  <br>   $ hexo new “My Fist Post with hexo”</p>
<blockquote>
<p><img src="https://t1.picb.cc/uploads/2017/11/10/vOVrX.png" alt="vOVrX.png"><br>Update the file using Markdown language:</p>
</blockquote>
<figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">title: My Fist Post with hexo</span><br><span class="line">date: 2016-09-25 20:03:25</span><br><span class="line">tags:</span><br><span class="line">---</span><br><span class="line">This my first post using [<span class="string">Hexo</span>](<span class="link">https://hexo.io/</span>)! </span><br><span class="line"></span><br><span class="line"><span class="section">## First title</span></span><br><span class="line"></span><br><span class="line"><span class="section">### a first subtitile </span></span><br><span class="line"></span><br><span class="line"><span class="code">    /usr/bin/bash</span></span><br><span class="line"><span class="code">    $ hexo new "My New Post"</span></span><br><span class="line"><span class="code">    </span></span><br><span class="line"></span><br><span class="line"><span class="section">## Second title</span></span><br><span class="line"></span><br><span class="line">More info: [<span class="string">Writing</span>](<span class="link">https://hexo.io/docs/writing.html</span>)</span><br></pre></td></tr></table></figure>
<blockquote>
<p><img src="https://t1.picb.cc/uploads/2017/11/10/vOgSG.png" alt="vOgSG.png"><br>  Run the server again:<br> <br>   $ hexo server<br><img src="https://t1.picb.cc/uploads/2017/11/10/vOnxs.png" alt="vOnxs.png"></p>
</blockquote>
<h2 id="Deployment-on-Github"><a href="#Deployment-on-Github" class="headerlink" title="Deployment on Github"></a><font color="#5CACEE">Deployment on Github</font></h2><p>  Now what about Deployment, it�s exactly what we are going to do, first Create new Github repository :</p>
<blockquote>
<p><img src="https://t1.picb.cc/uploads/2017/11/10/vOwQc.png" alt="vOwQc.png"><br>  Click settings<br><img src="https://t1.picb.cc/uploads/2017/11/10/vOlt7.png" alt="vOlt7.png"><br><img src="https://t1.picb.cc/uploads/2017/11/10/vOCST.png" alt="vOCST.png"><br>  Then install hexo-deployer-git:<br> <br>  $ npm install hexo-deployer-git –save<br><img src="https://t1.picb.cc/uploads/2017/11/10/vOyx8.png" alt="vOyx8.png"></p>
</blockquote>
<p>  Click clone or download button:</p>
<blockquote>
<p><img src="https://t1.picb.cc/uploads/2017/11/10/vOu56.png" alt="vOu56.png"></p>
</blockquote>
<p>  Update _config.yaml file :</p>
<blockquote>
<p><img src="https://t1.picb.cc/uploads/2017/11/10/vOtyy.png" alt="vOtyy.png"></p>
</blockquote>
<p>  It�s time for deployement :<br>  <br> $ hexo deploy</p>
<blockquote>
<p><img src="https://t1.picb.cc/uploads/2017/11/10/vOJtg.png" alt="vOJtg.png"></p>
</blockquote>
<p>  To preview launch your browser.</p>
<blockquote>
<p><img src="https://t1.picb.cc/uploads/2017/11/10/vOmyM.png" alt="vOmyM.png"></p>
</blockquote>
<p>You can get see the blog on <a href="https://malektrainer.github.io/" target="_blank" rel="external">https://malektrainer.github.io/</a>.<br><br>You can find source code on <a href="https://github.com/malektrainer/myblog" target="_blank" rel="external">https://github.com/malektrainer/myblog</a>.</p>
]]></content>
      
        <categories>
            
            <category> blog </category>
            
        </categories>
        
        
        <tags>
            
            <tag> blog </tag>
            
            <tag> server </tag>
            
            <tag> environment </tag>
            
            <tag> mark </tag>
            
        </tags>
        
    </entry>
    
  
  
</search>
